import streamlit as st
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import pandas as pd
import plotly.express as px
from datetime import datetime
import tempfile
import os
from transformers import pipeline
import mediapipe as mp
import speech_recognition as sr
import threading
import queue
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê°ì • ì¼ê¸° - Emotion Diary",
    page_icon="ğŸ“”",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'emotion_history' not in st.session_state:
    st.session_state.emotion_history = []
if 'diary_entries' not in st.session_state:
    st.session_state.diary_entries = []
if 'recording' not in st.session_state:
    st.session_state.recording = False
if 'video_frames' not in st.session_state:
    st.session_state.video_frames = []
if 'current_emotion' not in st.session_state:
    st.session_state.current_emotion = 'neutral'
if 'emotion_timeline' not in st.session_state:
    st.session_state.emotion_timeline = []
if 'recording_start_time' not in st.session_state:
    st.session_state.recording_start_time = None
if 'webcam_active' not in st.session_state:
    st.session_state.webcam_active = False
if 'voice_recording' not in st.session_state:
    st.session_state.voice_recording = False
if 'transcribed_text' not in st.session_state:
    st.session_state.transcribed_text = ""
if 'audio_queue' not in st.session_state:
    st.session_state.audio_queue = queue.Queue()
if 'pending_save' not in st.session_state:
    st.session_state.pending_save = False
if 'save_data' not in st.session_state:
    st.session_state.save_data = None
if 'last_text_update' not in st.session_state:
    st.session_state.last_text_update = time.time()
if 'emotion_confirmed' not in st.session_state:
    st.session_state.emotion_confirmed = False
if 'confirmed_emotion' not in st.session_state:
    st.session_state.confirmed_emotion = None

# ì‚¬ì´ë“œë°” (ë©”ì¸ UIë³´ë‹¤ ë¨¼ì € ì‹¤í–‰ë˜ì–´ì•¼ anonymize_option ë³€ìˆ˜ê°€ ì •ì˜ë¨)
with st.sidebar:
    st.header("ğŸ“¸ ë…¹í™” ì„¤ì •")
    
    anonymize_option = st.selectbox(
        "ì „ì²´ í™”ë©´ ìµëª…í™” ë°©ì‹",
        ["ì›ë³¸", "ë¸”ëŸ¬", "í”½ì…€í™”", "ì¹´íˆ°"],
        key="anonymize",
        disabled=st.session_state.recording
    )
    
    show_emotion_overlay = st.checkbox(
        "ê°ì • ì •ë³´ ì˜¤ë²„ë ˆì´ í‘œì‹œ", 
        value=True,
        disabled=st.session_state.recording
    )
    
    if st.session_state.recording:
        st.warning("âš ï¸ ë…¹í™” ì¤‘ì—ëŠ” ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    ### ğŸ“¹ ìŒì„± ì˜ìƒ ì¼ê¸° ì‘ì„± ìˆœì„œ
    
    1. ğŸ¨ **ìµëª…í™” ë°©ì‹ ì„ íƒ**
    2. ğŸ”´ **ë…¹í™” ì‹œì‘** í´ë¦­
    3. ğŸ¤ **ë§í•˜ë©° ê°ì • í‘œí˜„**
    4. â¹ï¸ **ë…¹í™” ì¤‘ì§€ & ì €ì¥**
    5. âœ¨ **ì˜¤ëŠ˜ì˜ ê¸°ë¶„ ì„ íƒ**
    6. ğŸ“¥ **ì˜ìƒ & í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ**
    
    ### ğŸ­ ì§€ì› ê°ì •
    - ğŸ˜Š Happy (í–‰ë³µ)
    - ğŸ˜¢ Sad (ìŠ¬í””)
    - ğŸ˜  Angry (í™”ë‚¨)
    - ğŸ˜² Surprise (ë†€ëŒ)
    - ğŸ˜ Neutral (ì¤‘ë¦½)
    - ğŸ˜¨ Fear (ë‘ë ¤ì›€)
    - ğŸ¤¢ Disgust (í˜ì˜¤)
    
    ### ğŸ”’ ìµëª…í™” ë°©ì‹
    - **ì›ë³¸**: ì–¼êµ´ ê·¸ëŒ€ë¡œ
    - **ë¸”ëŸ¬**: ì „ì²´ í™”ë©´ íë¦¬ê²Œ
    - **í”½ì…€í™”**: ì „ì²´ í™”ë©´ ëª¨ìì´í¬
    - **ì¹´íˆ°**: ì „ì²´ í™”ë©´ ë§Œí™” ìŠ¤íƒ€ì¼
    
    ### ğŸ¤ ìŒì„± ì…ë ¥
    - **ìë™ í™œì„±í™”**: ë…¹í™” ì‹œì‘ ì‹œ ìë™ í™œì„±í™”
    - **í•œêµ­ì–´ ì¸ì‹**: ì‹¤ì‹œê°„ í•œêµ­ì–´ ìŒì„± ì¸ì‹
    - **ì‹¤ì‹œê°„ ë³€í™˜**: ë§í•œ ë‚´ìš©ì„ ì¦‰ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - **ìë™ ì €ì¥**: ì¼ê¸°ë¡œ ìë™ ì €ì¥
    
    ### ğŸ’¡ ì´¬ì˜ íŒ
    - ğŸ’¡ **ë°ì€ ì¡°ëª…** ì‚¬ìš©
    - ğŸ“· **ì •ë©´ ì–¼êµ´** ìœ ì§€
    - ğŸ˜€ **ìì—°ìŠ¤ëŸ¬ìš´ í‘œì •**
    - ğŸ”‡ **ì¡°ìš©í•œ í™˜ê²½** (ìŒì„± ì¸ì‹ ìµœì í™”)
    - ğŸ¤ **ë§ˆì´í¬ ê°€ê¹Œì´**ì—ì„œ ë˜ë ·í•˜ê²Œ ë§í•˜ê¸°
    - ğŸ—£ï¸ **ì²œì²œíˆ ëª…í™•í•˜ê²Œ** ë°œìŒí•˜ê¸°
    """)
    
    st.markdown("---")
    
    # í†µê³„
    st.subheader("ğŸ“ˆ ì „ì²´ í†µê³„")
    total_entries = len(st.session_state.diary_entries)
    total_frames = sum([e['frame_count'] for e in st.session_state.diary_entries])
    voice_entries = len(st.session_state.diary_entries)
    st.metric("ì´ ì¼ê¸° ìˆ˜", total_entries)
    st.metric("ì´ í”„ë ˆì„ ìˆ˜", total_frames)
    st.metric("ìŒì„± ì…ë ¥ ì‚¬ìš©", f"{voice_entries}íšŒ")
    
    st.markdown("---")
    
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì´ˆê¸°í™”", type="secondary"):
        st.session_state.emotion_history = []
        st.session_state.diary_entries = []
        st.session_state.video_frames = []
        st.session_state.emotion_timeline = []
        st.session_state.recording = False
        st.session_state.webcam_active = False
        st.session_state.recording_start_time = None
        st.session_state.voice_recording = False
        st.session_state.transcribed_text = ""
        st.session_state.pending_save = False
        st.session_state.save_data = None
        st.session_state.emotion_confirmed = False
        st.session_state.confirmed_emotion = None
        st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        st.rerun()

# í•œê¸€ í°íŠ¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_korean_font(size=40):
    """í•œê¸€ì„ ì§€ì›í•˜ëŠ” í°íŠ¸ ë¡œë“œ"""
    font_paths = [
        "malgun.ttf",
        "C:/Windows/Fonts/malgun.ttf",
        "AppleGothic.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
        "NanumGothic.ttf",
    ]
    
    for font_path in font_paths:
        try:
            return ImageFont.truetype(font_path, size)
        except:
            continue
    
    return ImageFont.load_default()

# í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ì¶”ê°€í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def put_korean_text(image, text, position, font_size=40, color=(255, 255, 255)):
    """OpenCV ì´ë¯¸ì§€ì— í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€"""
    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(pil_image)
    font = load_korean_font(font_size)
    draw.text(position, text, fill=color, font=font)
    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

# ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_emotion_model():
    try:
        return pipeline("image-classification", model="dima806/facial_emotions_image_detection")
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# MediaPipe ì–¼êµ´ ê²€ì¶œ
@st.cache_resource
def load_face_detector():
    try:
        return mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5)
    except Exception as e:
        st.error(f"ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# ì „ì²´ í™”ë©´ ìµëª…í™” í•¨ìˆ˜ë“¤
def blur_frame(image: np.ndarray, strength: int = 51) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ë¸”ëŸ¬ ì²˜ë¦¬"""
    if strength % 2 == 0:
        strength += 1
    return cv2.GaussianBlur(image, (strength, strength), 0)

def pixelate_frame(image: np.ndarray, blocks: int = 16) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ í”½ì…€í™”"""
    h, w = image.shape[:2]
    if h > 0 and w > 0 and h > blocks and w > blocks:
        temp = cv2.resize(image, (blocks, blocks), interpolation=cv2.INTER_LINEAR)
        return cv2.resize(temp, (w, h), interpolation=cv2.INTER_NEAREST)
    return image

def cartoonize_frame(image: np.ndarray) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ì¹´íˆ° ìŠ¤íƒ€ì¼ ë³€í™˜"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        color = cv2.bilateralFilter(image, 9, 250, 250)
        for _ in range(2):
            color = cv2.bilateralFilter(color, 9, 250, 250)
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray = cv2.medianBlur(gray, 7)
        edges = cv2.adaptiveThreshold(
            gray, 255, 
            cv2.ADAPTIVE_THRESH_MEAN_C, 
            cv2.THRESH_BINARY, 
            9, 2
        )
        
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        cartoon = cv2.bitwise_and(color, edges_colored)
        cartoon = cv2.convertScaleAbs(cartoon, alpha=1.2, beta=10)
        
        return cartoon
    except Exception as e:
        print(f"ì¹´íˆ° ë³€í™˜ ì˜¤ë¥˜: {e}")
        return image

# ìŒì„± ì¸ì‹ í•¨ìˆ˜
def record_audio_continuous(audio_queue, stop_event):
    """ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì„ ì¸ì‹í•˜ëŠ” í•¨ìˆ˜"""
    recognizer = sr.Recognizer()
    recognizer.energy_threshold = 4000
    recognizer.dynamic_energy_threshold = True
    
    with sr.Microphone() as source:
        print("ë§ˆì´í¬ ì¡°ì • ì¤‘...")
        recognizer.adjust_for_ambient_noise(source, duration=1)
        print("ìŒì„± ì¸ì‹ ì‹œì‘!")
        
        while not stop_event.is_set():
            try:
                audio = recognizer.listen(source, timeout=1, phrase_time_limit=10)
                
                try:
                    text = recognizer.recognize_google(audio, language='ko-KR')
                    if text:
                        audio_queue.put(text)
                        print(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
                except sr.UnknownValueError:
                    pass
                except sr.RequestError as e:
                    print(f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
                    time.sleep(1)
            except sr.WaitTimeoutError:
                continue
            except Exception as e:
                print(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
                time.sleep(1)

# ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_emotion_quick(image: np.ndarray, model, face_detector) -> tuple[str, float, tuple]:
    """ë¹ ë¥¸ ê°ì • ë¶„ì„ (ì‹¤ì‹œê°„ìš©) - ì–¼êµ´ ìœ„ì¹˜ë§Œ ë°˜í™˜"""
    emotion = "neutral"
    confidence = 0.0
    face_bbox = None
    
    if model is None or face_detector is None:
        return emotion, confidence, face_bbox
    
    try:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            detection = results.detections[0]
            bboxC = detection.location_data.relative_bounding_box
            h, w, _ = image.shape
            x = int(bboxC.xmin * w)
            y = int(bboxC.ymin * h)
            width = int(bboxC.width * w)
            height = int(bboxC.height * h)
            
            x = max(0, x)
            y = max(0, y)
            width = min(width, w - x)
            height = min(height, h - y)
            
            face_bbox = (x, y, width, height)
            
            face = image[y:y+height, x:x+width]
            
            if face.size > 0:
                face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
                emotion_results = model(face_pil)
                
                if emotion_results:
                    emotion = emotion_results[0]['label']
                    confidence = emotion_results[0]['score']
    
    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    return emotion, confidence, face_bbox

# ë¹„ë””ì˜¤ ì €ì¥ í•¨ìˆ˜
def save_video(frames: list, filename: str, fps: int = 20):
    """í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
    if not frames or len(frames) == 0:
        return None
    
    height, width, _ = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    
    out = cv2.VideoWriter(filename, fourcc, fps, (width, height))
    
    for frame in frames:
        out.write(frame)
    
    out.release()
    return filename

# AI ê¸°ë°˜ ê¸°ë¶„ ì¶”ì²œ í•¨ìˆ˜
def suggest_mood_from_data(dominant_emotion: str, diary_text: str, emotion_timeline: list) -> list:
    """ê°ì • ë¶„ì„ê³¼ ì¼ê¸° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê¸°ë¶„ì„ ì¶”ì²œ"""
    suggestions = []
    
    emotion_to_mood = {
        'happy': [('í–‰ë³µí•œ', 'ë°ì€ í‘œì •ì´ ìì£¼ ë³´ì˜€ì–´ìš”'), ('ì¦ê±°ìš´', 'ê¸ì •ì ì¸ ì—ë„ˆì§€ê°€ ëŠê»´ì ¸ìš”'), ('ê¸°ìœ', 'ì›ƒëŠ” ëª¨ìŠµì´ ë§ì•˜ì–´ìš”')],
        'joy': [('ê¸°ìœ', 'í™˜í•œ ë¯¸ì†Œê°€ ì¸ìƒì ì´ì—ˆì–´ìš”'), ('ì‹ ë‚˜ëŠ”', 'í™œê¸°ì°¬ ëª¨ìŠµì´ ë³´ì˜€ì–´ìš”'), ('ì¦ê±°ìš´', 'ê¸ì •ì ì¸ ë¶„ìœ„ê¸°ì˜€ì–´ìš”')],
        'sad': [('ìŠ¬í”ˆ', 'ìš°ìš¸í•œ í‘œì •ì´ ë³´ì˜€ì–´ìš”'), ('ìš°ìš¸í•œ', 'í˜ë“  í•˜ë£¨ì˜€ë‚˜ë´ìš”'), ('ì¹¨ìš¸í•œ', 'ê¸°ìš´ì´ ì—†ì–´ ë³´ì˜€ì–´ìš”')],
        'angry': [('í™”ë‚œ', 'ë¶ˆí¸í•œ ê°ì •ì´ ëŠê»´ì¡Œì–´ìš”'), ('ì§œì¦ë‚œ', 'ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ìˆì—ˆë‚˜ë´ìš”'), ('ë¶ˆì¾Œí•œ', 'ê¸°ë¶„ì´ ì¢‹ì§€ ì•Šì•„ ë³´ì˜€ì–´ìš”')],
        'surprise': [('ë†€ë€', 'ì˜ˆìƒì¹˜ ëª»í•œ ì¼ì´ ìˆì—ˆë‚˜ë´ìš”'), ('ë‹¹í™©í•œ', 'ê°‘ì‘ìŠ¤ëŸ¬ìš´ ìƒí™©ì´ ìˆì—ˆë‚˜ìš”'), ('ì˜ì™¸ì˜', 'ìƒˆë¡œìš´ ì¼ì´ ìˆì—ˆë˜ ê²ƒ ê°™ì•„ìš”')],
        'fear': [('ë¶ˆì•ˆí•œ', 'ê±±ì •ì´ ë§ì•„ ë³´ì˜€ì–´ìš”'), ('ë‘ë ¤ìš´', 'ê¸´ì¥ëœ ëª¨ìŠµì´ì—ˆì–´ìš”'), ('ì´ˆì¡°í•œ', 'ë§ˆìŒì´ í¸ì¹˜ ì•Šì•„ ë³´ì˜€ì–´ìš”')],
        'disgust': [('ë¶ˆí¸í•œ', 'ê±°ë¶í•œ ìƒí™©ì´ ìˆì—ˆë‚˜ë´ìš”'), ('ì‹«ì€', 'ë§ˆìŒì— ë“¤ì§€ ì•ŠëŠ” ì¼ì´ ìˆì—ˆë‚˜ìš”'), ('ê±°ë¶í•œ', 'ë¶ˆì¾Œí•œ ê°ì •ì´ ëŠê»´ì¡Œì–´ìš”')],
        'neutral': [('í‰ì˜¨í•œ', 'ì°¨ë¶„í•œ í•˜ë£¨ì˜€ì–´ìš”'), ('ê³ ìš”í•œ', 'ì•ˆì •ì ì¸ ìƒíƒœì˜€ì–´ìš”'), ('ë‹´ë‹´í•œ', 'ì”ì”í•œ í•˜ë£¨ì˜€ë„¤ìš”')]
    }
    
    if dominant_emotion.lower() in emotion_to_mood:
        suggestions.extend(emotion_to_mood[dominant_emotion.lower()])
    else:
        suggestions.extend([('í‰ì˜¨í•œ', 'ì°¨ë¶„í•œ í•˜ë£¨ì˜€ì–´ìš”'), ('ë‹´ë‹´í•œ', 'íŠ¹ë³„í•œ ê°ì • ë³€í™”ê°€ ì—†ì—ˆì–´ìš”')])
    
    if emotion_timeline and len(emotion_timeline) > 5:
        emotions = [e['emotion'] for e in emotion_timeline]
        unique_emotions = len(set(emotions))
        
        if unique_emotions >= 4:
            suggestions.insert(0, ('ë³µì¡í•œ', 'ë‹¤ì–‘í•œ ê°ì •ì„ ëŠë‚€ í•˜ë£¨ì˜€ë„¤ìš”'))
        elif unique_emotions == 1:
            suggestions.insert(0, ('ì¼ê´€ëœ', 'í•˜ë£¨ ì¢…ì¼ ë¹„ìŠ·í•œ ê¸°ë¶„ì´ì—ˆì–´ìš”'))
    
    positive_keywords = ['ì¢‹', 'í–‰ë³µ', 'ê¸°ì¨', 'ì¦ê±°', 'ê°ì‚¬', 'ë¿Œë“¯', 'ì„±ê³µ', 'ì™„ì„±', 'ë‹¬ì„±', 'ì‚¬ë‘']
    negative_keywords = ['í˜ë“¤', 'í”¼ê³¤', 'ì§€ì¹˜', 'ìš°ìš¸', 'ìŠ¬í”„', 'í™”', 'ì§œì¦', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ë¶ˆì•ˆ', 'ê±±ì •']
    
    text_lower = diary_text.lower()
    positive_count = sum(1 for kw in positive_keywords if kw in text_lower)
    negative_count = sum(1 for kw in negative_keywords if kw in text_lower)
    
    if positive_count > negative_count + 2:
        if ('í–‰ë³µí•œ', 'ë°ì€ í‘œì •ì´ ìì£¼ ë³´ì˜€ì–´ìš”') not in suggestions:
            suggestions.insert(0, ('ê°ì‚¬í•œ', 'ê¸ì •ì ì¸ ë‹¨ì–´ë“¤ì´ ë§ì•˜ì–´ìš”'))
    elif negative_count > positive_count + 2:
        if ('ìŠ¬í”ˆ', 'ìš°ìš¸í•œ í‘œì •ì´ ë³´ì˜€ì–´ìš”') not in suggestions:
            suggestions.insert(0, ('ì§€ì¹œ', 'í˜ë“  í‘œí˜„ë“¤ì´ ë§ì•˜ì–´ìš”'))
    
    seen = set()
    unique_suggestions = []
    for mood, reason in suggestions:
        if mood not in seen:
            seen.add(mood)
            unique_suggestions.append((mood, reason))
    
    return unique_suggestions[:5]

# ë©”ì¸ UI
st.title("ğŸ“” ê°ì • ì˜ìƒ ì¼ê¸° - Emotion Video Diary")
st.markdown("*ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ ê°ì •ì„ ë¶„ì„í•˜ë©° ìŒì„±ìœ¼ë¡œ ì˜ìƒ ì¼ê¸°ë¥¼ ì‘ì„±í•˜ì„¸ìš”*")

st.markdown("---")

# ë ˆì´ì•„ì›ƒ êµ¬ì„± - ì›¹ìº ê³¼ ìŒì„± í…ìŠ¤íŠ¸ ì˜ì—­
col_webcam, col_text = st.columns([2, 1])

with col_webcam:
    st.subheader("ğŸ“¹ ì›¹ìº  í™”ë©´")
    
    # 1. ë…¹í™”/ê°ì • í™•ì •/ì™„ë£Œ í†µí•© ë²„íŠ¼ (ë§¨ ìœ„)
    if st.session_state.pending_save and st.session_state.save_data:
        # ê°ì • í™•ì • ì „ - ë…¹í™” ì‹œì‘ ë²„íŠ¼ì´ "ê°ì • í™•ì •í•˜ê¸°"ë¡œ ë³€ê²½
        if not st.session_state.emotion_confirmed:
            # ê°ì • ì„ íƒ UIëŠ” ì•„ë˜ì— í‘œì‹œë˜ê³ , ë²„íŠ¼ë§Œ ì—¬ê¸°ì—
            confirm_emotion = st.button("âœ… ê°ì • í™•ì •í•˜ê¸°", type="primary", use_container_width=True, key="confirm_top_btn")
        else:
            # ê°ì • í™•ì • í›„ - "ì™„ë£Œ" ë²„íŠ¼ìœ¼ë¡œ ë³€ê²½
            complete_action = st.button("âœ… ì™„ë£Œ", type="primary", use_container_width=True, key="complete_top_btn")
    elif not st.session_state.recording:
        start_recording = st.button("ğŸ”´ ë…¹í™” ì‹œì‘", type="primary", use_container_width=True)
    else:
        start_recording = False
        stop_recording = st.button("â¹ï¸ ë…¹í™” ì¤‘ì§€ & ì €ì¥", type="secondary", use_container_width=True)
    
    # 2. ë…¹í™” ìƒíƒœ í‘œì‹œ (ë²„íŠ¼ ë°”ë¡œ ì•„ë˜)
    status_placeholder = st.empty()
    
    # ë…¹í™” ì „ ìƒíƒœ í‘œì‹œ
    if not st.session_state.webcam_active and not st.session_state.pending_save:
        status_placeholder.info("ë…¹í™” ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”")
    
    # 3. ì›¹ìº  ìº¡ì²˜ ì˜ì—­ (ìƒíƒœ í‘œì‹œ ì•„ë˜) - ê³ ì • í¬ê¸°
    webcam_placeholder = st.empty()
    
    # ë…¹í™” ì‹œì‘ ì „ ëŒ€ê¸° í™”ë©´ í‘œì‹œ (ê³ ì • í¬ê¸° 640x480)
    if not st.session_state.webcam_active and not st.session_state.pending_save:
        waiting_image = np.zeros((480, 640, 3), dtype=np.uint8)
        waiting_image[:] = (50, 50, 50)
        webcam_placeholder.image(waiting_image, channels="BGR", width=640)
    
    # 4. ë‹¤ìš´ë¡œë“œ ì˜ì—­
    download_placeholder = st.empty()

# ìŒì„± í…ìŠ¤íŠ¸ ì˜ì—­
with col_text:
    st.subheader("ğŸ¤ ìŒì„± ì…ë ¥")
    voice_text_placeholder = st.empty()
    
    if st.session_state.recording and st.session_state.voice_recording:
        word_count = len(st.session_state.transcribed_text.split()) if st.session_state.transcribed_text else 0
        current_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì¸ì‹ ì¤‘... ë§ì”€í•´ì£¼ì„¸ìš”)"
        voice_text_placeholder.text_area(
            f"ì…ë ¥ëœ ë‚´ìš© (ë‹¨ì–´: {word_count}ê°œ)",
            value=current_text,
            height=480,
            disabled=True,
            key=f"voice_display_{time.time()}"
        )
    elif st.session_state.transcribed_text:
        word_count = len(st.session_state.transcribed_text.split())
        voice_text_placeholder.text_area(
            f"ì…ë ¥ëœ ë‚´ìš© (ë‹¨ì–´: {word_count}ê°œ)",
            value=st.session_state.transcribed_text,
            height=480,
            disabled=True,
            key="voice_display_saved"
        )
    else:
        voice_text_placeholder.text_area(
            "ì…ë ¥ëœ ë‚´ìš©",
            value="(ìŒì„± ì…ë ¥ ëŒ€ê¸° ì¤‘...)",
            height=480,
            disabled=True,
            key="voice_display_empty"
        )

# ëª¨ë¸ ë¡œë“œ
with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
    emotion_model = load_emotion_model()
    face_detector = load_face_detector()

if emotion_model is None or face_detector is None:
    st.error("âš ï¸ AI ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
    st.stop()

# ë…¹í™” ì‹œì‘ ì²˜ë¦¬
if 'start_recording' in locals() and start_recording:
    if not st.session_state.recording:
        st.session_state.recording = True
        st.session_state.webcam_active = True
        st.session_state.video_frames = []
        st.session_state.emotion_timeline = []
        st.session_state.recording_start_time = datetime.now()
        st.session_state.transcribed_text = ""
        
        # ìŒì„± ì¸ì‹ ì‹œì‘
        st.session_state.voice_recording = True
        st.session_state.audio_queue = queue.Queue()
        st.session_state.stop_event = threading.Event()
        st.session_state.audio_thread = threading.Thread(
            target=record_audio_continuous,
            args=(st.session_state.audio_queue, st.session_state.stop_event)
        )
        st.session_state.audio_thread.daemon = True
        st.session_state.audio_thread.start()
        
        st.rerun()

# ë…¹í™” ì¤‘ì§€ ì²˜ë¦¬
if st.session_state.recording and 'stop_recording' in locals() and stop_recording:
    st.session_state.recording = False
    st.session_state.webcam_active = False
    
    # ìŒì„± ì¸ì‹ ì¤‘ì§€
    if st.session_state.voice_recording:
        st.session_state.stop_event.set()
        st.session_state.voice_recording = False
        time.sleep(0.5)
    
    final_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì…ë ¥ ì—†ìŒ)"
    
    if st.session_state.video_frames and len(st.session_state.video_frames) > 0:
        status_placeholder.info("ğŸ’¾ ì˜ìƒ ì¼ê¸° ì €ì¥ ì¤‘...")
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            video_filename = f"emotion_diary_{timestamp}.mp4"
            text_filename = f"emotion_diary_{timestamp}.txt"
            
            temp_dir = tempfile.gettempdir()
            video_path = os.path.join(temp_dir, video_filename)
            text_path = os.path.join(temp_dir, text_filename)
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì¦‰ì‹œ ì €ì¥
            save_video(st.session_state.video_frames, video_path, fps=20)
            
            # ê°ì • í†µê³„ ê³„ì‚°
            if st.session_state.emotion_timeline:
                emotions_list = [e['emotion'] for e in st.session_state.emotion_timeline]
                emotion_counts = pd.Series(emotions_list).value_counts()
                dominant_emotion = emotion_counts.index[0] if len(emotion_counts) > 0 else "neutral"
                avg_confidence = np.mean([e['confidence'] for e in st.session_state.emotion_timeline])
            else:
                dominant_emotion = "neutral"
                avg_confidence = 0.0
            
            # AI ê¸°ë°˜ ê¸°ë¶„ ì¶”ì²œ
            suggested_moods = suggest_mood_from_data(
                dominant_emotion, 
                final_text, 
                st.session_state.emotion_timeline
            )
            
            # ë…¹í™” ì‹œê°„ ê³„ì‚°
            if st.session_state.recording_start_time:
                elapsed = datetime.now() - st.session_state.recording_start_time
                elapsed_seconds = int(elapsed.total_seconds())
                recording_duration = f"{elapsed_seconds // 60:02d}:{elapsed_seconds % 60:02d}"
            else:
                recording_duration = "00:00"
            
            # ì €ì¥ ë°ì´í„°ë¥¼ ì„¸ì…˜ì— ë³´ê´€
            st.session_state.save_data = {
                'timestamp': timestamp,
                'video_filename': video_filename,
                'video_path': video_path,
                'text_filename': text_filename,
                'text_path': text_path,
                'final_text': final_text,
                'dominant_emotion': dominant_emotion,
                'avg_confidence': avg_confidence,
                'suggested_moods': suggested_moods,
                'frame_count': len(st.session_state.video_frames),
                'recording_duration': recording_duration,
                'emotion_timeline': st.session_state.emotion_timeline.copy(),
                'anonymize_method': anonymize_option
            }
            
            st.session_state.pending_save = True
            st.session_state.video_frames = []
            st.session_state.recording_start_time = None
            
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            import traceback
            st.error(traceback.format_exc())
    else:
        st.warning("âš ï¸ ë…¹í™”ëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤!")
        st.session_state.video_frames = []
        st.session_state.recording_start_time = None

# ê¸°ë¶„ ì„ íƒ UI (pending_save ìƒíƒœì¼ ë•Œ) - col_webcam ì˜ì—­ì—ì„œ í‘œì‹œ
if st.session_state.pending_save and st.session_state.save_data:
    save_data = st.session_state.save_data
    
    with col_webcam:
        # ê°ì • í™•ì • ì „ ë‹¨ê³„
        if not st.session_state.emotion_confirmed:
            status_placeholder.info("âœ¨ ì˜¤ëŠ˜ì˜ ê°ì •ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•©ë‹ˆë‹¤)")
            
            # ê°ì • ì„ íƒ (ë¼ë””ì˜¤ ë²„íŠ¼)
            emotion_options = [
                "ğŸ˜Š Happy (í–‰ë³µ)",
                "ğŸ˜¢ Sad (ìŠ¬í””)",
                "ğŸ˜  Angry (í™”ë‚¨)",
                "ğŸ˜² Surprise (ë†€ëŒ)",
                "ğŸ˜ Neutral (ì¤‘ë¦½)",
                "ğŸ˜¨ Fear (ë‘ë ¤ì›€)",
                "ğŸ¤¢ Disgust (í˜ì˜¤)"
            ]
            
            # AI ì¶”ì²œ ê°ì •ì„ ê¸°ë³¸ ì„ íƒìœ¼ë¡œ ì„¤ì •
            dominant_emotion = save_data['dominant_emotion'].lower()
            emotion_map = {
                'happy': 0,
                'sad': 1,
                'angry': 2,
                'surprise': 3,
                'neutral': 4,
                'fear': 5,
                'disgust': 6,
                'joy': 0  # joyë„ happyë¡œ ë§¤í•‘
            }
            
            default_index = emotion_map.get(dominant_emotion, 4)
            
            selected_emotion = st.radio(
                f"ğŸ­ AI ì¶”ì²œ: **{save_data['dominant_emotion']}**",
                emotion_options,
                index=default_index,
                key="emotion_radio"
            )
            
            # ìƒë‹¨ì˜ "ê°ì • í™•ì •í•˜ê¸°" ë²„íŠ¼ì´ í´ë¦­ë˜ì—ˆì„ ë•Œ ì²˜ë¦¬
            if 'confirm_emotion' in locals() and confirm_emotion:
                # ì„ íƒëœ ê°ì • ì¶”ì¶œ (ì´ëª¨ì§€ì™€ ì˜ë¬¸ëª… ì œê±°)
                final_mood = selected_emotion.split('(')[1].replace(')', '').strip()
                
                # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
                with open(save_data['text_path'], 'w', encoding='utf-8') as f:
                    f.write(f"=== ê°ì • ì˜ìƒ ì¼ê¸° ===\n")
                    f.write(f"ë‚ ì§œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
                    f.write(f"ì˜¤ëŠ˜ì˜ ê°ì •: {final_mood}\n")
                    f.write(f"ìµëª…í™” ë°©ì‹: {save_data['anonymize_method']}\n")
                    f.write(f"\n=== ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥) ===\n\n")
                    f.write(save_data['final_text'])
                    f.write(f"\n\n=== ê°ì • ë¶„ì„ ê²°ê³¼ ===\n")
                    if save_data['emotion_timeline']:
                        emotions_list = [e['emotion'] for e in save_data['emotion_timeline']]
                        emotion_counts = pd.Series(emotions_list).value_counts()
                        f.write(f"ì£¼ìš” ê°ì •: {save_data['dominant_emotion']}\n")
                        f.write(f"í‰ê·  í™•ì‹ ë„: {save_data['avg_confidence']*100:.1f}%\n")
                        f.write(f"\nê°ì • ë¶„í¬:\n")
                        for emotion, count in emotion_counts.items():
                            percentage = (count / len(emotions_list)) * 100
                            f.write(f"  - {emotion}: {count}íšŒ ({percentage:.1f}%)\n")
                    f.write(f"\n=== AI ê°ì • ë¶„ì„ ===\n")
                    f.write(f"ë¶„ì„ëœ ì£¼ìš” ê°ì •: {save_data['dominant_emotion']}\n")
                    f.write(f"ì„ íƒí•œ ê°ì •: {final_mood}\n")
                
                # ì¼ê¸° í•­ëª© ì €ì¥
                entry = {
                    'timestamp': save_data['timestamp'],
                    'emotion': final_mood,
                    'diary_text': save_data['final_text'],
                    'video_filename': save_data['video_filename'],
                    'video_path': save_data['video_path'],
                    'text_filename': save_data['text_filename'],
                    'text_path': save_data['text_path'],
                    'dominant_emotion': save_data['dominant_emotion'],
                    'avg_confidence': save_data['avg_confidence'],
                    'frame_count': save_data['frame_count'],
                    'recording_duration': save_data['recording_duration'],
                    'emotion_timeline': save_data['emotion_timeline'],
                    'anonymize_method': save_data['anonymize_method'],
                    'voice_input_used': True
                }
                
                st.session_state.diary_entries.append(entry)
                st.session_state.confirmed_emotion = final_mood
                st.session_state.emotion_confirmed = True
                
                st.rerun()
        
        # ê°ì • í™•ì • í›„ - ì„¸ì…˜ ê°ì • ë¶„ì„ ë° ë‹¤ìš´ë¡œë“œ
        else:
            status_placeholder.success(f"âœ… ì˜ìƒ ì¼ê¸°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ê°ì •: {st.session_state.confirmed_emotion})")
            
            # ìƒë‹¨ì˜ "ì™„ë£Œ" ë²„íŠ¼ì´ í´ë¦­ë˜ì—ˆì„ ë•Œ ì²˜ë¦¬
            if 'complete_action' in locals() and complete_action:
                # ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.pending_save = False
                st.session_state.save_data = None
                st.session_state.emotion_confirmed = False
                st.session_state.confirmed_emotion = None
                
                st.rerun()
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.subheader(f"ğŸ“¥ íŒŒì¼ ë‹¤ìš´ë¡œë“œ: **{st.session_state.confirmed_emotion}**")
            
            col_dl1, col_dl2 = st.columns(2)
            
            with col_dl1:
                if os.path.exists(save_data['video_path']):
                    with open(save_data['video_path'], 'rb') as f:
                        video_bytes = f.read()
                        st.download_button(
                            label="ğŸ“¥ ì˜ìƒ ì¼ê¸° (MP4)",
                            data=video_bytes,
                            file_name=save_data['video_filename'],
                            mime="video/mp4",
                            type="primary",
                            use_container_width=True,
                            key="download_video_saved"
                        )
                else:
                    st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
            
            with col_dl2:
                if os.path.exists(save_data['text_path']):
                    with open(save_data['text_path'], 'r', encoding='utf-8') as f:
                        text_content = f.read()
                        st.download_button(
                            label="ğŸ“„ ì¼ê¸° í…ìŠ¤íŠ¸ (TXT)",
                            data=text_content,
                            file_name=save_data['text_filename'],
                            mime="text/plain",
                            type="secondary",
                            use_container_width=True,
                            key="download_text_saved"
                        )
                else:
                    st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
            
            # í˜„ì¬ ì„¸ì…˜ ê°ì • ë¶„ì„ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š í˜„ì¬ ì„¸ì…˜ ê°ì • ë¶„ì„")
            
            if save_data['emotion_timeline'] and len(save_data['emotion_timeline']) > 0:
                timeline_df = pd.DataFrame(save_data['emotion_timeline'])
                
                col_chart1, col_chart2 = st.columns(2)
                
                with col_chart1:
                    emotion_counts = timeline_df['emotion'].value_counts()
                    fig_pie = px.pie(
                        values=emotion_counts.values,
                        names=emotion_counts.index,
                        title="ê°ì • ë¶„í¬",
                        color_discrete_sequence=px.colors.qualitative.Set3
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
                
                with col_chart2:
                    # í”„ë ˆì„ì„ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ ë³€í™˜ (20fps ê¸°ì¤€)
                    timeline_df['time_seconds'] = timeline_df['frame'] / 20
                    timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
                    
                    # ì˜ìƒ ì´ ê¸¸ì´ ê³„ì‚°
                    max_time = timeline_df['time_seconds'].max()
                    
                    fig_line = px.line(
                        timeline_df,
                        x='time_seconds',
                        y='confidence_percent',
                        color='emotion',
                        title="í”„ë ˆì„ë³„ ê°ì • ë³€í™” (ì‹œê°„ì¶•)",
                        markers=True,
                        labels={
                            'time_seconds': 'ì˜ìƒ ì‹œê°„ (ì´ˆ)',
                            'confidence_percent': 'í™•ì‹ ë„ (%)',
                            'emotion': 'ê°ì •'
                        }
                    )
                    
                    # Yì¶•ì„ 0~100%ë¡œ ê³ ì •, 10% ë‹¨ìœ„
                    fig_line.update_yaxes(
                        range=[0, 100],
                        dtick=10,
                        title="í™•ì‹ ë„ (%)"
                    )
                    
                    # Xì¶•ì„ ì˜ìƒ ê¸¸ì´ì— ë§ê²Œ 10ì´ˆ ë‹¨ìœ„ë¡œ ì„¤ì •
                    import math
                    x_max = math.ceil(max_time / 10) * 10  # 10ì´ˆ ë‹¨ìœ„ë¡œ ì˜¬ë¦¼
                    fig_line.update_xaxes(
                        range=[0, x_max],
                        dtick=10,  # 10ì´ˆ ë‹¨ìœ„
                        title="ì˜ìƒ ì‹œê°„ (ì´ˆ)"
                    )
                    
                    st.plotly_chart(fig_line, use_container_width=True)
                
                st.subheader("ğŸ“‹ ê°ì • íƒ€ì„ë¼ì¸")
                display_timeline = timeline_df[['frame', 'timestamp', 'emotion', 'confidence']].copy()
                display_timeline['confidence'] = display_timeline['confidence'].apply(lambda x: f"{x*100:.1f}%")
                st.dataframe(display_timeline, use_container_width=True, height=200)

# ìµëª…í™” ë§µí•‘
anonymize_map = {
    "ì›ë³¸": None,
    "ë¸”ëŸ¬": "blur",
    "í”½ì…€í™”": "pixelate",
    "ì¹´íˆ°": "cartoon"
}

# ì›¹ìº  ì‹¤í–‰ (ë…¹í™” ì¤‘ì¼ ë•Œë§Œ)
if st.session_state.webcam_active:
    loading_image = np.zeros((480, 640, 3), dtype=np.uint8)
    loading_image[:] = (50, 50, 50)
    
    webcam_placeholder.image(loading_image, channels="BGR", width=640)
    status_placeholder.info("ğŸ“¹ ì›¹ìº ì„ ì‹œì‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        status_placeholder.error("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ìº ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.session_state.webcam_active = False
    else:
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        init_image = np.zeros((480, 640, 3), dtype=np.uint8)
        init_image[:] = (50, 50, 50)
        
        webcam_placeholder.image(init_image, channels="BGR", width=640)
        status_placeholder.info("ğŸ“¹ ì›¹ìº  ì´ˆê¸°í™” ì¤‘...")
        
        for _ in range(5):
            ret, _ = cap.read()
            if not ret:
                break
            time.sleep(0.1)
        
        ready_image = np.zeros((480, 640, 3), dtype=np.uint8)
        ready_image[:] = (50, 50, 50)
        
        webcam_placeholder.image(ready_image, channels="BGR", width=640)
        status_placeholder.success("âœ… ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ! ğŸ¤ ìŒì„± ì…ë ¥ í™œì„±í™”ë¨!")
        time.sleep(0.5)
        
        frame_count = 0
        
        while st.session_state.webcam_active:
            ret, frame = cap.read()
            
            if not ret:
                st.error("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            text_updated = False
            if st.session_state.voice_recording:
                try:
                    while not st.session_state.audio_queue.empty():
                        new_text = st.session_state.audio_queue.get_nowait()
                        if st.session_state.transcribed_text:
                            st.session_state.transcribed_text += " " + new_text
                        else:
                            st.session_state.transcribed_text = new_text
                        text_updated = True
                except queue.Empty:
                    pass
            
            # ì „ì²´ í”„ë ˆì„ ìµëª…í™” ì ìš©
            anonymized_frame = frame.copy()
            if anonymize_map[anonymize_option] == "blur":
                anonymized_frame = blur_frame(anonymized_frame)
            elif anonymize_map[anonymize_option] == "pixelate":
                anonymized_frame = pixelate_frame(anonymized_frame)
            elif anonymize_map[anonymize_option] == "cartoon":
                anonymized_frame = cartoonize_frame(anonymized_frame)
            
            # ê°ì • ë¶„ì„ (3í”„ë ˆì„ë§ˆë‹¤)
            face_bbox = None
            if frame_count % 3 == 0:
                emotion, confidence, face_bbox = analyze_emotion_quick(
                    frame.copy(), emotion_model, face_detector
                )
                st.session_state.current_emotion = emotion
                
                if st.session_state.recording:
                    st.session_state.emotion_timeline.append({
                        'frame': len(st.session_state.video_frames) + 1,
                        'emotion': emotion,
                        'confidence': confidence,
                        'timestamp': datetime.now().strftime('%H:%M:%S')
                    })
            
            display_frame = anonymized_frame.copy()
            
            # ê°ì • ì˜¤ë²„ë ˆì´ ì¶”ê°€
            if show_emotion_overlay and face_bbox:
                x, y, w, h = face_bbox
                emotion = st.session_state.current_emotion
                
                if frame_count % 3 == 0 and 'confidence' in locals():
                    text = f"{emotion} ({confidence*100:.1f}%)"
                else:
                    text = f"{emotion}"
                
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                
                display_frame = put_korean_text(
                    display_frame,
                    text,
                    (x, y-35),
                    font_size=20,
                    color=(0, 255, 0)
                )
            
            if st.session_state.recording:
                st.session_state.video_frames.append(display_frame)
            
            frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            webcam_placeholder.image(frame_rgb, channels="RGB", width=640)
            
            # ê°ì • ì´ëª¨ì§€
            emotion_emoji = {
                'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 
                'surprise': 'ğŸ˜²', 'neutral': 'ğŸ˜', 'fear': 'ğŸ˜¨',
                'disgust': 'ğŸ¤¢', 'joy': 'ğŸ˜„'
            }
            emoji = emotion_emoji.get(st.session_state.current_emotion.lower(), 'ğŸ˜')
            
            # ë…¹í™” ìƒíƒœ ì—…ë°ì´íŠ¸
            if st.session_state.recording:
                if st.session_state.recording_start_time:
                    elapsed = datetime.now() - st.session_state.recording_start_time
                    elapsed_seconds = int(elapsed.total_seconds())
                    minutes = elapsed_seconds // 60
                    seconds = elapsed_seconds % 60
                    
                    voice_status = ""
                    if st.session_state.voice_recording:
                        word_count = len(st.session_state.transcribed_text.split()) if st.session_state.transcribed_text else 0
                        voice_status = f" | ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ (ë‹¨ì–´: {word_count}ê°œ)"
                    
                    emotion_status = f" | {emoji} {st.session_state.current_emotion}"
                    
                    status_placeholder.success(
                        f"ğŸ”´ ë…¹í™” ì¤‘: {minutes:02d}:{seconds:02d} | í”„ë ˆì„: {len(st.session_state.video_frames)}{emotion_status}{voice_status}"
                    )
            else:
                if len(st.session_state.video_frames) > 0:
                    status_placeholder.warning(f"â¹ï¸ ë…¹í™” ì¤‘ì§€ë¨ ({len(st.session_state.video_frames)} í”„ë ˆì„)")
                else:
                    status_placeholder.info("âšª ëŒ€ê¸° ì¤‘")
            
            frame_count += 1
        
        cap.release()

# ê°ì • ë³€í™” ì‹œê°í™” (ë…¹í™” ì¤‘ì´ê±°ë‚˜ pending_saveê°€ ì•„ë‹ ë•Œë§Œ í‘œì‹œ)
if st.session_state.emotion_timeline and len(st.session_state.emotion_timeline) > 0 and not st.session_state.pending_save:
    st.markdown("---")
    st.subheader("ğŸ“Š í˜„ì¬ ì„¸ì…˜ ê°ì • ë¶„ì„")
    
    timeline_df = pd.DataFrame(st.session_state.emotion_timeline)
    
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        emotion_counts = timeline_df['emotion'].value_counts()
        fig_pie = px.pie(
            values=emotion_counts.values,
            names=emotion_counts.index,
            title="ê°ì • ë¶„í¬",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with col_chart2:
        # í”„ë ˆì„ì„ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ ë³€í™˜ (20fps ê¸°ì¤€)
        timeline_df['time_seconds'] = timeline_df['frame'] / 20
        timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
        
        fig_line = px.line(
            timeline_df,
            x='time_seconds',
            y='confidence_percent',
            color='emotion',
            title="í”„ë ˆì„ë³„ ê°ì • ë³€í™” (ì‹œê°„ì¶•)",
            markers=True,
            labels={
                'time_seconds': 'ì‹œê°„ (ì´ˆ)',
                'confidence_percent': 'í™•ì‹ ë„ (%)',
                'emotion': 'ê°ì •'
            }
        )
        
        # Yì¶•ì„ 0~100%ë¡œ ê³ ì •, 10% ë‹¨ìœ„
        fig_line.update_yaxes(
            range=[0, 100],
            dtick=10,
            title="í™•ì‹ ë„ (%)"
        )
        
        # Xì¶•ì„ ì˜ìƒ ê¸¸ì´ì— ë§ê²Œ ì„¤ì •
        max_time = timeline_df['time_seconds'].max()
        fig_line.update_xaxes(
            range=[0, max_time + 0.5],
            title="ì˜ìƒ ì‹œê°„ (ì´ˆ)"
        )
        
        st.plotly_chart(fig_line, use_container_width=True)
    
    st.subheader("ğŸ“‹ ê°ì • íƒ€ì„ë¼ì¸")
    display_timeline = timeline_df[['frame', 'timestamp', 'emotion', 'confidence']].copy()
    display_timeline['confidence'] = display_timeline['confidence'].apply(lambda x: f"{x*100:.1f}%")
    st.dataframe(display_timeline, use_container_width=True, height=200)

# ì €ì¥ëœ ì¼ê¸° ëª©ë¡
st.markdown("---")
st.subheader("ğŸ“š ì €ì¥ëœ ì˜ìƒ ì¼ê¸°")

if st.session_state.diary_entries:
    for i, entry in enumerate(reversed(st.session_state.diary_entries)):
        emotion_display = f" - ê°ì •: {entry.get('emotion', 'ë¯¸ê¸°ë¡')}" if 'emotion' in entry else ""
        with st.expander(f"ğŸ“” ì¼ê¸° #{len(st.session_state.diary_entries)-i} - {entry['timestamp']}{emotion_display}"):
            col_info1, col_info2 = st.columns(2)
            
            with col_info1:
                if 'emotion' in entry:
                    st.write(f"**âœ¨ ì˜¤ëŠ˜ì˜ ê°ì •:** {entry['emotion']}")
                st.write("**ğŸ“ ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥):**")
                st.write(entry['diary_text'])
                st.write(f"**ğŸ­ ì£¼ìš” ê°ì •:** {entry['dominant_emotion']}")
                st.write(f"**ğŸ“Š í‰ê·  í™•ì‹ ë„:** {entry['avg_confidence']*100:.1f}%")
                st.write("**ğŸ¤ ìŒì„± ì…ë ¥:** ì‚¬ìš©ë¨ âœ“")
            
            with col_info2:
                st.write(f"**ğŸ¬ í”„ë ˆì„ ìˆ˜:** {entry['frame_count']}")
                st.write(f"**ğŸ”’ ìµëª…í™”:** {entry['anonymize_method']}")
                st.write(f"**â±ï¸ ë…¹í™” ì‹œê°„:** {entry.get('recording_duration', '00:00')}")
                st.write(f"**ğŸ“ ì˜ìƒ ê¸¸ì´:** ì•½ {entry['frame_count'] / 20:.1f}ì´ˆ")
                
                col_dl1, col_dl2 = st.columns(2)
                
                with col_dl1:
                    if os.path.exists(entry['video_path']):
                        with open(entry['video_path'], 'rb') as f:
                            video_bytes = f.read()
                            st.download_button(
                                label="ğŸ“¥ ì˜ìƒ",
                                data=video_bytes,
                                file_name=entry['video_filename'],
                                mime="video/mp4",
                                key=f"download_video_{i}",
                                use_container_width=True
                            )
                    else:
                        st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
                
                with col_dl2:
                    if os.path.exists(entry['text_path']):
                        with open(entry['text_path'], 'r', encoding='utf-8') as f:
                            text_content = f.read()
                            st.download_button(
                                label="ğŸ“„ í…ìŠ¤íŠ¸",
                                data=text_content,
                                file_name=entry['text_filename'],
                                mime="text/plain",
                                key=f"download_text_{i}",
                                use_container_width=True
                            )
                    else:
                        st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
            
            if entry.get('emotion_timeline') and len(entry['emotion_timeline']) > 0:
                st.markdown("---")
                st.write("**ğŸ“Š ê°ì • ë¶„ì„**")
                
                timeline_df = pd.DataFrame(entry['emotion_timeline'])
                
                col_chart1, col_chart2 = st.columns(2)
                
                with col_chart1:
                    emotion_counts = timeline_df['emotion'].value_counts()
                    fig_pie = px.pie(
                        values=emotion_counts.values,
                        names=emotion_counts.index,
                        title="ê°ì • ë¶„í¬",
                        color_discrete_sequence=px.colors.qualitative.Set3,
                        height=300
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
                
                with col_chart2:
                    # í”„ë ˆì„ì„ ì‹œê°„(ì´ˆ)ìœ¼ë¡œ ë³€í™˜ (20fps ê¸°ì¤€)
                    timeline_df['time_seconds'] = timeline_df['frame'] / 20
                    timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
                    
                    # ì˜ìƒ ì´ ê¸¸ì´ ê³„ì‚°
                    max_time = timeline_df['time_seconds'].max()
                    
                    fig_line = px.line(
                        timeline_df,
                        x='time_seconds',
                        y='confidence_percent',
                        color='emotion',
                        title="í”„ë ˆì„ë³„ ê°ì • ë³€í™” (ì‹œê°„ì¶•)",
                        markers=True,
                        height=300,
                        labels={
                            'time_seconds': 'ì˜ìƒ ì‹œê°„ (ì´ˆ)',
                            'confidence_percent': 'í™•ì‹ ë„ (%)',
                            'emotion': 'ê°ì •'
                        }
                    )
                    
                    # Yì¶•ì„ 0~100%ë¡œ ê³ ì •, 10% ë‹¨ìœ„
                    fig_line.update_yaxes(
                        range=[0, 100],
                        dtick=10,
                        title="í™•ì‹ ë„ (%)"
                    )
                    
                    # Xì¶•ì„ ì˜ìƒ ê¸¸ì´ì— ë§ê²Œ 10ì´ˆ ë‹¨ìœ„ë¡œ ì„¤ì •
                    import math
                    x_max = math.ceil(max_time / 10) * 10  # 10ì´ˆ ë‹¨ìœ„ë¡œ ì˜¬ë¦¼
                    fig_line.update_xaxes(
                        range=[0, x_max],
                        dtick=10,  # 10ì´ˆ ë‹¨ìœ„
                        title="ì˜ìƒ ì‹œê°„ (ì´ˆ)"
                    )
                    
                    st.plotly_chart(fig_line, use_container_width=True)
else:
    st.info("ğŸ“­ ì•„ì§ ì €ì¥ëœ ì˜ìƒ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë…¹í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")
