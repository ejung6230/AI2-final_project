import streamlit as st
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import pandas as pd
import plotly.express as px
from datetime import datetime
import tempfile
import os
from transformers import pipeline
import mediapipe as mp
import speech_recognition as sr
import threading
import queue
import time
import json
import pickle
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from collections import Counter

# ë¡œì»¬ ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì • (í˜„ì¬ í´ë”ì— ì €ì¥)
DATA_DIR = Path("emotion_diary_data")
DATA_DIR.mkdir(exist_ok=True)

DIARY_DATA_FILE = DATA_DIR / "diary_entries.json"
USER_MODEL_FILE = DATA_DIR / "user_emotion_model.pkl"
USER_STATS_FILE = DATA_DIR / "user_stats.json"
VIDEOS_DIR = DATA_DIR / "videos"
VIDEOS_DIR.mkdir(exist_ok=True)

# ì˜ì–´-í•œê¸€ ê°ì • ë§¤í•‘
EMOTION_TRANSLATION = {
    'happy': 'í–‰ë³µ',
    'sad': 'ìŠ¬í””',
    'angry': 'í™”ë‚¨',
    'surprise': 'ë†€ëŒ',
    'neutral': 'ì¤‘ë¦½',
    'fear': 'ë‘ë ¤ì›€',
    'disgust': 'í˜ì˜¤',
    'joy': 'í–‰ë³µ'  # joyëŠ” happyì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
}

# í•œê¸€-ì˜ì–´ ê°ì • ì—­ë§¤í•‘
EMOTION_REVERSE_TRANSLATION = {v: k for k, v in EMOTION_TRANSLATION.items() if k != 'joy'}

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê°ì • ì¼ê¸° - Emotion Diary",
    page_icon="ğŸ“”",
    layout="wide"
)

# ë¡œì»¬ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_local_data():
    """ë¡œì»¬ì— ì €ì¥ëœ ì¼ê¸° ë°ì´í„° ë¡œë“œ"""
    if DIARY_DATA_FILE.exists():
        try:
            with open(DIARY_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ” í•­ëª©ë§Œ í•„í„°ë§
                valid_entries = []
                for entry in data:
                    if os.path.exists(entry.get('video_path', '')):
                        valid_entries.append(entry)
                return valid_entries
        except Exception as e:
            st.warning(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    return []

def save_local_data(entries):
    """ì¼ê¸° ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì €ì¥"""
    try:
        with open(DIARY_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(entries, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def load_user_stats():
    """ì‚¬ìš©ì í†µê³„ ë¡œë“œ"""
    if USER_STATS_FILE.exists():
        try:
            with open(USER_STATS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {
        'total_entries': 0,
        'emotion_distribution': {},
        'ai_vs_user_agreement': 0,
        'last_updated': None
    }

def save_user_stats(stats):
    """ì‚¬ìš©ì í†µê³„ ì €ì¥"""
    try:
        with open(USER_STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"í†µê³„ ì €ì¥ ì˜¤ë¥˜: {e}")

# ì‚¬ìš©ì ë§ì¶¤ ëª¨ë¸ í´ë˜ìŠ¤
class PersonalizedEmotionModel:
    def __init__(self):
        self.model = None
        self.emotion_mapping = {
            'í–‰ë³µ': 0, 'ìŠ¬í””': 1, 'í™”ë‚¨': 2, 'ë†€ëŒ': 3,
            'ì¤‘ë¦½': 4, 'ë‘ë ¤ì›€': 5, 'í˜ì˜¤': 6
        }
        self.reverse_mapping = {v: k for k, v in self.emotion_mapping.items()}
        self.training_data = []
        self.load_model()
    
    def load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        if USER_MODEL_FILE.exists():
            try:
                with open(USER_MODEL_FILE, 'rb') as f:
                    data = pickle.load(f)
                    self.model = data.get('model')
                    self.training_data = data.get('training_data', [])
                    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.training_data)}ê°œ í•™ìŠµ ë°ì´í„°")
                    return True
            except Exception as e:
                print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            print(f"â„¹ï¸ ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ (ê²½ë¡œ: {USER_MODEL_FILE})")
        return False
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            with open(USER_MODEL_FILE, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'training_data': self.training_data
                }, f)
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {len(self.training_data)}ê°œ í•™ìŠµ ë°ì´í„°")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_features(self, emotion_timeline, text):
        """ê°ì • íƒ€ì„ë¼ì¸ê³¼ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        if not emotion_timeline:
            return None
        
        # ê°ì • ë¶„í¬
        emotions = [e['emotion'] for e in emotion_timeline]
        emotion_counts = Counter(emotions)
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        features = []
        
        # ê° ê°ì •ì˜ ë¹„ìœ¨
        for emotion in ['í–‰ë³µ', 'ìŠ¬í””', 'í™”ë‚¨', 'ë†€ëŒ', 'ì¤‘ë¦½', 'ë‘ë ¤ì›€', 'í˜ì˜¤']:
            features.append(emotion_counts.get(emotion, 0) / len(emotions))
        
        # í‰ê·  í™•ì‹ ë„
        avg_confidence = np.mean([e['confidence'] for e in emotion_timeline])
        features.append(avg_confidence)
        
        # ê°ì • ë³€í™” íšŸìˆ˜ (ê°ì •ì´ ë°”ë€ íšŸìˆ˜)
        emotion_changes = sum(1 for i in range(1, len(emotions)) if emotions[i] != emotions[i-1])
        features.append(emotion_changes / len(emotions))
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´
        text_length = len(text.split()) if text else 0
        features.append(min(text_length / 100, 1.0))  # ì •ê·œí™”
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ê°ì •
        most_common = emotion_counts.most_common(1)[0][0]
        features.append(self.emotion_mapping.get(most_common, 4))
        
        return features
    
    def add_training_sample(self, emotion_timeline, text, ai_emotion, user_emotion):
        """í•™ìŠµ ìƒ˜í”Œ ì¶”ê°€"""
        features = self.extract_features(emotion_timeline, text)
        if features is None:
            print("âŒ í•™ìŠµ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜")
            return False
        
        user_emotion_code = self.emotion_mapping.get(user_emotion, 4)
        
        self.training_data.append({
            'features': features,
            'ai_emotion': ai_emotion,
            'user_emotion': user_emotion_code,
            'timestamp': datetime.now().isoformat()
        })
        
        print(f"âœ… í•™ìŠµ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: ì´ {len(self.training_data)}ê°œ (AI: {ai_emotion}, ì‚¬ìš©ì: {user_emotion})")
        
        # í•™ìŠµ ë°ì´í„° ì¶”ê°€ í›„ í•­ìƒ ì €ì¥
        self.save_model()
        
        return True
    
    def train(self):
        """ëª¨ë¸ í•™ìŠµ"""
        if len(self.training_data) < 3:
            return False
        
        X = [d['features'] for d in self.training_data]
        y = [d['user_emotion'] for d in self.training_data]
        
        self.model = RandomForestClassifier(n_estimators=50, random_state=42)
        self.model.fit(X, y)
        
        self.save_model()
        return True
    
    def predict(self, emotion_timeline, text, ai_emotion):
        """ë§ì¶¤í˜• ê°ì • ì˜ˆì¸¡"""
        if self.model is None or len(self.training_data) < 3:
            # í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ AI ì˜ˆì¸¡ ì‚¬ìš©
            return ai_emotion, 0.0, False
        
        features = self.extract_features(emotion_timeline, text)
        if features is None:
            return ai_emotion, 0.0, False
        
        try:
            prediction = self.model.predict([features])[0]
            probabilities = self.model.predict_proba([features])[0]
            confidence = max(probabilities)
            
            predicted_emotion = self.reverse_mapping.get(prediction, 'ì¤‘ë¦½')
            return predicted_emotion, confidence, True
        except Exception as e:
            print(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return ai_emotion, 0.0, False

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'diary_entries' not in st.session_state:
    st.session_state.diary_entries = load_local_data()
if 'recording' not in st.session_state:
    st.session_state.recording = False
if 'video_frames' not in st.session_state:
    st.session_state.video_frames = []
if 'current_emotion' not in st.session_state:
    st.session_state.current_emotion = 'ì¤‘ë¦½'
if 'emotion_timeline' not in st.session_state:
    st.session_state.emotion_timeline = []
if 'recording_start_time' not in st.session_state:
    st.session_state.recording_start_time = None
if 'webcam_active' not in st.session_state:
    st.session_state.webcam_active = False
if 'voice_recording' not in st.session_state:
    st.session_state.voice_recording = False
if 'transcribed_text' not in st.session_state:
    st.session_state.transcribed_text = ""
if 'audio_queue' not in st.session_state:
    st.session_state.audio_queue = queue.Queue()
if 'pending_save' not in st.session_state:
    st.session_state.pending_save = False
if 'save_data' not in st.session_state:
    st.session_state.save_data = None
if 'emotion_confirmed' not in st.session_state:
    st.session_state.emotion_confirmed = False
if 'confirmed_emotion' not in st.session_state:
    st.session_state.confirmed_emotion = None
if 'personalized_model' not in st.session_state:
    st.session_state.personalized_model = PersonalizedEmotionModel()
if 'user_stats' not in st.session_state:
    st.session_state.user_stats = load_user_stats()

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“¸ ë…¹í™” ì„¤ì •")
    
    anonymize_option = st.selectbox(
        "ì „ì²´ í™”ë©´ ìµëª…í™” ë°©ì‹",
        ["ì›ë³¸", "ë¸”ëŸ¬", "í”½ì…€í™”", "ì¹´íˆ°"],
        key="anonymize",
        disabled=st.session_state.recording
    )
    
    show_emotion_overlay = st.checkbox(
        "ê°ì • ì •ë³´ ì˜¤ë²„ë ˆì´ í‘œì‹œ", 
        value=True,
        disabled=st.session_state.recording
    )
    
    if st.session_state.recording:
        st.warning("âš ï¸ ë…¹í™” ì¤‘ì—ëŠ” ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    st.header("ğŸ¤– AI ê°œì¸í™” ìƒíƒœ")
    
    model_trained = st.session_state.personalized_model.model is not None
    training_count = len(st.session_state.personalized_model.training_data)
    
    if model_trained:
        st.success(f"âœ… ë§ì¶¤ ëª¨ë¸ í™œì„±í™”ë¨")
        st.metric("í•™ìŠµ ë°ì´í„°", f"{training_count}ê°œ")
        
        # ì¼ì¹˜ìœ¨ í‘œì‹œ
        if st.session_state.user_stats['total_entries'] > 0:
            agreement = st.session_state.user_stats['ai_vs_user_agreement']
            st.metric("AI-ì‚¬ìš©ì ì¼ì¹˜ìœ¨", f"{agreement:.1f}%")
    else:
        st.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì¤‘")
        st.metric("ìˆ˜ì§‘ëœ ë°ì´í„°", f"{training_count}/3ê°œ")
        if training_count < 3:
            st.caption(f"ë§ì¶¤ ëª¨ë¸ í™œì„±í™”ê¹Œì§€ {3-training_count}ê°œ í•„ìš”")
    
    st.markdown("---")
    
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    ### ğŸ“¹ ìŒì„± ì˜ìƒ ì¼ê¸° ì‘ì„± ìˆœì„œ
    
    1. ğŸ¨ **ìµëª…í™” ë°©ì‹ ì„ íƒ**
    2. ğŸ”´ **ë…¹í™” ì‹œì‘** í´ë¦­
    3. ğŸ¤ **ë§í•˜ë©° ê°ì • í‘œí˜„**
    4. â¹ï¸ **ë…¹í™” ì¤‘ì§€ & ì €ì¥**
    5. âœ¨ **ì˜¤ëŠ˜ì˜ ê¸°ë¶„ ì„ íƒ**
    6. ğŸ“¥ **ì˜ìƒ & í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ**
    
    ### ğŸ¤– AI ê°œì¸í™” ê¸°ëŠ¥
    - **ìë™ í•™ìŠµ**: ì¼ê¸°ë¥¼ 3ê°œ ì´ìƒ ì‘ì„±í•˜ë©´ ìë™ìœ¼ë¡œ ë§ì¶¤ ëª¨ë¸ì´ í™œì„±í™”ë©ë‹ˆë‹¤
    - **ë§ì¶¤ ì¶”ì²œ**: ì‚¬ìš©ìì˜ ê³¼ê±° ì„ íƒ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ë” ì •í™•í•œ ê°ì •ì„ ì¶”ì²œí•©ë‹ˆë‹¤
    - **ì§€ì† ê°œì„ **: ì¼ê¸°ë¥¼ ì‘ì„±í• ìˆ˜ë¡ AIê°€ ì‚¬ìš©ìë¥¼ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤
    
    ### ğŸ­ ì§€ì› ê°ì •
    - ğŸ˜Š í–‰ë³µ
    - ğŸ˜¢ ìŠ¬í””
    - ğŸ˜  í™”ë‚¨
    - ğŸ˜² ë†€ëŒ
    - ğŸ˜ ì¤‘ë¦½
    - ğŸ˜¨ ë‘ë ¤ì›€
    - ğŸ¤¢ í˜ì˜¤
    
    ### ğŸ”’ ìµëª…í™” ë°©ì‹
    - **ì›ë³¸**: ì–¼êµ´ ê·¸ëŒ€ë¡œ
    - **ë¸”ëŸ¬**: ì „ì²´ í™”ë©´ íë¦¬ê²Œ
    - **í”½ì…€í™”**: ì „ì²´ í™”ë©´ ëª¨ìì´í¬
    - **ì¹´íˆ°**: ì „ì²´ í™”ë©´ ë§Œí™” ìŠ¤íƒ€ì¼
    
    ### ğŸ¤ ìŒì„± ì…ë ¥
    - **ìë™ í™œì„±í™”**: ë…¹í™” ì‹œì‘ ì‹œ ìë™ í™œì„±í™”
    - **í•œêµ­ì–´ ì¸ì‹**: ì‹¤ì‹œê°„ í•œêµ­ì–´ ìŒì„± ì¸ì‹
    - **ì‹¤ì‹œê°„ ë³€í™˜**: ë§í•œ ë‚´ìš©ì„ ì¦‰ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - **ìë™ ì €ì¥**: ì¼ê¸°ë¡œ ìë™ ì €ì¥
    
    ### ğŸ’¡ ì´¬ì˜ íŒ
    - ğŸ’¡ **ë°ì€ ì¡°ëª…** ì‚¬ìš©
    - ğŸ“· **ì •ë©´ ì–¼êµ´** ìœ ì§€
    - ğŸ˜€ **ìì—°ìŠ¤ëŸ¬ìš´ í‘œì •**
    - ğŸ”‡ **ì¡°ìš©í•œ í™˜ê²½** (ìŒì„± ì¸ì‹ ìµœì í™”)
    - ğŸ¤ **ë§ˆì´í¬ ê°€ê¹Œì´**ì—ì„œ ë˜ë ·í•˜ê²Œ ë§í•˜ê¸°
    - ğŸ—£ï¸ **ì²œì²œíˆ ëª…í™•í•˜ê²Œ** ë°œìŒí•˜ê¸°
    
    ### ğŸ’¾ ë°ì´í„° ì €ì¥ ìœ„ì¹˜
    - **ì €ì¥ í´ë”**: `emotion_diary_data/`
    - **ì˜ìƒ íŒŒì¼**: `emotion_diary_data/videos/`
    - ì•± ì‹¤í–‰ í´ë”ì— ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
    - ëª¨ë“  ë°ì´í„°ëŠ” ë¡œì»¬ì— ì•ˆì „í•˜ê²Œ ë³´ê´€ë©ë‹ˆë‹¤
    """)
    
    st.markdown("---")
    
    # í†µê³„
    st.subheader("ğŸ“ˆ ì „ì²´ í†µê³„")
    total_entries = len(st.session_state.diary_entries)
    total_frames = sum([e.get('frame_count', 0) for e in st.session_state.diary_entries])
    voice_entries = len(st.session_state.diary_entries)
    st.metric("ì´ ì¼ê¸° ìˆ˜", total_entries)
    st.metric("ì´ í”„ë ˆì„ ìˆ˜", total_frames)
    st.metric("ìŒì„± ì…ë ¥ ì‚¬ìš©", f"{voice_entries}íšŒ")
    
    # ê°ì • ë¶„í¬ í‘œì‹œ
    if st.session_state.user_stats['emotion_distribution']:
        st.markdown("**ê°ì • ë¶„í¬**")
        for emotion, count in sorted(
            st.session_state.user_stats['emotion_distribution'].items(), 
            key=lambda x: x[1], 
            reverse=True
        ):
            st.caption(f"{emotion}: {count}íšŒ")
    
    st.markdown("---")
    
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì´ˆê¸°í™”", type="secondary"):
        # ë¡œì»¬ íŒŒì¼ ì‚­ì œ
        try:
            if DIARY_DATA_FILE.exists():
                DIARY_DATA_FILE.unlink()
            if USER_MODEL_FILE.exists():
                USER_MODEL_FILE.unlink()
            if USER_STATS_FILE.exists():
                USER_STATS_FILE.unlink()
        except Exception as e:
            st.error(f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.diary_entries = []
        st.session_state.video_frames = []
        st.session_state.emotion_timeline = []
        st.session_state.recording = False
        st.session_state.webcam_active = False
        st.session_state.recording_start_time = None
        st.session_state.voice_recording = False
        st.session_state.transcribed_text = ""
        st.session_state.pending_save = False
        st.session_state.save_data = None
        st.session_state.emotion_confirmed = False
        st.session_state.confirmed_emotion = None
        st.session_state.personalized_model = PersonalizedEmotionModel()
        st.session_state.user_stats = {
            'total_entries': 0,
            'emotion_distribution': {},
            'ai_vs_user_agreement': 0,
            'last_updated': None
        }
        st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        st.rerun()

# í•œê¸€ í°íŠ¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_korean_font(size=40):
    """í•œê¸€ì„ ì§€ì›í•˜ëŠ” í°íŠ¸ ë¡œë“œ"""
    font_paths = [
        "malgun.ttf",
        "C:/Windows/Fonts/malgun.ttf",
        "AppleGothic.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
        "NanumGothic.ttf",
    ]
    
    for font_path in font_paths:
        try:
            return ImageFont.truetype(font_path, size)
        except:
            continue
    
    return ImageFont.load_default()

# í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ì¶”ê°€í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def put_korean_text(image, text, position, font_size=40, color=(255, 255, 255)):
    """OpenCV ì´ë¯¸ì§€ì— í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€"""
    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(pil_image)
    font = load_korean_font(font_size)
    draw.text(position, text, fill=color, font=font)
    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

# ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_emotion_model():
    try:
        return pipeline("image-classification", model="dima806/facial_emotions_image_detection")
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# MediaPipe ì–¼êµ´ ê²€ì¶œ
@st.cache_resource
def load_face_detector():
    try:
        return mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5)
    except Exception as e:
        st.error(f"ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# ì „ì²´ í™”ë©´ ìµëª…í™” í•¨ìˆ˜ë“¤
def blur_frame(image: np.ndarray, strength: int = 51) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ë¸”ëŸ¬ ì²˜ë¦¬"""
    if strength % 2 == 0:
        strength += 1
    return cv2.GaussianBlur(image, (strength, strength), 0)

def pixelate_frame(image: np.ndarray, blocks: int = 16) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ í”½ì…€í™”"""
    h, w = image.shape[:2]
    if h > 0 and w > 0 and h > blocks and w > blocks:
        temp = cv2.resize(image, (blocks, blocks), interpolation=cv2.INTER_LINEAR)
        return cv2.resize(temp, (w, h), interpolation=cv2.INTER_NEAREST)
    return image

def cartoonize_frame(image: np.ndarray) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ì¹´íˆ° ìŠ¤íƒ€ì¼ ë³€í™˜"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        color = cv2.bilateralFilter(image, 9, 250, 250)
        for _ in range(2):
            color = cv2.bilateralFilter(color, 9, 250, 250)
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray = cv2.medianBlur(gray, 7)
        edges = cv2.adaptiveThreshold(
            gray, 255, 
            cv2.ADAPTIVE_THRESH_MEAN_C, 
            cv2.THRESH_BINARY, 
            9, 2
        )
        
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        cartoon = cv2.bitwise_and(color, edges_colored)
        cartoon = cv2.convertScaleAbs(cartoon, alpha=1.2, beta=10)
        
        return cartoon
    except Exception as e:
        print(f"ì¹´íˆ° ë³€í™˜ ì˜¤ë¥˜: {e}")
        return image

# ìŒì„± ì¸ì‹ í•¨ìˆ˜
def record_audio_continuous(audio_queue, stop_event):
    """ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì„ ì¸ì‹í•˜ëŠ” í•¨ìˆ˜"""
    recognizer = sr.Recognizer()
    recognizer.energy_threshold = 4000
    recognizer.dynamic_energy_threshold = True
    
    with sr.Microphone() as source:
        print("ë§ˆì´í¬ ì¡°ì • ì¤‘...")
        recognizer.adjust_for_ambient_noise(source, duration=1)
        print("ìŒì„± ì¸ì‹ ì‹œì‘!")
        
        while not stop_event.is_set():
            try:
                audio = recognizer.listen(source, timeout=1, phrase_time_limit=10)
                
                try:
                    text = recognizer.recognize_google(audio, language='ko-KR')
                    if text:
                        audio_queue.put(text)
                        print(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
                except sr.UnknownValueError:
                    pass
                except sr.RequestError as e:
                    print(f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
                    time.sleep(1)
            except sr.WaitTimeoutError:
                continue
            except Exception as e:
                print(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
                time.sleep(1)

# ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_emotion_quick(image: np.ndarray, model, face_detector) -> tuple[str, float, tuple]:
    """ë¹ ë¥¸ ê°ì • ë¶„ì„ (ì‹¤ì‹œê°„ìš©) - ì–¼êµ´ ìœ„ì¹˜ë§Œ ë°˜í™˜"""
    emotion = 'ì¤‘ë¦½'
    confidence = 0.0
    face_bbox = None
    
    if model is None or face_detector is None:
        return emotion, confidence, face_bbox
    
    try:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            detection = results.detections[0]
            bboxC = detection.location_data.relative_bounding_box
            h, w, _ = image.shape
            x = int(bboxC.xmin * w)
            y = int(bboxC.ymin * h)
            width = int(bboxC.width * w)
            height = int(bboxC.height * h)
            
            x = max(0, x)
            y = max(0, y)
            width = min(width, w - x)
            height = min(height, h - y)
            
            face_bbox = (x, y, width, height)
            
            face = image[y:y+height, x:x+width]
            
            if face.size > 0:
                face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
                emotion_results = model(face_pil)
                
                if emotion_results:
                    # ì˜ì–´ ê°ì •ì„ í•œê¸€ë¡œ ë³€í™˜
                    emotion_eng = emotion_results[0]['label'].lower()
                    emotion = EMOTION_TRANSLATION.get(emotion_eng, 'ì¤‘ë¦½')
                    confidence = emotion_results[0]['score']
    
    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    return emotion, confidence, face_bbox

# ë¹„ë””ì˜¤ ì €ì¥ í•¨ìˆ˜
def save_video(frames: list, filename: str, fps: int = 20):
    """í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
    if not frames or len(frames) == 0:
        return None
    
    height, width, _ = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    
    out = cv2.VideoWriter(filename, fourcc, fps, (width, height))
    
    for frame in frames:
        out.write(frame)
    
    out.release()
    return filename

# ë©”ì¸ UI
st.title("ğŸ“” ê°ì • ì˜ìƒ ì¼ê¸° - Emotion Video Diary")
st.markdown("*ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ ê°ì •ì„ ë¶„ì„í•˜ë©° ìŒì„±ìœ¼ë¡œ ì˜ìƒ ì¼ê¸°ë¥¼ ì‘ì„±í•˜ì„¸ìš”*")

st.markdown("---")

st.subheader("ğŸ“¹ ì›¹ìº  í™”ë©´")

# 1. ë…¹í™” ìƒíƒœ í‘œì‹œ
status_placeholder = st.empty()

# ë…¹í™” ì „ ìƒíƒœ í‘œì‹œ
if not st.session_state.webcam_active and not st.session_state.pending_save:
    status_placeholder.info("ì•„ë˜ ë…¹í™” ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”")

# ë ˆì´ì•„ì›ƒ êµ¬ì„±
if st.session_state.pending_save:
    webcam_placeholder = st.empty()
else:
    col_webcam, col_text = st.columns([2, 1])

    with col_webcam:
        webcam_placeholder = st.empty()
        
        if not st.session_state.webcam_active and not st.session_state.pending_save:
            waiting_image = np.zeros((480, 640, 3), dtype=np.uint8)
            waiting_image[:] = (50, 50, 50)
            webcam_placeholder.image(waiting_image, channels="BGR", width=640)

    with col_text:
        voice_text_placeholder = st.empty()
        
        if st.session_state.recording and st.session_state.voice_recording:
            current_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì¸ì‹ ì¤‘... ë§ì”€í•´ì£¼ì„¸ìš”)"
            voice_text_placeholder.text_area(
                f"ìŒì„± í…ìŠ¤íŠ¸",
                value=current_text,
                height=480,
                disabled=True,
                key=f"voice_display_{time.time()}"
            )
        elif st.session_state.transcribed_text:
            voice_text_placeholder.text_area(
                f"ìŒì„± í…ìŠ¤íŠ¸",
                value=st.session_state.transcribed_text,
                height=480,
                disabled=True,
                key="voice_display_saved"
            )
        else:
            voice_text_placeholder.text_area(
                "ìŒì„± í…ìŠ¤íŠ¸",
                value="(ìŒì„± ì…ë ¥ ëŒ€ê¸° ì¤‘...)",
                height=480,
                disabled=True,
                key="voice_display_empty"
            )

# 3. ë…¹í™” ë²„íŠ¼
if not st.session_state.pending_save:
    if not st.session_state.recording:
        start_recording = st.button("ğŸ”´ ë…¹í™” ì‹œì‘", type="primary", use_container_width=True)
    else:
        start_recording = False
        stop_recording = st.button("â¹ï¸ ë…¹í™” ì¤‘ì§€ & ì €ì¥", type="secondary", use_container_width=True)

# ëª¨ë¸ ë¡œë“œ
with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
    emotion_model = load_emotion_model()
    face_detector = load_face_detector()

if emotion_model is None or face_detector is None:
    st.error("âš ï¸ AI ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
    st.stop()

# ë…¹í™” ì‹œì‘ ì²˜ë¦¬
if 'start_recording' in locals() and start_recording:
    if not st.session_state.recording:
        st.session_state.recording = True
        st.session_state.webcam_active = True
        st.session_state.video_frames = []
        st.session_state.emotion_timeline = []
        st.session_state.recording_start_time = datetime.now()
        st.session_state.transcribed_text = ""
        
        st.session_state.voice_recording = True
        st.session_state.audio_queue = queue.Queue()
        st.session_state.stop_event = threading.Event()
        st.session_state.audio_thread = threading.Thread(
            target=record_audio_continuous,
            args=(st.session_state.audio_queue, st.session_state.stop_event)
        )
        st.session_state.audio_thread.daemon = True
        st.session_state.audio_thread.start()
        
        st.rerun()

# ë…¹í™” ì¤‘ì§€ ì²˜ë¦¬
if st.session_state.recording and 'stop_recording' in locals() and stop_recording:
    st.session_state.recording = False
    st.session_state.webcam_active = False
    
    if st.session_state.voice_recording:
        st.session_state.stop_event.set()
        st.session_state.voice_recording = False
        time.sleep(0.5)
    
    final_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì…ë ¥ ì—†ìŒ)"
    
    if st.session_state.video_frames and len(st.session_state.video_frames) > 0:
        status_placeholder.info("ğŸ’¾ ì˜ìƒ ì¼ê¸° ì €ì¥ ì¤‘...")
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            video_filename = f"emotion_diary_{timestamp}.mp4"
            text_filename = f"emotion_diary_{timestamp}.txt"
            
            # ë¡œì»¬ í´ë”ì— ì €ì¥
            video_path = str(VIDEOS_DIR / video_filename)
            text_path = str(VIDEOS_DIR / text_filename)
            
            save_video(st.session_state.video_frames, video_path, fps=20)
            
            if st.session_state.emotion_timeline:
                emotions_list = [e['emotion'] for e in st.session_state.emotion_timeline]
                emotion_counts = pd.Series(emotions_list).value_counts()
                dominant_emotion = emotion_counts.index[0] if len(emotion_counts) > 0 else "ì¤‘ë¦½"
                avg_confidence = np.mean([e['confidence'] for e in st.session_state.emotion_timeline])
            else:
                dominant_emotion = "ì¤‘ë¦½"
                avg_confidence = 0.0
            
            # ë§ì¶¤í˜• AI ì˜ˆì¸¡
            personalized_emotion, personalized_confidence, is_personalized = \
                st.session_state.personalized_model.predict(
                    st.session_state.emotion_timeline,
                    final_text,
                    dominant_emotion
                )
            
            if st.session_state.recording_start_time:
                elapsed = datetime.now() - st.session_state.recording_start_time
                elapsed_seconds = int(elapsed.total_seconds())
                recording_duration = f"{elapsed_seconds // 60:02d}:{elapsed_seconds % 60:02d}"
            else:
                recording_duration = "00:00"
            
            st.session_state.save_data = {
                'timestamp': timestamp,
                'video_filename': video_filename,
                'video_path': video_path,
                'text_filename': text_filename,
                'text_path': text_path,
                'final_text': final_text,
                'dominant_emotion': dominant_emotion,
                'avg_confidence': avg_confidence,
                'frame_count': len(st.session_state.video_frames),
                'recording_duration': recording_duration,
                'emotion_timeline': st.session_state.emotion_timeline.copy(),
                'anonymize_method': anonymize_option,
                'personalized_emotion': personalized_emotion,
                'personalized_confidence': personalized_confidence,
                'is_personalized': is_personalized
            }
            
            st.session_state.pending_save = True
            st.session_state.video_frames = []
            st.session_state.recording_start_time = None
            
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            import traceback
            st.error(traceback.format_exc())
    else:
        st.warning("âš ï¸ ë…¹í™”ëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤!")
        st.session_state.video_frames = []
        st.session_state.recording_start_time = None

# ê¸°ë¶„ ì„ íƒ UI
if st.session_state.pending_save and st.session_state.save_data:
    save_data = st.session_state.save_data
    
    if not st.session_state.emotion_confirmed:
        # AI ì¶”ì²œ í‘œì‹œ
        if save_data['is_personalized']:
            status_placeholder.success(
                f"âœ¨ ë§ì¶¤í˜• AI ì¶”ì²œ: **{save_data['personalized_emotion']}** "
                f"(í™•ì‹ ë„: {save_data['personalized_confidence']*100:.1f}%)"
            )
            st.info("ğŸ¤– ì‚¬ìš©ìë‹˜ì˜ ê³¼ê±° ê°ì • íŒ¨í„´ì„ ë¶„ì„í•œ ë§ì¶¤ ì¶”ì²œì…ë‹ˆë‹¤!")
        else:
            status_placeholder.info(
                f"âœ¨ AI ì¶”ì²œ: **{save_data['dominant_emotion']}** "
                f"(ê¸°ë³¸ ë¶„ì„ ê²°ê³¼)"
            )
        
        emotion_options = [
            "ğŸ˜Š í–‰ë³µ",
            "ğŸ˜¢ ìŠ¬í””",
            "ğŸ˜  í™”ë‚¨",
            "ğŸ˜² ë†€ëŒ",
            "ğŸ˜ ì¤‘ë¦½",
            "ğŸ˜¨ ë‘ë ¤ì›€",
            "ğŸ¤¢ í˜ì˜¤"
        ]
        
        # ì¶”ì²œ ê°ì •ì„ ê¸°ë³¸ ì„ íƒìœ¼ë¡œ
        recommended_emotion = save_data['personalized_emotion'] if save_data['is_personalized'] else save_data['dominant_emotion']
        emotion_map = {
            'í–‰ë³µ': 0, 'ìŠ¬í””': 1, 'í™”ë‚¨': 2, 'ë†€ëŒ': 3,
            'ì¤‘ë¦½': 4, 'ë‘ë ¤ì›€': 5, 'í˜ì˜¤': 6
        }
        
        default_index = emotion_map.get(recommended_emotion, 4)
        
        selected_emotion = st.radio(
            "ğŸ­ ì˜¤ëŠ˜ì˜ ê°ì •ì„ ì„ íƒí•´ì£¼ì„¸ìš”:",
            emotion_options,
            index=default_index,
            key="emotion_radio"
        )
        
        confirm_emotion = st.button("âœ… ê°ì • í™•ì •í•˜ê¸°", type="primary", use_container_width=True, key="confirm_bottom_btn")
        
        if confirm_emotion:
            # ì„ íƒëœ ê°ì •ì—ì„œ í•œê¸€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            final_mood = selected_emotion.split()[1]
            
            # í•™ìŠµ ë°ì´í„°ì— ì¶”ê°€
            st.session_state.personalized_model.add_training_sample(
                save_data['emotion_timeline'],
                save_data['final_text'],
                save_data['dominant_emotion'],
                final_mood
            )
            
            # ëª¨ë¸ í•™ìŠµ (3ê°œ ì´ìƒì¼ ë•Œ)
            if len(st.session_state.personalized_model.training_data) >= 3:
                with st.spinner("ğŸ¤– ë§ì¶¤í˜• AI í•™ìŠµ ì¤‘..."):
                    if st.session_state.personalized_model.train():
                        st.success("âœ… AI í•™ìŠµ ì™„ë£Œ! ë‹¤ìŒ ì¼ê¸°ë¶€í„° ë” ì •í™•í•œ ì¶”ì²œì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
            with open(save_data['text_path'], 'w', encoding='utf-8') as f:
                f.write(f"=== ê°ì • ì˜ìƒ ì¼ê¸° ===\n")
                f.write(f"ë‚ ì§œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
                f.write(f"ì˜¤ëŠ˜ì˜ ê°ì •: {final_mood}\n")
                f.write(f"ìµëª…í™” ë°©ì‹: {save_data['anonymize_method']}\n")
                f.write(f"\n=== ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥) ===\n\n")
                f.write(save_data['final_text'])
                f.write(f"\n\n=== ê°ì • ë¶„ì„ ê²°ê³¼ ===\n")
                if save_data['emotion_timeline']:
                    emotions_list = [e['emotion'] for e in save_data['emotion_timeline']]
                    emotion_counts = pd.Series(emotions_list).value_counts()
                    f.write(f"ì£¼ìš” ê°ì •: {save_data['dominant_emotion']}\n")
                    f.write(f"í‰ê·  í™•ì‹ ë„: {save_data['avg_confidence']*100:.1f}%\n")
                    f.write(f"\nê°ì • ë¶„í¬:\n")
                    for emotion, count in emotion_counts.items():
                        percentage = (count / len(emotions_list)) * 100
                        f.write(f"  - {emotion}: {count}íšŒ ({percentage:.1f}%)\n")
                f.write(f"\n=== AI ì¶”ì²œ ===\n")
                if save_data['is_personalized']:
                    f.write(f"ë§ì¶¤í˜• AI ì¶”ì²œ: {save_data['personalized_emotion']} (í™•ì‹ ë„: {save_data['personalized_confidence']*100:.1f}%)\n")
                else:
                    f.write(f"ê¸°ë³¸ AI ë¶„ì„: {save_data['dominant_emotion']}\n")
                f.write(f"ìµœì¢… ì„ íƒ: {final_mood}\n")
            
            # ì¼ê¸° í•­ëª© ì €ì¥
            entry = {
                'timestamp': save_data['timestamp'],
                'emotion': final_mood,
                'diary_text': save_data['final_text'],
                'video_filename': save_data['video_filename'],
                'video_path': save_data['video_path'],
                'text_filename': save_data['text_filename'],
                'text_path': save_data['text_path'],
                'dominant_emotion': save_data['dominant_emotion'],
                'avg_confidence': save_data['avg_confidence'],
                'frame_count': save_data['frame_count'],
                'recording_duration': save_data['recording_duration'],
                'emotion_timeline': save_data['emotion_timeline'],
                'anonymize_method': save_data['anonymize_method'],
                'voice_input_used': True,
                'ai_recommended': save_data['personalized_emotion'] if save_data['is_personalized'] else save_data['dominant_emotion'],
                'is_personalized': save_data['is_personalized']
            }
            
            st.session_state.diary_entries.append(entry)
            
            # ë¡œì»¬ì— ì €ì¥
            save_local_data(st.session_state.diary_entries)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            stats = st.session_state.user_stats
            stats['total_entries'] += 1
            stats['emotion_distribution'][final_mood] = stats['emotion_distribution'].get(final_mood, 0) + 1
            
            # AI-ì‚¬ìš©ì ì¼ì¹˜ìœ¨ ê³„ì‚°
            recommended = save_data['personalized_emotion'] if save_data['is_personalized'] else save_data['dominant_emotion']
            matches = sum(1 for e in st.session_state.diary_entries 
                         if e['emotion'] == e['ai_recommended'])
            stats['ai_vs_user_agreement'] = (matches / stats['total_entries']) * 100
            stats['last_updated'] = datetime.now().isoformat()
            
            save_user_stats(stats)
            st.session_state.user_stats = stats
            
            st.session_state.confirmed_emotion = final_mood
            st.session_state.emotion_confirmed = True
            
            st.rerun()
    
    else:
        status_placeholder.success(f"âœ… ì˜ìƒ ì¼ê¸°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ê°ì •: {st.session_state.confirmed_emotion})")
        
        if save_data['emotion_timeline'] and len(save_data['emotion_timeline']) > 0:
            timeline_df = pd.DataFrame(save_data['emotion_timeline'])
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                emotion_counts = timeline_df['emotion'].value_counts()
                fig_pie = px.pie(
                    values=emotion_counts.values,
                    names=emotion_counts.index,
                    title="ê°ì • ë¶„í¬",
                    color_discrete_sequence=px.colors.qualitative.Set3
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col_chart2:
                timeline_df['time_seconds'] = timeline_df['frame'] / 20
                timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
                
                max_time = timeline_df['time_seconds'].max()
                
                fig_line = px.line(
                    timeline_df,
                    x='time_seconds',
                    y='confidence_percent',
                    color='emotion',
                    title="í”„ë ˆì„ë³„ ê°ì • ë³€í™” (ì‹œê°„ì¶•)",
                    markers=True,
                    labels={
                        'time_seconds': 'ì˜ìƒ ì‹œê°„ (ì´ˆ)',
                        'confidence_percent': 'í™•ì‹ ë„ (%)',
                        'emotion': 'ê°ì •'
                    }
                )
                
                fig_line.update_yaxes(range=[0, 100], dtick=10, title="í™•ì‹ ë„ (%)")
                
                import math
                x_max = math.ceil(max_time / 10) * 10
                fig_line.update_xaxes(range=[0, x_max], dtick=10, title="ì˜ìƒ ì‹œê°„ (ì´ˆ)")
                
                st.plotly_chart(fig_line, use_container_width=True)
            
            st.markdown("**ğŸ“‹ ê°ì • íƒ€ì„ë¼ì¸**")
            display_timeline = timeline_df[['frame', 'timestamp', 'emotion', 'confidence']].copy()
            display_timeline['confidence'] = display_timeline['confidence'].apply(lambda x: f"{x*100:.1f}%")
            st.dataframe(display_timeline, use_container_width=True, height=200)
        
        col_dl1, col_dl2 = st.columns(2)
        
        with col_dl1:
            if os.path.exists(save_data['video_path']):
                with open(save_data['video_path'], 'rb') as f:
                    video_bytes = f.read()
                    st.download_button(
                        label="ğŸ“¥ ì˜ìƒ ì¼ê¸° (MP4) íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=video_bytes,
                        file_name=save_data['video_filename'],
                        mime="video/mp4",
                        type="secondary",
                        use_container_width=True,
                        key="download_video_saved"
                    )
            else:
                st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
        
        with col_dl2:
            if os.path.exists(save_data['text_path']):
                with open(save_data['text_path'], 'r', encoding='utf-8') as f:
                    text_content = f.read()
                    st.download_button(
                        label="ğŸ“„ ì¼ê¸° í…ìŠ¤íŠ¸ (TXT) íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=text_content,
                        file_name=save_data['text_filename'],
                        mime="text/plain",
                        type="secondary",
                        use_container_width=True,
                        key="download_text_saved"
                    )
            else:
                st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
        
        complete_action = st.button("âœ… í™•ì¸ ì™„ë£Œ", type="primary", use_container_width=True, key="complete_bottom_btn")
        
        if complete_action:
            st.session_state.pending_save = False
            st.session_state.save_data = None
            st.session_state.emotion_confirmed = False
            st.session_state.confirmed_emotion = None
            
            st.rerun()

# ìµëª…í™” ë§µí•‘
anonymize_map = {
    "ì›ë³¸": None,
    "ë¸”ëŸ¬": "blur",
    "í”½ì…€í™”": "pixelate",
    "ì¹´íˆ°": "cartoon"
}

# ì›¹ìº  ì‹¤í–‰
if st.session_state.webcam_active:
    loading_image = np.zeros((480, 640, 3), dtype=np.uint8)
    loading_image[:] = (50, 50, 50)
    
    webcam_placeholder.image(loading_image, channels="BGR", width=640)
    status_placeholder.info("ğŸ“¹ ì›¹ìº ì„ ì‹œì‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        status_placeholder.error("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ìº ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.session_state.webcam_active = False
    else:
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        init_image = np.zeros((480, 640, 3), dtype=np.uint8)
        init_image[:] = (50, 50, 50)
        
        webcam_placeholder.image(init_image, channels="BGR", width=640)
        status_placeholder.info("ğŸ“¹ ì›¹ìº  ì´ˆê¸°í™” ì¤‘...")
        
        for _ in range(5):
            ret, _ = cap.read()
            if not ret:
                break
            time.sleep(0.1)
        
        ready_image = np.zeros((480, 640, 3), dtype=np.uint8)
        ready_image[:] = (50, 50, 50)
        
        webcam_placeholder.image(ready_image, channels="BGR", width=640)
        status_placeholder.success("âœ… ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ! ğŸ¤ ìŒì„± ì…ë ¥ í™œì„±í™”ë¨!")
        time.sleep(0.5)
        
        frame_count = 0
        
        while st.session_state.webcam_active:
            ret, frame = cap.read()
            
            if not ret:
                st.error("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            text_updated = False
            if st.session_state.voice_recording:
                try:
                    while not st.session_state.audio_queue.empty():
                        new_text = st.session_state.audio_queue.get_nowait()
                        if st.session_state.transcribed_text:
                            st.session_state.transcribed_text += " " + new_text
                        else:
                            st.session_state.transcribed_text = new_text
                        text_updated = True
                except queue.Empty:
                    pass
            
            anonymized_frame = frame.copy()
            if anonymize_map[anonymize_option] == "blur":
                anonymized_frame = blur_frame(anonymized_frame)
            elif anonymize_map[anonymize_option] == "pixelate":
                anonymized_frame = pixelate_frame(anonymized_frame)
            elif anonymize_map[anonymize_option] == "cartoon":
                anonymized_frame = cartoonize_frame(anonymized_frame)
            
            face_bbox = None
            if frame_count % 3 == 0:
                emotion, confidence, face_bbox = analyze_emotion_quick(
                    frame.copy(), emotion_model, face_detector
                )
                st.session_state.current_emotion = emotion
                
                if st.session_state.recording:
                    st.session_state.emotion_timeline.append({
                        'frame': len(st.session_state.video_frames) + 1,
                        'emotion': emotion,
                        'confidence': confidence,
                        'timestamp': datetime.now().strftime('%H:%M:%S')
                    })
            
            display_frame = anonymized_frame.copy()
            
            if show_emotion_overlay and face_bbox:
                x, y, w, h = face_bbox
                emotion = st.session_state.current_emotion
                
                if frame_count % 3 == 0 and 'confidence' in locals():
                    text = f"{emotion} ({confidence*100:.1f}%)"
                else:
                    text = f"{emotion}"
                
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                
                display_frame = put_korean_text(
                    display_frame,
                    text,
                    (x, y-35),
                    font_size=20,
                    color=(0, 255, 0)
                )
            
            if st.session_state.recording:
                st.session_state.video_frames.append(display_frame)
            
            frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            webcam_placeholder.image(frame_rgb, channels="RGB", width=640)
            
            emotion_emoji = {
                'í–‰ë³µ': 'ğŸ˜Š', 'ìŠ¬í””': 'ğŸ˜¢', 'í™”ë‚¨': 'ğŸ˜ ', 
                'ë†€ëŒ': 'ğŸ˜²', 'ì¤‘ë¦½': 'ğŸ˜', 'ë‘ë ¤ì›€': 'ğŸ˜¨',
                'í˜ì˜¤': 'ğŸ¤¢'
            }
            emoji = emotion_emoji.get(st.session_state.current_emotion, 'ğŸ˜')
            
            if st.session_state.recording:
                if st.session_state.recording_start_time:
                    elapsed = datetime.now() - st.session_state.recording_start_time
                    elapsed_seconds = int(elapsed.total_seconds())
                    minutes = elapsed_seconds // 60
                    seconds = elapsed_seconds % 60
                    
                    voice_status = ""
                    if st.session_state.voice_recording:
                        word_count = len(st.session_state.transcribed_text.split()) if st.session_state.transcribed_text else 0
                        voice_status = f" | ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ (ë‹¨ì–´: {word_count}ê°œ)"
                    
                    emotion_status = f" | {emoji} {st.session_state.current_emotion}"
                    
                    status_placeholder.success(
                        f"ğŸ”´ ë…¹í™” ì¤‘: {minutes:02d}:{seconds:02d} | í”„ë ˆì„: {len(st.session_state.video_frames)}{emotion_status}{voice_status}"
                    )
            else:
                if len(st.session_state.video_frames) > 0:
                    status_placeholder.warning(f"â¹ï¸ ë…¹í™” ì¤‘ì§€ë¨ ({len(st.session_state.video_frames)} í”„ë ˆì„)")
                else:
                    status_placeholder.info("âšª ëŒ€ê¸° ì¤‘")
            
            frame_count += 1
        
        cap.release()

# ì €ì¥ëœ ì¼ê¸° ëª©ë¡
st.markdown("---")
st.subheader("ğŸ“š ì €ì¥ëœ ì˜ìƒ ì¼ê¸°")
if st.session_state.diary_entries:
    # ê°œë³„ ì¼ê¸°
    for i, entry in enumerate(reversed(st.session_state.diary_entries)):
        emotion_display = f" - ê°ì •: {entry.get('emotion', 'ë¯¸ê¸°ë¡')}" if 'emotion' in entry else ""
        ai_rec = entry.get('ai_recommended', 'ì—†ìŒ')
        is_personalized = "AI ë§ì¶¤ ì¶”ì²œ" if entry.get('is_personalized', False) else "AI ê¸°ë³¸ ì¶”ì²œ"
        
        with st.expander(f"ğŸ“” ì¼ê¸° #{len(st.session_state.diary_entries)-i} - {entry['timestamp']}{emotion_display} ({is_personalized}: {ai_rec})"):
            col_info1, col_info2 = st.columns(2)
            
            with col_info1:
                if 'emotion' in entry:
                    st.write(f"**âœ¨ ì˜¤ëŠ˜ì˜ ê°ì •:** {entry['emotion']}")
                st.write(f"**ğŸ¤– {is_personalized}:** {ai_rec}")
                if entry['emotion'] == ai_rec:
                    st.success("âœ… AI ì¶”ì²œê³¼ ì¼ì¹˜")
                else:
                    st.info("â„¹ï¸ ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ê°ì • ì„ íƒ")
                
                st.write("**ğŸ“ ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥):**")
                st.write(entry['diary_text'])
                st.write(f"**ğŸ­ ì£¼ìš” ê°ì •:** {entry['dominant_emotion']}")
                st.write(f"**ğŸ“Š í‰ê·  í™•ì‹ ë„:** {entry['avg_confidence']*100:.1f}%")
                st.write("**ğŸ¤ ìŒì„± ì…ë ¥:** ì‚¬ìš©ë¨ âœ“")
            
            with col_info2:
                st.write(f"**ğŸ¬ í”„ë ˆì„ ìˆ˜:** {entry['frame_count']}")
                st.write(f"**ğŸ”’ ìµëª…í™”:** {entry['anonymize_method']}")
                st.write(f"**â±ï¸ ë…¹í™” ì‹œê°„:** {entry.get('recording_duration', '00:00')}")
                st.write(f"**ğŸ“ ì˜ìƒ ê¸¸ì´:** ì•½ {entry['frame_count'] / 20:.1f}ì´ˆ")
                
                col_dl1, col_dl2 = st.columns(2)
                
                with col_dl1:
                    if os.path.exists(entry['video_path']):
                        with open(entry['video_path'], 'rb') as f:
                            video_bytes = f.read()
                            st.download_button(
                                label="ğŸ“¥ ì˜ìƒ",
                                data=video_bytes,
                                file_name=entry['video_filename'],
                                mime="video/mp4",
                                key=f"download_video_{i}",
                                use_container_width=True
                            )
                    else:
                        st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
                
                with col_dl2:
                    if os.path.exists(entry['text_path']):
                        with open(entry['text_path'], 'r', encoding='utf-8') as f:
                            text_content = f.read()
                            st.download_button(
                                label="ğŸ“„ í…ìŠ¤íŠ¸",
                                data=text_content,
                                file_name=entry['text_filename'],
                                mime="text/plain",
                                key=f"download_text_{i}",
                                use_container_width=True
                            )
                    else:
                        st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
            
            if entry.get('emotion_timeline') and len(entry['emotion_timeline']) > 0:
                st.markdown("---")
                st.write("**ğŸ“Š AI ê°ì • ë¶„ì„**")
                
                timeline_df = pd.DataFrame(entry['emotion_timeline'])
                
                col_chart1, col_chart2 = st.columns(2)
                
                with col_chart1:
                    emotion_counts = timeline_df['emotion'].value_counts()
                    fig_pie = px.pie(
                        values=emotion_counts.values,
                        names=emotion_counts.index,
                        title="ê°ì • ë¶„í¬",
                        color_discrete_sequence=px.colors.qualitative.Set3,
                        height=300
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
                
                with col_chart2:
                    timeline_df['time_seconds'] = timeline_df['frame'] / 20
                    timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
                    
                    max_time = timeline_df['time_seconds'].max()
                    
                    fig_line = px.line(
                        timeline_df,
                        x='time_seconds',
                        y='confidence_percent',
                        color='emotion',
                        title="í”„ë ˆì„ë³„ ê°ì • ë³€í™” (ì‹œê°„ì¶•)",
                        markers=True,
                        height=300,
                        labels={
                            'time_seconds': 'ì˜ìƒ ì‹œê°„ (ì´ˆ)',
                            'confidence_percent': 'í™•ì‹ ë„ (%)',
                            'emotion': 'ê°ì •'
                        }
                    )
                    
                    fig_line.update_yaxes(range=[0, 100], dtick=10, title="í™•ì‹ ë„ (%)")
                    
                    import math
                    x_max = math.ceil(max_time / 10) * 10
                    fig_line.update_xaxes(range=[0, x_max], dtick=10, title="ì˜ìƒ ì‹œê°„ (ì´ˆ)")
                    
                    st.plotly_chart(fig_line, use_container_width=True)
    
    # ì „ì²´ í†µê³„
    st.markdown("---")
    st.markdown("### ğŸ“Š ì „ì²´ ê°ì • ë¶„ì„")
    
    all_emotions = [e['emotion'] for e in st.session_state.diary_entries]
    emotion_df = pd.DataFrame({'ê°ì •': all_emotions})
    
    col_stat1, col_stat2 = st.columns(2)
    
    with col_stat1:
        emotion_counts = emotion_df['ê°ì •'].value_counts()
        fig_overall = px.pie(
            values=emotion_counts.values,
            names=emotion_counts.index,
            title="ì „ì²´ ê°ì • ë¶„í¬",
            color_discrete_sequence=px.colors.qualitative.Pastel
        )
        st.plotly_chart(fig_overall, use_container_width=True)
    
    with col_stat2:
        # AI ì¶”ì²œ vs ì‚¬ìš©ì ì„ íƒ ë¹„êµ
        ai_matches = sum(1 for e in st.session_state.diary_entries 
                        if e['emotion'] == e.get('ai_recommended', ''))
        match_rate = (ai_matches / len(st.session_state.diary_entries)) * 100
        
        st.metric("AI-ì‚¬ìš©ì ì¼ì¹˜ìœ¨", f"{match_rate:.1f}%")
        
        personalized_count = sum(1 for e in st.session_state.diary_entries 
                                if e.get('is_personalized', False))
        st.metric("ë§ì¶¤í˜• ì¶”ì²œ ì‚¬ìš©", f"{personalized_count}íšŒ")
        st.metric("í•™ìŠµ ë°ì´í„°", f"{len(st.session_state.personalized_model.training_data)}ê°œ")
else:
    st.info("ğŸ“­ ì•„ì§ ì €ì¥ëœ ì˜ìƒ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë…¹í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")