import streamlit as st
import cv2
import numpy as np
from PIL import Image
import pandas as pd
import plotly.express as px
from datetime import datetime
import tempfile
import os
from transformers import pipeline
import mediapipe as mp
import speech_recognition as sr
import threading
import queue
import time

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê°ì • ì¼ê¸° - Emotion Diary",
    page_icon="ğŸ“”",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'emotion_history' not in st.session_state:
    st.session_state.emotion_history = []
if 'diary_entries' not in st.session_state:
    st.session_state.diary_entries = []
if 'recording' not in st.session_state:
    st.session_state.recording = False
if 'video_frames' not in st.session_state:
    st.session_state.video_frames = []
if 'current_emotion' not in st.session_state:
    st.session_state.current_emotion = 'neutral'
if 'emotion_timeline' not in st.session_state:
    st.session_state.emotion_timeline = []
if 'recording_start_time' not in st.session_state:
    st.session_state.recording_start_time = None
if 'webcam_active' not in st.session_state:
    st.session_state.webcam_active = False
if 'last_saved_entry' not in st.session_state:
    st.session_state.last_saved_entry = None
if 'voice_recording' not in st.session_state:
    st.session_state.voice_recording = False
if 'transcribed_text' not in st.session_state:
    st.session_state.transcribed_text = ""
if 'audio_queue' not in st.session_state:
    st.session_state.audio_queue = queue.Queue()
if 'show_emotion_chart' not in st.session_state:
    st.session_state.show_emotion_chart = False

# ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_emotion_model():
    try:
        return pipeline("image-classification", model="dima806/facial_emotions_image_detection")
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# MediaPipe ì–¼êµ´ ê²€ì¶œ
@st.cache_resource
def load_face_detector():
    try:
        return mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5)
    except Exception as e:
        st.error(f"ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# ì „ì²´ í™”ë©´ ìµëª…í™” í•¨ìˆ˜ë“¤
def blur_frame(image: np.ndarray, strength: int = 51) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ë¸”ëŸ¬ ì²˜ë¦¬"""
    if strength % 2 == 0:
        strength += 1
    return cv2.GaussianBlur(image, (strength, strength), 0)

def pixelate_frame(image: np.ndarray, blocks: int = 16) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ í”½ì…€í™”"""
    h, w = image.shape[:2]
    if h > 0 and w > 0 and h > blocks and w > blocks:
        # ì¶•ì†Œ
        temp = cv2.resize(image, (blocks, blocks), interpolation=cv2.INTER_LINEAR)
        # í™•ëŒ€ (í”½ì…€ íš¨ê³¼)
        return cv2.resize(temp, (w, h), interpolation=cv2.INTER_NEAREST)
    return image

def cartoonize_frame(image: np.ndarray) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ì¹´íˆ° ìŠ¤íƒ€ì¼ ë³€í™˜"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        # 1. ìƒ‰ìƒ ë‹¨ìˆœí™” (bilateral filterë¥¼ ì—¬ëŸ¬ ë²ˆ ì ìš©)
        color = cv2.bilateralFilter(image, 9, 250, 250)
        for _ in range(2):
            color = cv2.bilateralFilter(color, 9, 250, 250)
        
        # 2. ì—ì§€ ê²€ì¶œ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray = cv2.medianBlur(gray, 7)
        edges = cv2.adaptiveThreshold(
            gray, 255, 
            cv2.ADAPTIVE_THRESH_MEAN_C, 
            cv2.THRESH_BINARY, 
            9, 2
        )
        
        # 3. ì—ì§€ë¥¼ 3ì±„ë„ë¡œ ë³€í™˜
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        
        # 4. ì¹´íˆ° íš¨ê³¼ ìƒì„±
        cartoon = cv2.bitwise_and(color, edges_colored)
        
        # 5. ë°ê¸° ì¡°ì • (ì¹´íˆ° ëŠë‚Œ ê°•í™”)
        cartoon = cv2.convertScaleAbs(cartoon, alpha=1.2, beta=10)
        
        return cartoon
    except Exception as e:
        print(f"ì¹´íˆ° ë³€í™˜ ì˜¤ë¥˜: {e}")
        return image

# ìŒì„± ì¸ì‹ í•¨ìˆ˜
def record_audio_continuous(audio_queue, stop_event):
    """ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì„ ì¸ì‹í•˜ëŠ” í•¨ìˆ˜"""
    recognizer = sr.Recognizer()
    recognizer.energy_threshold = 4000
    recognizer.dynamic_energy_threshold = True
    
    with sr.Microphone() as source:
        print("ë§ˆì´í¬ ì¡°ì • ì¤‘...")
        recognizer.adjust_for_ambient_noise(source, duration=1)
        print("ìŒì„± ì¸ì‹ ì‹œì‘!")
        
        while not stop_event.is_set():
            try:
                # ì§§ì€ timeoutìœ¼ë¡œ ìì£¼ í™•ì¸
                audio = recognizer.listen(source, timeout=1, phrase_time_limit=10)
                
                try:
                    # í•œêµ­ì–´ ì¸ì‹
                    text = recognizer.recognize_google(audio, language='ko-KR')
                    if text:
                        audio_queue.put(text)
                        print(f"ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
                except sr.UnknownValueError:
                    pass  # ìŒì„±ì„ ì¸ì‹í•˜ì§€ ëª»í•¨
                except sr.RequestError as e:
                    print(f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
                    time.sleep(1)
            except sr.WaitTimeoutError:
                continue  # timeoutì´ë©´ ê³„ì† ì§„í–‰
            except Exception as e:
                print(f"ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
                time.sleep(1)

# ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_emotion_quick(image: np.ndarray, model, face_detector) -> tuple[str, float, tuple]:
    """ë¹ ë¥¸ ê°ì • ë¶„ì„ (ì‹¤ì‹œê°„ìš©) - ì–¼êµ´ ìœ„ì¹˜ë§Œ ë°˜í™˜"""
    emotion = "neutral"
    confidence = 0.0
    face_bbox = None
    
    if model is None or face_detector is None:
        return emotion, confidence, face_bbox
    
    try:
        # ì–¼êµ´ ê²€ì¶œ
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            detection = results.detections[0]
            bboxC = detection.location_data.relative_bounding_box
            h, w, _ = image.shape
            x = int(bboxC.xmin * w)
            y = int(bboxC.ymin * h)
            width = int(bboxC.width * w)
            height = int(bboxC.height * h)
            
            # ê²½ê³„ ì²´í¬
            x = max(0, x)
            y = max(0, y)
            width = min(width, w - x)
            height = min(height, h - y)
            
            face_bbox = (x, y, width, height)
            
            # ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
            face = image[y:y+height, x:x+width]
            
            if face.size > 0:
                # ê°ì • ë¶„ì„
                face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
                emotion_results = model(face_pil)
                
                if emotion_results:
                    emotion = emotion_results[0]['label']
                    confidence = emotion_results[0]['score']
    
    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    return emotion, confidence, face_bbox

# ë¹„ë””ì˜¤ ì €ì¥ í•¨ìˆ˜
def save_video(frames: list, filename: str, fps: int = 20):
    """í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
    if not frames or len(frames) == 0:
        return None
    
    height, width, _ = frames[0].shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    
    out = cv2.VideoWriter(filename, fourcc, fps, (width, height))
    
    for frame in frames:
        out.write(frame)
    
    out.release()
    return filename

# ë©”ì¸ UI
st.title("ğŸ“” ê°ì • ì˜ìƒ ì¼ê¸° - Emotion Video Diary")
st.markdown("*ì›¹ìº ìœ¼ë¡œ ì‹¤ì‹œê°„ ê°ì •ì„ ë¶„ì„í•˜ë©° ì˜ìƒ ì¼ê¸°ë¥¼ ì‘ì„±í•˜ì„¸ìš”*")

# ìŒì„± ì¸ì‹ëœ í…ìŠ¤íŠ¸ í‘œì‹œ
if st.session_state.transcribed_text:
    st.info(f"ğŸ¤ ìŒì„±ìœ¼ë¡œ ì…ë ¥ëœ í…ìŠ¤íŠ¸: {st.session_state.transcribed_text}")

st.subheader("âœï¸ ì˜¤ëŠ˜ì˜ ì¼ê¸°")
diary_text = st.text_area(
    "ì˜¤ëŠ˜ì˜ ê°ì •ê³¼ ìƒê°ì„ ììœ ë¡­ê²Œ ì ì–´ë³´ì„¸ìš” (ë˜ëŠ” ìŒì„±ìœ¼ë¡œ ì…ë ¥)", 
    value=st.session_state.transcribed_text,
    height=150, 
    key="diary_input"
)

# ì›¹ìº  ìŠ¤íŠ¸ë¦¬ë°
st.markdown("---")

# ë ˆì´ì•„ì›ƒ êµ¬ì„±
col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("ğŸ“¸ ë…¹í™” ì„¤ì •")
    anonymize_option = st.selectbox(
        "ì „ì²´ í™”ë©´ ìµëª…í™” ë°©ì‹",
        ["ì›ë³¸", "ë¸”ëŸ¬", "í”½ì…€í™”", "ì¹´íˆ°"],
        key="anonymize",
        disabled=st.session_state.recording  # ë…¹í™” ì¤‘ì—ëŠ” ë³€ê²½ ë¶ˆê°€
    )
    
    show_emotion_overlay = st.checkbox(
        "ê°ì • ì •ë³´ ì˜¤ë²„ë ˆì´ í‘œì‹œ", 
        value=True,
        disabled=st.session_state.recording  # ë…¹í™” ì¤‘ì—ëŠ” ë³€ê²½ ë¶ˆê°€
    )
    
    # ìŒì„± ì…ë ¥ì€ í•­ìƒ í™œì„±í™”
    use_voice_input = True
    
    if st.session_state.recording:
        st.warning("âš ï¸ ë…¹í™” ì¤‘ì—ëŠ” ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ’¡ ìŒì„± ì…ë ¥ì´ ìë™ìœ¼ë¡œ í™œì„±í™”ë©ë‹ˆë‹¤. ë…¹í™” ì¤‘ ë§í•œ ë‚´ìš©ì´ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.")
    
    st.info("""
    ğŸ’¡ **ì‚¬ìš© ë°©ë²•:**
    1. ìµëª…í™” ë°©ì‹ ì„ íƒ
    2. 'ğŸ”´ ë…¹í™” ì‹œì‘' í´ë¦­
    3. ê°ì • í‘œí˜„í•˜ë©° ì´ì•¼ê¸°
    4. 'â¹ï¸ ë…¹í™” ì¤‘ì§€ & ì €ì¥' í´ë¦­
    5. ì˜ìƒ & í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ
    """)

with col2:
    st.subheader("ğŸ“¹ ì›¹ìº  í™”ë©´")
    
    if not st.session_state.recording:
        start_recording = st.button("ğŸ”´ ë…¹í™” ì‹œì‘", type="primary", use_container_width=True)
    else:
        start_recording = False
        stop_recording = st.button("â¹ï¸ ë…¹í™” ì¤‘ì§€ & ì €ì¥", type="secondary", use_container_width=True)
    
    # ë…¹í™” ìƒíƒœ í‘œì‹œ
    status_placeholder = st.empty()
    emotion_display = st.empty()

    # ëª¨ë¸ ë¡œë“œ
    with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
        emotion_model = load_emotion_model()
        face_detector = load_face_detector()

    if emotion_model is None or face_detector is None:
        st.error("âš ï¸ AI ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
        st.stop()

    # ë…¹í™” ì‹œì‘ ì²˜ë¦¬
    if 'start_recording' in locals() and start_recording:
        if not st.session_state.recording:
            st.session_state.recording = True
            st.session_state.webcam_active = True
            st.session_state.video_frames = []
            st.session_state.emotion_timeline = []
            st.session_state.recording_start_time = datetime.now()
            st.session_state.transcribed_text = ""
            st.session_state.show_emotion_chart = False
            
            # ìŒì„± ì¸ì‹ ì‹œì‘
            if use_voice_input:
                st.session_state.voice_recording = True
                st.session_state.audio_queue = queue.Queue()
                st.session_state.stop_event = threading.Event()
                st.session_state.audio_thread = threading.Thread(
                    target=record_audio_continuous,
                    args=(st.session_state.audio_queue, st.session_state.stop_event)
                )
                st.session_state.audio_thread.daemon = True
                st.session_state.audio_thread.start()
            
            st.success("ğŸ”´ ë…¹í™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()

    # ë…¹í™” ì¤‘ì§€ ì²˜ë¦¬
    if st.session_state.recording and 'stop_recording' in locals() and stop_recording:
        st.session_state.recording = False
        st.session_state.webcam_active = False
        
        # ìŒì„± ì¸ì‹ ì¤‘ì§€
        if st.session_state.voice_recording:
            with st.spinner("ğŸ¤ ìŒì„± ì¸ì‹ ì¢…ë£Œ ì¤‘..."):
                st.session_state.stop_event.set()
                st.session_state.voice_recording = False
                time.sleep(0.5)  # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        
        # ìµœì¢… í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ë‚´ìš©ì´ ì—†ì–´ë„ ì €ì¥ ê°€ëŠ¥)
        final_text = diary_text if diary_text else st.session_state.transcribed_text
        if not final_text:
            final_text = "(ì¼ê¸° ë‚´ìš© ì—†ìŒ)"  # ë¹ˆ ë‚´ìš©ì¼ ê²½ìš° ê¸°ë³¸ í…ìŠ¤íŠ¸
        
        # ì¦‰ì‹œ ë¹„ë””ì˜¤ ì €ì¥ (í”„ë ˆì„ë§Œ ìˆìœ¼ë©´ ì €ì¥)
        if st.session_state.video_frames:
            # ì§„í–‰ë¥  í‘œì‹œ ì˜ì—­ ìƒì„±
            save_container = st.container()
            
            with save_container:
                save_status = st.empty()
                progress_text = st.empty()
                progress_bar = st.progress(0)
                
                save_status.info("ğŸ’¾ ì˜ìƒ ì¼ê¸° ì €ì¥ ì‹œì‘...")
                
                try:
                    # ë¹„ë””ì˜¤ ì €ì¥
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    video_filename = f"emotion_diary_{timestamp}.mp4"
                    text_filename = f"emotion_diary_{timestamp}.txt"
                    
                    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
                    temp_dir = tempfile.gettempdir()
                    video_path = os.path.join(temp_dir, video_filename)
                    text_path = os.path.join(temp_dir, text_filename)
                    
                    # 1ë‹¨ê³„: ë¹„ë””ì˜¤ ì¸ì½”ë”©
                    progress_text.text("ğŸ“¹ ë¹„ë””ì˜¤ ì¸ì½”ë”© ì¤‘... (1/3)")
                    progress_bar.progress(10)
                    time.sleep(0.2)
                    
                    save_video(st.session_state.video_frames, video_path, fps=20)
                    progress_bar.progress(40)
                    time.sleep(0.2)
                    
                    # 2ë‹¨ê³„: ê°ì • ë¶„ì„
                    progress_text.text("ğŸ­ ê°ì • ë°ì´í„° ë¶„ì„ ì¤‘... (2/3)")
                    progress_bar.progress(50)
                    time.sleep(0.2)
                    
                    # ê°ì • í†µê³„ ê³„ì‚°
                    if st.session_state.emotion_timeline:
                        emotions_list = [e['emotion'] for e in st.session_state.emotion_timeline]
                        emotion_counts = pd.Series(emotions_list).value_counts()
                        dominant_emotion = emotion_counts.index[0] if len(emotion_counts) > 0 else "neutral"
                        avg_confidence = np.mean([e['confidence'] for e in st.session_state.emotion_timeline])
                    else:
                        dominant_emotion = "neutral"
                        avg_confidence = 0.0
                    
                    progress_bar.progress(70)
                    time.sleep(0.2)
                    
                    # 3ë‹¨ê³„: í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
                    progress_text.text("ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì¤‘... (3/3)")
                    progress_bar.progress(80)
                    time.sleep(0.2)
                    
                    # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
                    with open(text_path, 'w', encoding='utf-8') as f:
                        f.write(f"=== ê°ì • ì˜ìƒ ì¼ê¸° ===\n")
                        f.write(f"ë‚ ì§œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
                        f.write(f"ìµëª…í™” ë°©ì‹: {anonymize_option}\n")
                        f.write(f"\n=== ì¼ê¸° ë‚´ìš© ===\n\n")
                        f.write(final_text)
                        f.write(f"\n\n=== ê°ì • ë¶„ì„ ê²°ê³¼ ===\n")
                        if st.session_state.emotion_timeline:
                            emotions_list = [e['emotion'] for e in st.session_state.emotion_timeline]
                            emotion_counts = pd.Series(emotions_list).value_counts()
                            f.write(f"ì£¼ìš” ê°ì •: {emotion_counts.index[0] if len(emotion_counts) > 0 else 'neutral'}\n")
                            f.write(f"í‰ê·  í™•ì‹ ë„: {np.mean([e['confidence'] for e in st.session_state.emotion_timeline])*100:.1f}%\n")
                            f.write(f"\nê°ì • ë¶„í¬:\n")
                            for emotion, count in emotion_counts.items():
                                percentage = (count / len(emotions_list)) * 100
                                f.write(f"  - {emotion}: {count}íšŒ ({percentage:.1f}%)\n")
                    
                    progress_bar.progress(90)
                    time.sleep(0.2)
                    
                    # ë…¹í™” ì‹œê°„ ê³„ì‚°
                    if st.session_state.recording_start_time:
                        elapsed = datetime.now() - st.session_state.recording_start_time
                        elapsed_seconds = int(elapsed.total_seconds())
                        recording_duration = f"{elapsed_seconds // 60:02d}:{elapsed_seconds % 60:02d}"
                    else:
                        recording_duration = "00:00"
                    
                    # ì¼ê¸° í•­ëª© ì €ì¥
                    entry = {
                        'timestamp': timestamp,
                        'diary_text': final_text,
                        'video_filename': video_filename,
                        'video_path': video_path,
                        'text_filename': text_filename,
                        'text_path': text_path,
                        'dominant_emotion': dominant_emotion,
                        'avg_confidence': avg_confidence,
                        'frame_count': len(st.session_state.video_frames),
                        'recording_duration': recording_duration,
                        'emotion_timeline': st.session_state.emotion_timeline.copy(),
                        'anonymize_method': anonymize_option,
                        'voice_input_used': use_voice_input
                    }
                    
                    st.session_state.diary_entries.append(entry)
                    st.session_state.last_saved_entry = entry
                    st.session_state.show_emotion_chart = True  # ê°ì • ì°¨íŠ¸ í‘œì‹œ í”Œë˜ê·¸
                    
                    progress_bar.progress(100)
                    time.sleep(0.3)
                    
                    # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ (í•˜ë‚˜ë§Œ í‘œì‹œ)
                    save_status.success(f"âœ… ì˜ìƒ ì¼ê¸°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! ({len(st.session_state.video_frames)} í”„ë ˆì„, {recording_duration})")
                    progress_text.empty()
                    progress_bar.empty()
                    
                    st.balloons()
                    
                    # ë…¹í™” ìƒíƒœ ì´ˆê¸°í™” (ê°ì • íƒ€ì„ë¼ì¸ì€ ìœ ì§€)
                    st.session_state.video_frames = []
                    st.session_state.recording_start_time = None
                    
                    # ì ì‹œ ëŒ€ê¸° í›„ rerun (ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡)
                    time.sleep(1.5)
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                    import traceback
                    st.error(traceback.format_exc())
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ìƒíƒœ ì´ˆê¸°í™”
                    st.session_state.video_frames = []
                    st.session_state.recording_start_time = None
        else:
            st.warning("âš ï¸ ë…¹í™”ëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤! ë…¹í™”ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
            
            # ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.video_frames = []
            st.session_state.recording_start_time = None
            time.sleep(1.0)
            st.rerun()

    # ìµëª…í™” ë§µí•‘
    anonymize_map = {
        "ì›ë³¸": None,
        "ë¸”ëŸ¬": "blur",
        "í”½ì…€í™”": "pixelate",
        "ì¹´íˆ°": "cartoon"
    }

    # ì›¹ìº  ìº¡ì²˜ ì˜ì—­
    FRAME_WINDOW = st.image([])

    # ì›¹ìº  ì‹¤í–‰ (ë…¹í™” ì¤‘ì¼ ë•Œë§Œ)
    if st.session_state.webcam_active:
        # ì›¹ìº  ë¡œë”© ë©”ì‹œì§€ë¥¼ status_placeholderì— í‘œì‹œ
        status_placeholder.info("ğŸ“¹ ì›¹ìº ì„ ì‹œì‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
        
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            status_placeholder.error("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ìº ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            st.session_state.webcam_active = False
        else:
            # ì›¹ìº  í•´ìƒë„ ì„¤ì •
            cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            cap.set(cv2.CAP_PROP_FPS, 30)
            
            # ì²« í”„ë ˆì„ì´ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            status_placeholder.info("ğŸ“¹ ì›¹ìº  ì´ˆê¸°í™” ì¤‘... ì²« í”„ë ˆì„ ëŒ€ê¸° ì¤‘...")
            
            # ì›¹ìº ì´ ì•ˆì •í™”ë  ë•Œê¹Œì§€ ëª‡ í”„ë ˆì„ ìŠ¤í‚µ
            for _ in range(5):
                ret, _ = cap.read()
                if not ret:
                    break
                time.sleep(0.1)
            
            # ì¤€ë¹„ ì™„ë£Œ ë©”ì‹œì§€
            status_placeholder.success("âœ… ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ!")
            time.sleep(0.5)
            
            frame_count = 0
            
            while st.session_state.webcam_active:
                ret, frame = cap.read()
                
                if not ret:
                    st.error("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ìŒì„± ì¸ì‹ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                if st.session_state.voice_recording:
                    try:
                        while not st.session_state.audio_queue.empty():
                            new_text = st.session_state.audio_queue.get_nowait()
                            if st.session_state.transcribed_text:
                                st.session_state.transcribed_text += " " + new_text
                            else:
                                st.session_state.transcribed_text = new_text
                    except queue.Empty:
                        pass
                
                # ì „ì²´ í”„ë ˆì„ ìµëª…í™” ì ìš© (ê°ì • ë¶„ì„ ì „ì—)
                anonymized_frame = frame.copy()
                if anonymize_map[anonymize_option] == "blur":
                    anonymized_frame = blur_frame(anonymized_frame)
                elif anonymize_map[anonymize_option] == "pixelate":
                    anonymized_frame = pixelate_frame(anonymized_frame)
                elif anonymize_map[anonymize_option] == "cartoon":
                    anonymized_frame = cartoonize_frame(anonymized_frame)
                
                # ê°ì • ë¶„ì„ (ì›ë³¸ í”„ë ˆì„ìœ¼ë¡œ, 3í”„ë ˆì„ë§ˆë‹¤ - ì„±ëŠ¥ ìµœì í™”)
                face_bbox = None
                if frame_count % 3 == 0:
                    emotion, confidence, face_bbox = analyze_emotion_quick(
                        frame.copy(), emotion_model, face_detector
                    )
                    st.session_state.current_emotion = emotion
                    
                    # ë…¹í™” ì¤‘ì´ë©´ ê°ì • íƒ€ì„ë¼ì¸ ì—…ë°ì´íŠ¸
                    if st.session_state.recording:
                        st.session_state.emotion_timeline.append({
                            'frame': len(st.session_state.video_frames) + 1,
                            'emotion': emotion,
                            'confidence': confidence,
                            'timestamp': datetime.now().strftime('%H:%M:%S')
                        })
                
                # ìµœì¢… í‘œì‹œìš© í”„ë ˆì„ (ìµëª…í™”ëœ í”„ë ˆì„)
                display_frame = anonymized_frame.copy()
                
                # ê°ì • ì˜¤ë²„ë ˆì´ ì¶”ê°€ (ìµëª…í™”ëœ í”„ë ˆì„ ìœ„ì—)
                if show_emotion_overlay and face_bbox:
                    x, y, w, h = face_bbox
                    
                    emotion_emoji = {
                        'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 
                        'surprise': 'ğŸ˜²', 'neutral': 'ğŸ˜', 'fear': 'ğŸ˜¨',
                        'disgust': 'ğŸ¤¢', 'joy': 'ğŸ˜„'
                    }
                    emotion = st.session_state.current_emotion
                    emoji = emotion_emoji.get(emotion.lower(), 'ğŸ˜')
                    
                    # ê°ì • ë¶„ì„ ì •ë³´ê°€ ìˆìœ¼ë©´ confidenceë„ í‘œì‹œ
                    if frame_count % 3 == 0 and 'confidence' in locals():
                        text = f"{emoji} {emotion} ({confidence*100:.1f}%)"
                    else:
                        text = f"{emoji} {emotion}"
                    
                    # ì˜¤ë²„ë ˆì´ ì¶”ê°€
                    cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
                    cv2.rectangle(display_frame, (x, y-30), (x + text_width, y), (0, 255, 0), -1)
                    cv2.putText(display_frame, text, (x, y-10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                # ë…¹í™” ì¤‘ì´ë©´ ìµëª…í™”ëœ í”„ë ˆì„ ì €ì¥
                if st.session_state.recording:
                    st.session_state.video_frames.append(display_frame)
                
                # í™”ë©´ì— í‘œì‹œ
                frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
                FRAME_WINDOW.image(frame_rgb, channels="RGB")
                
                # ê°ì • í‘œì‹œ
                emotion_emoji = {
                    'happy': 'ğŸ˜Š', 'sad': 'ğŸ˜¢', 'angry': 'ğŸ˜ ', 
                    'surprise': 'ğŸ˜²', 'neutral': 'ğŸ˜', 'fear': 'ğŸ˜¨',
                    'disgust': 'ğŸ¤¢', 'joy': 'ğŸ˜„'
                }
                emoji = emotion_emoji.get(st.session_state.current_emotion.lower(), 'ğŸ˜')
                emotion_display.metric(
                    "í˜„ì¬ ê°ì •", 
                    f"{emoji} {st.session_state.current_emotion}"
                )
                
                # ë…¹í™” ìƒíƒœ ì—…ë°ì´íŠ¸ (ìŒì„± ì¸ì‹ ìƒíƒœë¥¼ í•¨ê»˜ í‘œì‹œ)
                if st.session_state.recording:
                    if st.session_state.recording_start_time:
                        elapsed = datetime.now() - st.session_state.recording_start_time
                        elapsed_seconds = int(elapsed.total_seconds())
                        minutes = elapsed_seconds // 60
                        seconds = elapsed_seconds % 60
                        
                        # ìŒì„± ì¸ì‹ ìƒíƒœ ì¶”ê°€
                        voice_status = ""
                        if st.session_state.voice_recording:
                            word_count = len(st.session_state.transcribed_text.split()) if st.session_state.transcribed_text else 0
                            voice_status = f" | ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ (ë‹¨ì–´: {word_count}ê°œ)"
                        
                        status_placeholder.success(
                            f"ğŸ”´ ë…¹í™” ì¤‘: {minutes:02d}:{seconds:02d} | í”„ë ˆì„: {len(st.session_state.video_frames)}{voice_status}"
                        )
                else:
                    if len(st.session_state.video_frames) > 0:
                        status_placeholder.warning(f"â¹ï¸ ë…¹í™” ì¤‘ì§€ë¨ ({len(st.session_state.video_frames)} í”„ë ˆì„)")
                    else:
                        status_placeholder.info("âšª ëŒ€ê¸° ì¤‘")
                
                frame_count += 1
            
            cap.release()

# ë°©ê¸ˆ ì €ì¥ëœ ì˜ìƒ ë‹¤ìš´ë¡œë“œ
if st.session_state.last_saved_entry:
    st.markdown("---")
    st.subheader("ğŸ“¥ ë°©ê¸ˆ ì €ì¥ëœ ì˜ìƒ ì¼ê¸°")
    
    entry = st.session_state.last_saved_entry
    
    col_result1, col_result2 = st.columns(2)
    
    with col_result1:
        st.write("**ğŸ“Š ë…¹í™” ì •ë³´:**")
        st.metric("â±ï¸ ë…¹í™” ì‹œê°„", entry['recording_duration'])
        st.metric("ğŸ“¹ ì´ í”„ë ˆì„ ìˆ˜", f"{entry['frame_count']} í”„ë ˆì„")
        st.metric("ğŸ­ ì£¼ìš” ê°ì •", entry['dominant_emotion'])
        st.metric("ğŸ“ˆ í‰ê·  í™•ì‹ ë„", f"{entry['avg_confidence']*100:.1f}%")
        if entry.get('voice_input_used'):
            st.metric("ğŸ¤ ìŒì„± ì…ë ¥", "ì‚¬ìš©ë¨")
    
    with col_result2:
        st.write("**ğŸ“¥ ë‹¤ìš´ë¡œë“œ:**")
        
        # ì˜ìƒ ë‹¤ìš´ë¡œë“œ
        if os.path.exists(entry['video_path']):
            with open(entry['video_path'], 'rb') as f:
                video_bytes = f.read()
                st.download_button(
                    label="ğŸ“¥ ì˜ìƒ ì¼ê¸° ë‹¤ìš´ë¡œë“œ (MP4)",
                    data=video_bytes,
                    file_name=entry['video_filename'],
                    mime="video/mp4",
                    type="primary",
                    use_container_width=True,
                    key="download_latest_video"
                )
        
        # í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ
        if os.path.exists(entry['text_path']):
            with open(entry['text_path'], 'r', encoding='utf-8') as f:
                text_content = f.read()
                st.download_button(
                    label="ğŸ“„ ì¼ê¸° í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ (TXT)",
                    data=text_content,
                    file_name=entry['text_filename'],
                    mime="text/plain",
                    type="secondary",
                    use_container_width=True,
                    key="download_latest_text"
                )
        
        if st.button("âœ… í™•ì¸ ì™„ë£Œ", type="secondary", use_container_width=True):
            st.session_state.last_saved_entry = None
            st.session_state.show_emotion_chart = False
            st.rerun()

# ê°ì • ë³€í™” ì‹œê°í™” (ë…¹í™” ì™„ë£Œ í›„ì—ë§Œ í‘œì‹œ)
if st.session_state.show_emotion_chart and st.session_state.emotion_timeline and len(st.session_state.emotion_timeline) > 0:
    st.markdown("---")
    st.subheader("ğŸ“Š í˜„ì¬ ì„¸ì…˜ ê°ì • ë¶„ì„")
    
    timeline_df = pd.DataFrame(st.session_state.emotion_timeline)
    
    col_chart1, col_chart2 = st.columns(2)
    
    with col_chart1:
        # ê°ì • ë¶„í¬
        emotion_counts = timeline_df['emotion'].value_counts()
        fig_pie = px.pie(
            values=emotion_counts.values,
            names=emotion_counts.index,
            title="ê°ì • ë¶„í¬",
            color_discrete_sequence=px.colors.qualitative.Set3
        )
        st.plotly_chart(fig_pie, use_container_width=True)
    
    with col_chart2:
        # ì‹œê°„ì— ë”°ë¥¸ ê°ì • ë³€í™”
        fig_line = px.line(
            timeline_df,
            x='frame',
            y='confidence',
            color='emotion',
            title="í”„ë ˆì„ë³„ ê°ì • ë³€í™”",
            markers=True
        )
        st.plotly_chart(fig_line, use_container_width=True)
    
    # ê°ì • íƒ€ì„ë¼ì¸ í…Œì´ë¸”
    st.subheader("ğŸ“‹ ê°ì • íƒ€ì„ë¼ì¸")
    display_timeline = timeline_df[['frame', 'timestamp', 'emotion', 'confidence']].copy()
    display_timeline['confidence'] = display_timeline['confidence'].apply(lambda x: f"{x*100:.1f}%")
    st.dataframe(display_timeline, use_container_width=True, height=200)

# ì €ì¥ëœ ì¼ê¸° ëª©ë¡
st.markdown("---")
st.subheader("ğŸ“š ì €ì¥ëœ ì˜ìƒ ì¼ê¸°")

if st.session_state.diary_entries:
    for i, entry in enumerate(reversed(st.session_state.diary_entries)):
        with st.expander(f"ğŸ“” ì¼ê¸° #{len(st.session_state.diary_entries)-i} - {entry['timestamp']}"):
            col_info1, col_info2 = st.columns(2)
            
            with col_info1:
                st.write("**ğŸ“ ì¼ê¸° ë‚´ìš©:**")
                st.write(entry['diary_text'] if entry['diary_text'] else "_ë‚´ìš© ì—†ìŒ_")
                st.write(f"**ğŸ­ ì£¼ìš” ê°ì •:** {entry['dominant_emotion']}")
                st.write(f"**ğŸ“Š í‰ê·  í™•ì‹ ë„:** {entry['avg_confidence']*100:.1f}%")
                if entry.get('voice_input_used'):
                    st.write("**ğŸ¤ ìŒì„± ì…ë ¥:** ì‚¬ìš©ë¨")
            
            with col_info2:
                st.write(f"**ğŸ¬ í”„ë ˆì„ ìˆ˜:** {entry['frame_count']}")
                st.write(f"**ğŸ”’ ìµëª…í™”:** {entry['anonymize_method']}")
                st.write(f"**â±ï¸ ë…¹í™” ì‹œê°„:** {entry.get('recording_duration', '00:00')}")
                st.write(f"**ğŸ“ ì˜ìƒ ê¸¸ì´:** ì•½ {entry['frame_count'] / 20:.1f}ì´ˆ")
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ë“¤
                col_dl1, col_dl2 = st.columns(2)
                
                with col_dl1:
                    # ë¹„ë””ì˜¤ ë‹¤ìš´ë¡œë“œ
                    if os.path.exists(entry['video_path']):
                        with open(entry['video_path'], 'rb') as f:
                            video_bytes = f.read()
                            st.download_button(
                                label="ğŸ“¥ ì˜ìƒ",
                                data=video_bytes,
                                file_name=entry['video_filename'],
                                mime="video/mp4",
                                key=f"download_video_{i}",
                                use_container_width=True
                            )
                    else:
                        st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
                
                with col_dl2:
                    # í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ
                    if os.path.exists(entry['text_path']):
                        with open(entry['text_path'], 'r', encoding='utf-8') as f:
                            text_content = f.read()
                            st.download_button(
                                label="ğŸ“„ í…ìŠ¤íŠ¸",
                                data=text_content,
                                file_name=entry['text_filename'],
                                mime="text/plain",
                                key=f"download_text_{i}",
                                use_container_width=True
                            )
                    else:
                        st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
else:
    st.info("ğŸ“­ ì•„ì§ ì €ì¥ëœ ì˜ìƒ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë…¹í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    ### ğŸ“¹ ì˜ìƒ ì¼ê¸° ì‘ì„± ìˆœì„œ
    
    1. ğŸ¨ **ìµëª…í™” ë°©ì‹ ì„ íƒ**
    2. ğŸ”´ **ë…¹í™” ì‹œì‘** í´ë¦­
    3. ğŸ˜Š **ê°ì • í‘œí˜„í•˜ë©° ì´ì•¼ê¸°**
    4. â¹ï¸ **ë…¹í™” ì¤‘ì§€ & ì €ì¥**
    5. ğŸ“Š **ê°ì • ë¶„ì„ í™•ì¸**
    6. ğŸ“¥ **ì˜ìƒ & í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ**
    
    ### ğŸ­ ì§€ì› ê°ì •
    - ğŸ˜Š Happy (í–‰ë³µ)
    - ğŸ˜¢ Sad (ìŠ¬í””)
    - ğŸ˜  Angry (í™”ë‚¨)
    - ğŸ˜² Surprise (ë†€ëŒ)
    - ğŸ˜ Neutral (ì¤‘ë¦½)
    - ğŸ˜¨ Fear (ë‘ë ¤ì›€)
    - ğŸ¤¢ Disgust (í˜ì˜¤)
    
    ### ğŸ”’ ìµëª…í™” ë°©ì‹
    - **ì›ë³¸**: ì–¼êµ´ ê·¸ëŒ€ë¡œ
    - **ë¸”ëŸ¬**: ì „ì²´ í™”ë©´ íë¦¬ê²Œ
    - **í”½ì…€í™”**: ì „ì²´ í™”ë©´ ëª¨ìì´í¬
    - **ì¹´íˆ°**: ì „ì²´ í™”ë©´ ë§Œí™” ìŠ¤íƒ€ì¼
    
    ### ğŸ¤ ìŒì„± ì…ë ¥
    - í•œêµ­ì–´ ìŒì„± ì¸ì‹
    - ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë³€í™˜
    - ìë™ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ
    
    ### ğŸ’¡ ì´¬ì˜ íŒ
    - ğŸ’¡ ë°ì€ ì¡°ëª… ì‚¬ìš©
    - ğŸ“· ì •ë©´ ì–¼êµ´ ìœ ì§€
    - ğŸ˜€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œì •
    - ğŸ”‡ ì¡°ìš©í•œ í™˜ê²½ (ìŒì„± ì…ë ¥ ì‹œ)
    - ğŸ¤ ë§ˆì´í¬ ê°€ê¹Œì´ì—ì„œ ë§í•˜ê¸°
    
    ### ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ
    - Python 3.12
    - Streamlit
    - OpenCV
    - MediaPipe
    - Hugging Face
    - SpeechRecognition
    - Plotly
    
    """)
    
    st.markdown("---")
    
    # í†µê³„
    st.subheader("ğŸ“ˆ ì „ì²´ í†µê³„")
    total_entries = len(st.session_state.diary_entries)
    total_frames = sum([e['frame_count'] for e in st.session_state.diary_entries])
    voice_entries = sum([1 for e in st.session_state.diary_entries if e.get('voice_input_used')])
    st.metric("ì´ ì¼ê¸° ìˆ˜", total_entries)
    st.metric("ì´ í”„ë ˆì„ ìˆ˜", total_frames)
    st.metric("ìŒì„± ì…ë ¥ ì‚¬ìš©", f"{voice_entries}íšŒ")
    
    st.markdown("---")
    
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì´ˆê¸°í™”", type="secondary"):
        st.session_state.emotion_history = []
        st.session_state.diary_entries = []
        st.session_state.video_frames = []
        st.session_state.emotion_timeline = []
        st.session_state.recording = False
        st.session_state.webcam_active = False
        st.session_state.recording_start_time = None
        st.session_state.last_saved_entry = None
        st.session_state.voice_recording = False
        st.session_state.transcribed_text = ""
        st.session_state.show_emotion_chart = False
        st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        st.rerun()