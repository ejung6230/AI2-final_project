import streamlit as st
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import pandas as pd
import plotly.express as px
from datetime import datetime
import os
from transformers import pipeline
import mediapipe as mp
import speech_recognition as sr
import threading
import queue
import time
import json
import pickle
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from collections import Counter

# ì œë¯¸ë‚˜ì´ SDK ì„í¬íŠ¸
try:
    from google import genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False
    print("âš ï¸ google-genai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ë°©ë²•: pip install google-genai")

# ì œë¯¸ë‚˜ì´ API ì„¤ì •
GEMINI_API_KEY = "AIzaSyDdOJZsmnmTjuC0Uc--j1ZKhXsXtUxvR2I"

# ë¡œì»¬ ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì • (í˜„ì¬ í´ë”ì— ì €ì¥)
DATA_DIR = Path("emotion_diary_data")
DATA_DIR.mkdir(exist_ok=True)

DIARY_DATA_FILE = DATA_DIR / "diary_entries.json"
USER_MODEL_FILE = DATA_DIR / "user_emotion_model.pkl"
USER_STATS_FILE = DATA_DIR / "user_stats.json"
VIDEOS_DIR = DATA_DIR / "videos"
VIDEOS_DIR.mkdir(exist_ok=True)

# ì˜ì–´-í•œê¸€ ê°ì • ë§¤í•‘
EMOTION_TRANSLATION = {
    'happy': 'í–‰ë³µí•¨',
    'sad': 'ìŠ¬í””',
    'angry': 'í™”ë‚¨',
    'surprise': 'ë†€ëŒ',
    'neutral': 'ë‹´ë‹´í•¨',
    'fear': 'ë‘ë ¤ì›€',
    'disgust': 'í˜ì˜¤',
    'joy': 'í–‰ë³µí•¨'  # joyëŠ” happyì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
}

# ê°ì • ì´ëª¨ì§€ ë§¤í•‘
emotion_emoji_map = {
    'í–‰ë³µí•¨': 'ğŸ˜Š',
    'ìŠ¬í””': 'ğŸ˜¢',
    'í™”ë‚¨': 'ğŸ˜ ',
    'ë†€ëŒ': 'ğŸ˜²',
    'ë‹´ë‹´í•¨': 'ğŸ˜',
    'ë‘ë ¤ì›€': 'ğŸ˜¨',
    'í˜ì˜¤': 'ğŸ¤¢'
}

# í•œê¸€-ì˜ì–´ ê°ì • ì—­ë§¤í•‘
EMOTION_REVERSE_TRANSLATION = {v: k for k, v in EMOTION_TRANSLATION.items() if k != 'joy'}

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ê°ì • ì¼ê¸° - Emotion Diary",
    page_icon="ğŸ“”",
    layout="wide"
)

# ì œë¯¸ë‚˜ì´ API í˜¸ì¶œ í•¨ìˆ˜
def get_gemini_advice(emotion: str, diary_text: str, emotion_timeline: list) -> str:
    """ì œë¯¸ë‚˜ì´ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê°ì • ì¼ê¸°ì— ëŒ€í•œ ì¡°ì–¸ì„ ë°›ìŠµë‹ˆë‹¤"""
    
    # SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
    if not GEMINI_AVAILABLE:
        return f"âš ï¸ ì œë¯¸ë‚˜ì´ SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\n\nì„¤ì¹˜ ë°©ë²•: pip install google-genai\n\ní•˜ì§€ë§Œ ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”! ì˜¤ëŠ˜ '{emotion}' ê°ì •ì„ ì†”ì§í•˜ê²Œ ê¸°ë¡í•˜ì‹  ê²ƒë§Œìœ¼ë¡œë„ í›Œë¥­í•©ë‹ˆë‹¤. ê°ì • ì¼ê¸°ëŠ” ìê¸° ì„±ì°°ì˜ ì†Œì¤‘í•œ ë„êµ¬ì…ë‹ˆë‹¤. ğŸŒˆ"
    
    try:
        # ê°ì • ë¶„í¬ ê³„ì‚°
        emotions_list = [e['emotion'] for e in emotion_timeline] if emotion_timeline else []
        emotion_counts = Counter(emotions_list)
        
        # ì£¼ìš” ê°ì •ë§Œ ì¶”ì¶œ (íšŸìˆ˜ ì œì™¸, ìƒìœ„ 3ê°œ)
        top_emotions = [k for k, v in emotion_counts.most_common(3)]
        emotion_summary = ", ".join(top_emotions) if top_emotions else "ë¶„ì„ ë°ì´í„° ì—†ìŒ"
        
        # ì£¼ìš” ê°ì • ì„¤ëª…
        if len(top_emotions) > 1:
            main_emotion_desc = f"ì£¼ë¡œ {top_emotions[0]} ê°ì •ì´ ë§ì•˜ê³ , {', '.join(top_emotions[1:])}ë„ ëŠë¼ì…¨ë„¤ìš”"
        elif len(top_emotions) == 1:
            main_emotion_desc = f"ëŒ€ë¶€ë¶„ {top_emotions[0]} ê°ì •ì´ì—ˆë„¤ìš”"
        else:
            main_emotion_desc = "ê°ì • ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³  ë”°ëœ»í•œ ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ì˜¤ëŠ˜ ì‘ì„±í•œ ê°ì • ì¼ê¸°ë¥¼ ë³´ê³  ì§„ì‹¬ì–´ë¦° ì¡°ì–¸ê³¼ ìœ„ë¡œë¥¼ í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ìì˜ ì˜¤ëŠ˜ ê°ì •**: {emotion}
**ì¼ê¸° ë‚´ìš©**: {diary_text}
**ê°ì • ë¶„ì„**: {main_emotion_desc}

ë‹¤ìŒ ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì¹œê·¼í•˜ê³  ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš” (ì¡´ëŒ“ë§ ì‚¬ìš©)
2. ì‚¬ìš©ìì˜ ê°ì •ì„ ê³µê°í•˜ê³  ì¸ì •í•´ì£¼ì„¸ìš”
3. ê°ì • ë¶„ì„ ê²°ê³¼ì˜ êµ¬ì²´ì ì¸ íšŸìˆ˜ë‚˜ ìˆ«ìëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
4. "ëŒ€ë¶€ë¶„", "ì£¼ë¡œ", "ë§ì´" ê°™ì€ í‘œí˜„ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
5. ê¸ì •ì ì´ê³  ê±´ì„¤ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´ì£¼ì„¸ìš”
6. í•„ìš”í•˜ë‹¤ë©´ êµ¬ì²´ì ì¸ ì‹¤ì²œ ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”
7. 200-300ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”
8. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ì¹œê·¼ê°ì„ ë”í•´ì£¼ì„¸ìš”

ì‘ë‹µ í˜•ì‹:
[ê³µê°ê³¼ ì¸ì •] â†’ [ì¡°ì–¸ ë˜ëŠ” ê²©ë ¤] â†’ [ë§ˆë¬´ë¦¬ ì‘ì›]

ì£¼ì˜ì‚¬í•­: "28íšŒ", "10ë²ˆ" ê°™ì€ êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ íšŸìˆ˜ëŠ” ì ˆëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”!
"""

        # ì œë¯¸ë‚˜ì´ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = genai.Client(api_key=GEMINI_API_KEY)
        
        print("ì œë¯¸ë‚˜ì´ API í˜¸ì¶œ ì‹œì‘...")
        
        # API í˜¸ì¶œ - gemini-2.5-flash ì‚¬ìš©
        response = client.models.generate_content(
            model="gemini-2.5-flash-lite",
            contents=prompt
        )
        
        print("âœ… ì œë¯¸ë‚˜ì´ ì‘ë‹µ ë°›ìŒ!")
        
        # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        advice = response.text.strip()
        
        if advice:
            return advice
        else:
            return f"âš ï¸ AI ì¡°ì–¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì˜¤ëŠ˜ '{emotion}' ê°ì •ì„ ëŠë¼ì…¨êµ°ìš”. ê°ì •ì„ ê¸°ë¡í•˜ê³  í‘œí˜„í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ í° ì˜ë¯¸ê°€ ìˆìŠµë‹ˆë‹¤. ğŸ’™"
            
    except Exception as e:
        error_msg = str(e)
        print(f"âŒ ì œë¯¸ë‚˜ì´ API ì˜¤ë¥˜: {error_msg}")
        import traceback
        traceback.print_exc()
        
        # ì‚¬ìš©ì ì¹œí™”ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€
        if "API_KEY" in error_msg.upper() or "authentication" in error_msg.lower():
            return f"âš ï¸ API í‚¤ ì¸ì¦ ì˜¤ë¥˜\n\ní•˜ì§€ë§Œ ì˜¤ëŠ˜ '{emotion}' ê°ì •ì„ ê¸°ë¡í•˜ì‹  ê²ƒì€ ë§¤ìš° ì˜ë¯¸ìˆëŠ” í–‰ë™ì…ë‹ˆë‹¤. ğŸ’š"
        elif "quota" in error_msg.lower() or "limit" in error_msg.lower():
            return f"âš ï¸ API ì‚¬ìš© í•œë„ ì´ˆê³¼\n\nê·¸ë˜ë„ ì˜¤ëŠ˜ '{emotion}' ê°ì •ì„ í‘œí˜„í•˜ì‹  ê²ƒë§Œìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤. ğŸ’ª"
        else:
            return f"âš ï¸ AI ì¡°ì–¸ ì„œë¹„ìŠ¤ ì˜¤ë¥˜\n\ní•˜ì§€ë§Œ ì˜¤ëŠ˜ '{emotion}' ê°ì •ì„ ê¸°ë¡í•˜ì‹  ê²ƒì€ ë§¤ìš° ì˜ë¯¸ìˆëŠ” í–‰ë™ì…ë‹ˆë‹¤. ê°ì •ì„ ê¸€ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œë„ ë§ˆìŒì´ ì •ë¦¬ë˜ê³  ì¹˜ìœ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ’š\n\nğŸ’ª ì§€ê¸ˆ ì´ ìˆœê°„ ëŠë¼ëŠ” ê°ì •ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ë°›ì•„ë“¤ì—¬ ì£¼ì„¸ìš”. ë‚´ì¼ì€ ë˜ ë‹¤ë¥¸ í•˜ë£¨ê°€ ì‹œì‘ë©ë‹ˆë‹¤!"

# ë¡œì»¬ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
def load_local_data():
    """ë¡œì»¬ì— ì €ì¥ëœ ì¼ê¸° ë°ì´í„° ë¡œë“œ"""
    if DIARY_DATA_FILE.exists():
        try:
            with open(DIARY_DATA_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ” í•­ëª©ë§Œ í•„í„°ë§
                valid_entries = []
                for entry in data:
                    video_path = entry.get('video_path', '')
                    # ìƒëŒ€ ê²½ë¡œì™€ ì ˆëŒ€ ê²½ë¡œ ëª¨ë‘ í™•ì¸
                    if os.path.exists(video_path):
                        valid_entries.append(entry)
                    elif os.path.exists(os.path.join(os.getcwd(), video_path)):
                        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
                        entry['video_path'] = os.path.join(os.getcwd(), video_path)
                        entry['text_path'] = os.path.join(os.getcwd(), entry.get('text_path', ''))
                        valid_entries.append(entry)
                return valid_entries
        except Exception as e:
            st.warning(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    return []

def save_local_data(entries):
    """ì¼ê¸° ë°ì´í„°ë¥¼ ë¡œì»¬ì— ì €ì¥"""
    try:
        with open(DIARY_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(entries, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        st.error(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def load_user_stats():
    """ì‚¬ìš©ì í†µê³„ ë¡œë“œ"""
    if USER_STATS_FILE.exists():
        try:
            with open(USER_STATS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {
        'total_entries': 0,
        'emotion_distribution': {},
        'ai_vs_user_agreement': 0,
        'last_updated': None
    }

def save_user_stats(stats):
    """ì‚¬ìš©ì í†µê³„ ì €ì¥"""
    try:
        with open(USER_STATS_FILE, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"í†µê³„ ì €ì¥ ì˜¤ë¥˜: {e}")

# ì‚¬ìš©ì ë§ì¶¤ ëª¨ë¸ í´ë˜ìŠ¤
class PersonalizedEmotionModel:
    def __init__(self):
        self.model = None
        self.emotion_mapping = {
            'í–‰ë³µí•¨': 0, 'ìŠ¬í””': 1, 'í™”ë‚¨': 2, 'ë†€ëŒ': 3,
            'ë‹´ë‹´í•¨': 4, 'ë‘ë ¤ì›€': 5, 'í˜ì˜¤': 6
        }
        self.reverse_mapping = {v: k for k, v in self.emotion_mapping.items()}
        self.training_data = []
        self.load_model()
    
    def load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        if USER_MODEL_FILE.exists():
            try:
                with open(USER_MODEL_FILE, 'rb') as f:
                    data = pickle.load(f)
                    self.model = data.get('model')
                    self.training_data = data.get('training_data', [])
                    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {len(self.training_data)}ê°œ í•™ìŠµ ë°ì´í„°")
                    return True
            except Exception as e:
                print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        else:
            print(f"â„¹ï¸ ì €ì¥ëœ ëª¨ë¸ ì—†ìŒ (ê²½ë¡œ: {USER_MODEL_FILE})")
        return False
    
    def save_model(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            with open(USER_MODEL_FILE, 'wb') as f:
                pickle.dump({
                    'model': self.model,
                    'training_data': self.training_data
                }, f)
            print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {len(self.training_data)}ê°œ í•™ìŠµ ë°ì´í„°")
            return True
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_features(self, emotion_timeline, text):
        """ê°ì • íƒ€ì„ë¼ì¸ê³¼ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        if not emotion_timeline:
            return None
        
        # ê°ì • ë¶„í¬
        emotions = [e['emotion'] for e in emotion_timeline]
        emotion_counts = Counter(emotions)
        
        # íŠ¹ì§• ë²¡í„° ìƒì„±
        features = []
        
        # ê° ê°ì •ì˜ ë¹„ìœ¨
        for emotion in ['í–‰ë³µí•¨', 'ìŠ¬í””', 'í™”ë‚¨', 'ë†€ëŒ', 'ë‹´ë‹´í•¨', 'ë‘ë ¤ì›€', 'í˜ì˜¤']:
            features.append(emotion_counts.get(emotion, 0) / len(emotions))
        
        # í‰ê·  í™•ì‹ ë„
        avg_confidence = np.mean([e['confidence'] for e in emotion_timeline])
        features.append(avg_confidence)
        
        # ê°ì • ë³€í™” íšŸìˆ˜ (ê°ì •ì´ ë°”ë€ íšŸìˆ˜)
        emotion_changes = sum(1 for i in range(1, len(emotions)) if emotions[i] != emotions[i-1])
        features.append(emotion_changes / len(emotions))
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´
        text_length = len(text.split()) if text else 0
        features.append(min(text_length / 100, 1.0))  # ì •ê·œí™”
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ê°ì •
        most_common = emotion_counts.most_common(1)[0][0]
        features.append(self.emotion_mapping.get(most_common, 4))
        
        return features
    
    def add_training_sample(self, emotion_timeline, text, ai_emotion, user_emotion):
        """í•™ìŠµ ìƒ˜í”Œ ì¶”ê°€"""
        features = self.extract_features(emotion_timeline, text)
        if features is None:
            print("âŒ í•™ìŠµ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜")
            return False
        
        user_emotion_code = self.emotion_mapping.get(user_emotion, 4)
        
        self.training_data.append({
            'features': features,
            'ai_emotion': ai_emotion,
            'user_emotion': user_emotion_code,
            'timestamp': datetime.now().isoformat()
        })
        
        print(f"âœ… í•™ìŠµ ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: ì´ {len(self.training_data)}ê°œ (AI: {ai_emotion}, ì‚¬ìš©ì: {user_emotion})")
        
        # í•™ìŠµ ë°ì´í„° ì¶”ê°€ í›„ í•­ìƒ ì €ì¥
        self.save_model()
        
        return True
    
    def train(self):
        """ëª¨ë¸ í•™ìŠµ"""
        if len(self.training_data) < 3:
            return False
        
        X = [d['features'] for d in self.training_data]
        y = [d['user_emotion'] for d in self.training_data]
        
        self.model = RandomForestClassifier(n_estimators=50, random_state=42)
        self.model.fit(X, y)
        
        self.save_model()
        return True
    
    def predict(self, emotion_timeline, text, ai_emotion):
        """ë§ì¶¤í˜• ê°ì • ì˜ˆì¸¡"""
        if self.model is None or len(self.training_data) < 3:
            # í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ AI ì˜ˆì¸¡ ì‚¬ìš©
            return ai_emotion, 0.0, False
        
        features = self.extract_features(emotion_timeline, text)
        if features is None:
            return ai_emotion, 0.0, False
        
        try:
            prediction = self.model.predict([features])[0]
            probabilities = self.model.predict_proba([features])[0]
            confidence = max(probabilities)
            
            predicted_emotion = self.reverse_mapping.get(prediction, 'ë‹´ë‹´í•¨')
            return predicted_emotion, confidence, True
        except Exception as e:
            print(f"ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return ai_emotion, 0.0, False

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'diary_entries' not in st.session_state:
    st.session_state.diary_entries = load_local_data()
if 'recording' not in st.session_state:
    st.session_state.recording = False
if 'video_frames' not in st.session_state:
    st.session_state.video_frames = []
if 'current_emotion' not in st.session_state:
    st.session_state.current_emotion = 'ë‹´ë‹´í•¨'
if 'emotion_timeline' not in st.session_state:
    st.session_state.emotion_timeline = []
if 'recording_start_time' not in st.session_state:
    st.session_state.recording_start_time = None
if 'webcam_active' not in st.session_state:
    st.session_state.webcam_active = False
if 'voice_recording' not in st.session_state:
    st.session_state.voice_recording = False
if 'transcribed_text' not in st.session_state:
    st.session_state.transcribed_text = ""
if 'audio_queue' not in st.session_state:
    st.session_state.audio_queue = queue.Queue()
if 'audio_frames_queue' not in st.session_state:
    st.session_state.audio_frames_queue = queue.Queue()
if 'audio_frames' not in st.session_state:
    st.session_state.audio_frames = []
if 'pending_save' not in st.session_state:
    st.session_state.pending_save = False
if 'save_data' not in st.session_state:
    st.session_state.save_data = None
if 'emotion_confirmed' not in st.session_state:
    st.session_state.emotion_confirmed = False
if 'confirmed_emotion' not in st.session_state:
    st.session_state.confirmed_emotion = None
if 'personalized_model' not in st.session_state:
    st.session_state.personalized_model = PersonalizedEmotionModel()
if 'user_stats' not in st.session_state:
    st.session_state.user_stats = load_user_stats()
if 'gemini_advice' not in st.session_state:
    st.session_state.gemini_advice = None
if 'advice_loading' not in st.session_state:
    st.session_state.advice_loading = False
if 'processing_emotion' not in st.session_state:
    st.session_state.processing_emotion = None

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“¸ ë…¹í™” ì„¤ì •")
    
    anonymize_option = st.selectbox(
        "ì „ì²´ í™”ë©´ ìµëª…í™” ë°©ì‹",
        ["ì›ë³¸", "ë¸”ëŸ¬", "ê³° ì–¼êµ´ ğŸ»", "í† ë¼ ì–¼êµ´ ğŸ°", "ê³ ì–‘ì´ ì–¼êµ´ ğŸ±"],
        key="anonymize",
        disabled=st.session_state.recording
    )
    
    if st.session_state.recording:
        st.warning("âš ï¸ ë…¹í™” ì¤‘ì—ëŠ” ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    st.header("AI ê°œì¸í™” ìƒíƒœ")
    
    model_trained = st.session_state.personalized_model.model is not None
    training_count = len(st.session_state.personalized_model.training_data)
    
    if model_trained:
        st.success(f"âœ… ë§ì¶¤ ëª¨ë¸ í™œì„±í™”ë¨")
        st.metric("í•™ìŠµ ë°ì´í„°", f"{training_count}ê°œ")
        
        # ì¼ì¹˜ìœ¨ í‘œì‹œ
        if st.session_state.user_stats['total_entries'] > 0:
            agreement = st.session_state.user_stats['ai_vs_user_agreement']
            st.metric("AI-ì‚¬ìš©ì ì¼ì¹˜ìœ¨", f"{agreement:.1f}%")
    else:
        st.info(f"ğŸ“Š í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ ì¤‘")
        st.metric("ìˆ˜ì§‘ëœ ë°ì´í„°", f"{training_count}/3ê°œ")
        if training_count < 3:
            st.caption(f"ë§ì¶¤ ëª¨ë¸ í™œì„±í™”ê¹Œì§€ {3-training_count}ê°œ í•„ìš”")
    
    st.markdown("---")
    
    st.header("â„¹ï¸ ì‚¬ìš© ë°©ë²•")
    st.markdown("""
    ### ğŸ“¹ ìŒì„± ì˜ìƒ ì¼ê¸° ì‘ì„± ìˆœì„œ
    
    1. ğŸ¨ **ìµëª…í™” ë°©ì‹ ì„ íƒ**
    2. ğŸ”´ **ë…¹í™” ì‹œì‘** í´ë¦­
    3. ğŸ¤ **ë§í•˜ë©° ê°ì • í‘œí˜„**
    4. â¹ï¸ **ë…¹í™” ì¤‘ì§€ & ì €ì¥**
    5. âœ¨ **ì˜¤ëŠ˜ì˜ ê¸°ë¶„ ì„ íƒ**
    6. ğŸ­ **ì œë¯¸ë‚˜ì´ AI ì¡°ì–¸ ë°›ê¸°**
    7. ğŸ“¥ **ì˜ìƒ & í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ**
    
    ### ğŸŒŸ ì œë¯¸ë‚˜ì´ AI ì¡°ì–¸
    - **ê°œì¸í™”ëœ ì¡°ì–¸**: ì¼ê¸° ë‚´ìš©ê³¼ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤ ì¡°ì–¸ ì œê³µ
    - **ë”°ëœ»í•œ ìœ„ë¡œ**: ê³µê°ê³¼ ê²©ë ¤ì˜ ë©”ì‹œì§€
    - **ì‹¤ì²œ ê°€ëŠ¥í•œ íŒ**: êµ¬ì²´ì ì¸ ê°œì„  ë°©ë²• ì œì•ˆ
    
    ### âœ¨ AI ê°œì¸í™” ê¸°ëŠ¥
    - **ìë™ í•™ìŠµ**: ì¼ê¸°ë¥¼ 3ê°œ ì´ìƒ ì‘ì„±í•˜ë©´ ìë™ìœ¼ë¡œ ë§ì¶¤ ëª¨ë¸ì´ í™œì„±í™”ë©ë‹ˆë‹¤
    - **ë§ì¶¤ ì¶”ì²œ**: ì‚¬ìš©ìì˜ ê³¼ê±° ì„ íƒ íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ë” ì •í™•í•œ ê°ì •ì„ ì¶”ì²œí•©ë‹ˆë‹¤
    - **ì§€ì† ê°œì„ **: ì¼ê¸°ë¥¼ ì‘ì„±í• ìˆ˜ë¡ AIê°€ ì‚¬ìš©ìë¥¼ ë” ì˜ ì´í•´í•©ë‹ˆë‹¤
    
    ### ğŸ­ ì§€ì› ê°ì •
    - ğŸ˜Š í–‰ë³µí•¨
    - ğŸ˜¢ ìŠ¬í””
    - ğŸ˜  í™”ë‚¨
    - ğŸ˜² ë†€ëŒ
    - ğŸ˜ ë‹´ë‹´í•¨
    - ğŸ˜¨ ë‘ë ¤ì›€
    - ğŸ¤¢ í˜ì˜¤
    
    ### ğŸ”’ ìµëª…í™” ë°©ì‹
    - **ì›ë³¸**: ì–¼êµ´ ê·¸ëŒ€ë¡œ
    - **ë¸”ëŸ¬**: ì „ì²´ í™”ë©´ íë¦¬ê²Œ
    - **í”½ì…€í™”**: ì „ì²´ í™”ë©´ ëª¨ìì´í¬
    - **ì¹´íˆ°**: ì „ì²´ í™”ë©´ ë§Œí™” ìŠ¤íƒ€ì¼
    
    ### ğŸ¤ ìŒì„± ì…ë ¥
    - **ìë™ í™œì„±í™”**: ë…¹í™” ì‹œì‘ ì‹œ ìë™ í™œì„±í™”
    - **í•œêµ­ì–´ ì¸ì‹**: ì‹¤ì‹œê°„ í•œêµ­ì–´ ìŒì„± ì¸ì‹
    - **ì‹¤ì‹œê°„ ë³€í™˜**: ë§í•œ ë‚´ìš©ì„ ì¦‰ì‹œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    - **ìë™ ì €ì¥**: ì¼ê¸°ë¡œ ìë™ ì €ì¥
    
    ### ğŸ’¡ ì´¬ì˜ íŒ
    - ğŸ’¡ **ë°ì€ ì¡°ëª…** ì‚¬ìš©
    - ğŸ“· **ì •ë©´ ì–¼êµ´** ìœ ì§€
    - ğŸ˜€ **ìì—°ìŠ¤ëŸ¬ìš´ í‘œì •**
    - ğŸ”‡ **ì¡°ìš©í•œ í™˜ê²½** (ìŒì„± ì¸ì‹ ìµœì í™”)
    - ğŸ¤ **ë§ˆì´í¬ ê°€ê¹Œì´**ì—ì„œ ë˜ë ·í•˜ê²Œ ë§í•˜ê¸°
    - ğŸ—£ï¸ **ì²œì²œíˆ ëª…í™•í•˜ê²Œ** ë°œìŒí•˜ê¸°
    
    ### ğŸ’¾ ë°ì´í„° ì €ì¥ ìœ„ì¹˜
    - **ì €ì¥ í´ë”**: `emotion_diary_data/`
    - **ì˜ìƒ íŒŒì¼**: `emotion_diary_data/videos/`
    - ì•± ì‹¤í–‰ í´ë”ì— ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤
    - ëª¨ë“  ë°ì´í„°ëŠ” ë¡œì»¬ì— ì•ˆì „í•˜ê²Œ ë³´ê´€ë©ë‹ˆë‹¤
    """)
    
    st.markdown("---")
    
    # í†µê³„
    st.subheader("ğŸ“ˆ ì „ì²´ í†µê³„")
    total_entries = len(st.session_state.diary_entries)
    total_frames = sum([e.get('frame_count', 0) for e in st.session_state.diary_entries])
    voice_entries = len(st.session_state.diary_entries)
    st.metric("ì´ ì¼ê¸° ìˆ˜", total_entries)
    st.metric("ì´ í”„ë ˆì„ ìˆ˜", total_frames)
    st.metric("ìŒì„± ì…ë ¥ ì‚¬ìš©", f"{voice_entries}íšŒ")
    
    # ê°ì • ë¶„í¬ í‘œì‹œ
    if st.session_state.user_stats['emotion_distribution']:
        st.markdown("**ê°ì • ë¶„í¬**")
        for emotion, count in sorted(
            st.session_state.user_stats['emotion_distribution'].items(), 
            key=lambda x: x[1], 
            reverse=True
        ):
            st.caption(f"{emotion}: {count}íšŒ")
    
    st.markdown("---")
    
    if st.button("ğŸ—‘ï¸ ëª¨ë“  ê¸°ë¡ ì´ˆê¸°í™”", type="secondary"):
        # ë¡œì»¬ íŒŒì¼ ì‚­ì œ
        try:
            if DIARY_DATA_FILE.exists():
                DIARY_DATA_FILE.unlink()
            if USER_MODEL_FILE.exists():
                USER_MODEL_FILE.unlink()
            if USER_STATS_FILE.exists():
                USER_STATS_FILE.unlink()
        except Exception as e:
            st.error(f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {e}")
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        st.session_state.diary_entries = []
        st.session_state.video_frames = []
        st.session_state.emotion_timeline = []
        st.session_state.recording = False
        st.session_state.webcam_active = False
        st.session_state.recording_start_time = None
        st.session_state.voice_recording = False
        st.session_state.transcribed_text = ""
        st.session_state.pending_save = False
        st.session_state.save_data = None
        st.session_state.emotion_confirmed = False
        st.session_state.confirmed_emotion = None
        st.session_state.personalized_model = PersonalizedEmotionModel()
        st.session_state.user_stats = {
            'total_entries': 0,
            'emotion_distribution': {},
            'ai_vs_user_agreement': 0,
            'last_updated': None
        }
        st.session_state.gemini_advice = None
        st.session_state.advice_loading = False
        st.success("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
        st.rerun()

# í•œê¸€ í°íŠ¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_korean_font(size=40):
    """í•œê¸€ì„ ì§€ì›í•˜ëŠ” í°íŠ¸ ë¡œë“œ"""
    font_paths = [
        "malgun.ttf",
        "C:/Windows/Fonts/malgun.ttf",
        "AppleGothic.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
        "NanumGothic.ttf",
    ]
    
    for font_path in font_paths:
        try:
            return ImageFont.truetype(font_path, size)
        except:
            continue
    
    return ImageFont.load_default()

# í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ì¶”ê°€í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def put_korean_text(image, text, position, font_size=40, color=(255, 255, 255)):
    """OpenCV ì´ë¯¸ì§€ì— í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€"""
    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    draw = ImageDraw.Draw(pil_image)
    font = load_korean_font(font_size)
    draw.text(position, text, fill=color, font=font)
    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)

# ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_emotion_model():
    try:
        return pipeline("image-classification", model="dima806/facial_emotions_image_detection")
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# MediaPipe ì–¼êµ´ ê²€ì¶œ
@st.cache_resource
def load_face_detector():
    try:
        return mp.solutions.face_detection.FaceDetection(min_detection_confidence=0.5)
    except Exception as e:
        st.error(f"ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return None

# ì „ì²´ í™”ë©´ ìµëª…í™” í•¨ìˆ˜ë“¤
def blur_frame(image: np.ndarray, strength: int = 51) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ë¸”ëŸ¬ ì²˜ë¦¬"""
    if strength % 2 == 0:
        strength += 1
    return cv2.GaussianBlur(image, (strength, strength), 0)

def pixelate_frame(image: np.ndarray, blocks: int = 16) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ í”½ì…€í™”"""
    h, w = image.shape[:2]
    if h > 0 and w > 0 and h > blocks and w > blocks:
        temp = cv2.resize(image, (blocks, blocks), interpolation=cv2.INTER_LINEAR)
        return cv2.resize(temp, (w, h), interpolation=cv2.INTER_NEAREST)
    return image

def cartoonize_frame(image: np.ndarray) -> np.ndarray:
    """ì „ì²´ í”„ë ˆì„ ì¹´íˆ° ìŠ¤íƒ€ì¼ ë³€í™˜"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        color = cv2.bilateralFilter(image, 9, 250, 250)
        for _ in range(2):
            color = cv2.bilateralFilter(color, 9, 250, 250)
        
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray = cv2.medianBlur(gray, 7)
        edges = cv2.adaptiveThreshold(
            gray, 255, 
            cv2.ADAPTIVE_THRESH_MEAN_C, 
            cv2.THRESH_BINARY, 
            9, 2
        )
        
        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
        cartoon = cv2.bitwise_and(color, edges_colored)
        cartoon = cv2.convertScaleAbs(cartoon, alpha=1.2, beta=10)
        
        return cartoon
    except Exception as e:
        print(f"ì¹´íˆ° ë³€í™˜ ì˜¤ë¥˜: {e}")
        return image

def bear_face_mask(image: np.ndarray, face_detector) -> np.ndarray:
    """ì–¼êµ´ì„ ê·€ì—¬ìš´ ê³° ì–¼êµ´ë¡œ ëŒ€ì²´"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        result = image.copy()
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = image.shape
                x = int(bboxC.xmin * w)
                y = int(bboxC.ymin * h)
                width = int(bboxC.width * w)
                height = int(bboxC.height * h)
                
                # ê²½ê³„ í™•ì¸
                x = max(0, x)
                y = max(0, y)
                width = min(width, w - x)
                height = min(height, h - y)
                
                if width <= 0 or height <= 0:
                    continue
                
                # ì–¼êµ´ ì˜ì—­ì„ ì•½ê°„ í™•ì¥ (ê·€ í¬í•¨)
                margin = int(width * 0.3)
                bear_x = max(0, x - margin)
                bear_y = max(0, y - margin)
                bear_w = min(w - bear_x, width + margin * 2)
                bear_h = min(h - bear_y, height + margin * 2)
                
                # ê³° ì–¼êµ´ ê·¸ë¦¬ê¸°
                center_x = bear_x + bear_w // 2
                center_y = bear_y + bear_h // 2
                
                # ì–¼êµ´ (ì›) - ì—°í•œ ê°ˆìƒ‰
                face_radius = min(bear_w, bear_h) // 2
                cv2.circle(result, (center_x, center_y), face_radius, (150, 120, 80), -1)  # ì—°í•œ ê°ˆìƒ‰
                cv2.circle(result, (center_x, center_y), face_radius, (100, 70, 40), 3)  # ì§„í•œ ê°ˆìƒ‰ í…Œë‘ë¦¬
                
                # ê·€ (2ê°œ) - ê°ˆìƒ‰
                ear_radius = face_radius // 3
                left_ear_x = center_x - int(face_radius * 0.7)
                right_ear_x = center_x + int(face_radius * 0.7)
                ear_y = center_y - int(face_radius * 0.7)
                
                # ê·€ ë³¸ì²´
                cv2.circle(result, (left_ear_x, ear_y), ear_radius, (150, 120, 80), -1)
                cv2.circle(result, (left_ear_x, ear_y), ear_radius, (100, 70, 40), 2)
                cv2.circle(result, (right_ear_x, ear_y), ear_radius, (150, 120, 80), -1)
                cv2.circle(result, (right_ear_x, ear_y), ear_radius, (100, 70, 40), 2)
                
                # ê·€ ì•ˆìª½ - ë°ì€ ë…¸ë€ìƒ‰
                inner_ear_radius = ear_radius // 2
                cv2.circle(result, (left_ear_x, ear_y), inner_ear_radius, (100, 200, 255), -1)  # ë…¸ë€ìƒ‰
                cv2.circle(result, (right_ear_x, ear_y), inner_ear_radius, (100, 200, 255), -1)
                
                # ğŸ€ ë¦¬ë³¸ ì¶”ê°€ (ì˜¤ë¥¸ìª½ ê·€ ì˜†)
                ribbon_center_x = right_ear_x + int(ear_radius * 1.2)
                ribbon_center_y = ear_y - int(ear_radius * 0.3)
                ribbon_size = ear_radius // 2
                
                # ë¦¬ë³¸ ì™¼ìª½ ë‚˜ë¹„
                ribbon_left = (ribbon_center_x - ribbon_size, ribbon_center_y)
                cv2.circle(result, ribbon_left, ribbon_size, (100, 100, 255), -1)  # ë¶„í™ìƒ‰
                
                # ë¦¬ë³¸ ì˜¤ë¥¸ìª½ ë‚˜ë¹„
                ribbon_right = (ribbon_center_x + ribbon_size, ribbon_center_y)
                cv2.circle(result, ribbon_right, ribbon_size, (100, 100, 255), -1)
                
                # ë¦¬ë³¸ ì¤‘ì•™ ë§¤ë“­
                cv2.circle(result, (ribbon_center_x, ribbon_center_y), ribbon_size // 2, (80, 80, 200), -1)
                
                # ì–¼êµ´ ì¤‘ì•™ ë¶€ë¶„ - ë°ì€ ë…¸ë€ìƒ‰
                snout_radius = face_radius // 2
                snout_y = center_y + face_radius // 4
                cv2.circle(result, (center_x, snout_y), snout_radius, (120, 220, 255), -1)  # ë°ì€ ë…¸ë€ìƒ‰
                cv2.circle(result, (center_x, snout_y), snout_radius, (100, 180, 230), 2)  # í…Œë‘ë¦¬
                
                # ëˆˆ (2ê°œ) - í¬ê³  ë°˜ì§ì´ëŠ” ëˆˆ
                eye_radius = face_radius // 5
                left_eye_x = center_x - face_radius // 3
                right_eye_x = center_x + face_radius // 3
                eye_y = center_y - face_radius // 5
                
                # ëˆˆ í°ì
                cv2.circle(result, (left_eye_x, eye_y), eye_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x, eye_y), eye_radius, (255, 255, 255), -1)
                
                # ëˆˆë™ì
                pupil_radius = eye_radius * 2 // 3
                cv2.circle(result, (left_eye_x, eye_y), pupil_radius, (50, 30, 20), -1)
                cv2.circle(result, (right_eye_x, eye_y), pupil_radius, (50, 30, 20), -1)
                
                # ëˆˆ í•˜ì´ë¼ì´íŠ¸ (ë°˜ì§ì„)
                highlight_radius = eye_radius // 3
                cv2.circle(result, (left_eye_x - 3, eye_y - 3), highlight_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x - 3, eye_y - 3), highlight_radius, (255, 255, 255), -1)
                
                # ì½” (í•˜íŠ¸ ëª¨ì–‘ ì‹œë„ - íƒ€ì›)
                nose_w = snout_radius // 2
                nose_h = snout_radius // 3
                nose_y = snout_y - snout_radius // 4
                cv2.ellipse(result, (center_x, nose_y), (nose_w, nose_h), 0, 0, 360, (50, 30, 20), -1)
                
                # ì… (ê·€ì—¬ìš´ ë¯¸ì†Œ)
                mouth_y = snout_y + snout_radius // 3
                # ì•„ë˜ ê³¡ì„ 
                cv2.ellipse(result, (center_x, mouth_y), (snout_radius // 2, snout_radius // 4), 
                           0, 0, 180, (50, 30, 20), 2)
                # ì½”ì—ì„œ ì…ìœ¼ë¡œ ì„ 
                cv2.line(result, (center_x, nose_y + nose_h), (center_x, mouth_y - snout_radius // 4), 
                        (50, 30, 20), 2)
                
                # ë³¼ (ë¶„í™ìƒ‰ ë¸”ëŸ¬ì‹œ)
                blush_radius = face_radius // 6
                left_blush_x = center_x - int(face_radius * 0.5)
                right_blush_x = center_x + int(face_radius * 0.5)
                blush_y = center_y + face_radius // 6
                
                # ë°˜íˆ¬ëª… ë¸”ëŸ¬ì‹œ íš¨ê³¼
                overlay = result.copy()
                cv2.circle(overlay, (left_blush_x, blush_y), blush_radius, (128, 128, 255), -1)
                cv2.circle(overlay, (right_blush_x, blush_y), blush_radius, (128, 128, 255), -1)
                cv2.addWeighted(overlay, 0.3, result, 0.7, 0, result)
        
        return result
    except Exception as e:
        print(f"ê³° ì–¼êµ´ ë§ˆìŠ¤í¬ ì˜¤ë¥˜: {e}")
        return image

def rabbit_face_mask(image: np.ndarray, face_detector) -> np.ndarray:
    """ì–¼êµ´ì„ ê·€ì—¬ìš´ í† ë¼ ì–¼êµ´ë¡œ ëŒ€ì²´"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        result = image.copy()
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = image.shape
                x = int(bboxC.xmin * w)
                y = int(bboxC.ymin * h)
                width = int(bboxC.width * w)
                height = int(bboxC.height * h)
                
                x = max(0, x)
                y = max(0, y)
                width = min(width, w - x)
                height = min(height, h - y)
                
                if width <= 0 or height <= 0:
                    continue
                
                margin = int(width * 0.4)  # í† ë¼ ê·€ê°€ ê¸¸ì–´ì„œ ì—¬ìœ  ê³µê°„ ë” í•„ìš”
                rabbit_x = max(0, x - margin)
                rabbit_y = max(0, y - margin)
                rabbit_w = min(w - rabbit_x, width + margin * 2)
                rabbit_h = min(h - rabbit_y, height + margin * 2)
                
                center_x = rabbit_x + rabbit_w // 2
                center_y = rabbit_y + rabbit_h // 2
                
                # ì–¼êµ´ (ì›) - í°ìƒ‰
                face_radius = min(rabbit_w, rabbit_h) // 2
                cv2.circle(result, (center_x, center_y), face_radius, (240, 240, 250), -1)
                cv2.circle(result, (center_x, center_y), face_radius, (200, 200, 210), 3)
                
                # ê¸´ ê·€ (2ê°œ) - íƒ€ì›
                ear_width = face_radius // 4
                ear_height = int(face_radius * 0.8)
                left_ear_x = center_x - int(face_radius * 0.5)
                right_ear_x = center_x + int(face_radius * 0.5)
                ear_y = center_y - int(face_radius * 1.1)
                
                # ì™¼ìª½ ê·€
                cv2.ellipse(result, (left_ear_x, ear_y), (ear_width, ear_height), -15, 0, 360, (240, 240, 250), -1)
                cv2.ellipse(result, (left_ear_x, ear_y), (ear_width, ear_height), -15, 0, 360, (200, 200, 210), 2)
                # ê·€ ì•ˆìª½ (ë¶„í™)
                cv2.ellipse(result, (left_ear_x, ear_y), (ear_width//2, ear_height-10), -15, 0, 360, (200, 150, 255), -1)
                
                # ì˜¤ë¥¸ìª½ ê·€
                cv2.ellipse(result, (right_ear_x, ear_y), (ear_width, ear_height), 15, 0, 360, (240, 240, 250), -1)
                cv2.ellipse(result, (right_ear_x, ear_y), (ear_width, ear_height), 15, 0, 360, (200, 200, 210), 2)
                # ê·€ ì•ˆìª½ (ë¶„í™)
                cv2.ellipse(result, (right_ear_x, ear_y), (ear_width//2, ear_height-10), 15, 0, 360, (200, 150, 255), -1)
                
                # ëˆˆ (2ê°œ) - í° ëˆˆ
                eye_radius = face_radius // 5
                left_eye_x = center_x - face_radius // 3
                right_eye_x = center_x + face_radius // 3
                eye_y = center_y - face_radius // 5
                
                cv2.circle(result, (left_eye_x, eye_y), eye_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x, eye_y), eye_radius, (255, 255, 255), -1)
                
                pupil_radius = eye_radius * 2 // 3
                cv2.circle(result, (left_eye_x, eye_y), pupil_radius, (80, 50, 50), -1)
                cv2.circle(result, (right_eye_x, eye_y), pupil_radius, (80, 50, 50), -1)
                
                highlight_radius = eye_radius // 3
                cv2.circle(result, (left_eye_x - 3, eye_y - 3), highlight_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x - 3, eye_y - 3), highlight_radius, (255, 255, 255), -1)
                
                # ì½” (ì‘ì€ ì‚¼ê°í˜• - ë¶„í™)
                nose_y = center_y + face_radius // 8
                nose_size = face_radius // 8
                nose_pts = np.array([
                    [center_x, nose_y - nose_size//2],
                    [center_x - nose_size//2, nose_y + nose_size//2],
                    [center_x + nose_size//2, nose_y + nose_size//2]
                ], np.int32)
                cv2.fillPoly(result, [nose_pts], (180, 120, 255))
                
                # ì… (í† ë¼ íŠ¹ìœ ì˜ Yì ëª¨ì–‘)
                mouth_y = nose_y + nose_size
                # ì¤‘ì•™ ì„¸ë¡œì„ 
                cv2.line(result, (center_x, nose_y + nose_size//2), (center_x, mouth_y), (100, 70, 70), 2)
                # ì™¼ìª½ ê³¡ì„ 
                cv2.ellipse(result, (center_x - face_radius//6, mouth_y + face_radius//8), 
                           (face_radius//6, face_radius//8), 0, 180, 270, (100, 70, 70), 2)
                # ì˜¤ë¥¸ìª½ ê³¡ì„ 
                cv2.ellipse(result, (center_x + face_radius//6, mouth_y + face_radius//8), 
                           (face_radius//6, face_radius//8), 0, 270, 360, (100, 70, 70), 2)
                
                # ë³¼ (ë¶„í™ ë¸”ëŸ¬ì‹œ)
                blush_radius = face_radius // 7
                left_blush_x = center_x - int(face_radius * 0.5)
                right_blush_x = center_x + int(face_radius * 0.5)
                blush_y = center_y + face_radius // 6
                
                overlay = result.copy()
                cv2.circle(overlay, (left_blush_x, blush_y), blush_radius, (180, 150, 255), -1)
                cv2.circle(overlay, (right_blush_x, blush_y), blush_radius, (180, 150, 255), -1)
                cv2.addWeighted(overlay, 0.4, result, 0.6, 0, result)
                
                # ì•ë‹ˆ (2ê°œ)
                tooth_width = face_radius // 8
                tooth_height = face_radius // 6
                left_tooth_x = center_x - tooth_width // 2
                right_tooth_x = center_x + tooth_width // 2
                tooth_y = mouth_y + face_radius // 6
                
                cv2.rectangle(result, (left_tooth_x - tooth_width, tooth_y), 
                            (left_tooth_x, tooth_y + tooth_height), (255, 255, 255), -1)
                cv2.rectangle(result, (right_tooth_x, tooth_y), 
                            (right_tooth_x + tooth_width, tooth_y + tooth_height), (255, 255, 255), -1)
        
        return result
    except Exception as e:
        print(f"í† ë¼ ì–¼êµ´ ë§ˆìŠ¤í¬ ì˜¤ë¥˜: {e}")
        return image

def cat_face_mask(image: np.ndarray, face_detector) -> np.ndarray:
    """ì–¼êµ´ì„ ê·€ì—¬ìš´ ê³ ì–‘ì´ ì–¼êµ´ë¡œ ëŒ€ì²´"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        result = image.copy()
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = image.shape
                x = int(bboxC.xmin * w)
                y = int(bboxC.ymin * h)
                width = int(bboxC.width * w)
                height = int(bboxC.height * h)
                
                x = max(0, x)
                y = max(0, y)
                width = min(width, w - x)
                height = min(height, h - y)
                
                if width <= 0 or height <= 0:
                    continue
                
                margin = int(width * 0.3)
                cat_x = max(0, x - margin)
                cat_y = max(0, y - margin)
                cat_w = min(w - cat_x, width + margin * 2)
                cat_h = min(h - cat_y, height + margin * 2)
                
                center_x = cat_x + cat_w // 2
                center_y = cat_y + cat_h // 2
                
                # ì–¼êµ´ (ì›) - ì£¼í™©ìƒ‰ (ê³ ì–‘ì´)
                face_radius = min(cat_w, cat_h) // 2
                cv2.circle(result, (center_x, center_y), face_radius, (100, 160, 255), -1)  # ì£¼í™©ìƒ‰
                cv2.circle(result, (center_x, center_y), face_radius, (70, 130, 220), 3)
                
                # ì‚¼ê°í˜• ê·€ (2ê°œ)
                ear_size = face_radius // 2
                left_ear_x = center_x - int(face_radius * 0.6)
                right_ear_x = center_x + int(face_radius * 0.6)
                ear_y = center_y - int(face_radius * 0.8)
                
                # ì™¼ìª½ ê·€
                left_ear_pts = np.array([
                    [left_ear_x, ear_y],
                    [left_ear_x - ear_size//2, ear_y - ear_size],
                    [left_ear_x + ear_size//2, ear_y - ear_size//3]
                ], np.int32)
                cv2.fillPoly(result, [left_ear_pts], (100, 160, 255))
                cv2.polylines(result, [left_ear_pts], True, (70, 130, 220), 2)
                
                # ì™¼ìª½ ê·€ ì•ˆìª½ (ë¶„í™)
                left_inner_ear = np.array([
                    [left_ear_x, ear_y - ear_size//4],
                    [left_ear_x - ear_size//4, ear_y - ear_size//2],
                    [left_ear_x + ear_size//4, ear_y - ear_size//4]
                ], np.int32)
                cv2.fillPoly(result, [left_inner_ear], (150, 150, 255))
                
                # ì˜¤ë¥¸ìª½ ê·€
                right_ear_pts = np.array([
                    [right_ear_x, ear_y],
                    [right_ear_x + ear_size//2, ear_y - ear_size],
                    [right_ear_x - ear_size//2, ear_y - ear_size//3]
                ], np.int32)
                cv2.fillPoly(result, [right_ear_pts], (100, 160, 255))
                cv2.polylines(result, [right_ear_pts], True, (70, 130, 220), 2)
                
                # ì˜¤ë¥¸ìª½ ê·€ ì•ˆìª½ (ë¶„í™)
                right_inner_ear = np.array([
                    [right_ear_x, ear_y - ear_size//4],
                    [right_ear_x + ear_size//4, ear_y - ear_size//2],
                    [right_ear_x - ear_size//4, ear_y - ear_size//4]
                ], np.int32)
                cv2.fillPoly(result, [right_inner_ear], (150, 150, 255))
                
                # ëˆˆ (ê³ ì–‘ì´ ëˆˆ - íƒ€ì›)
                eye_width = face_radius // 5
                eye_height = face_radius // 3
                left_eye_x = center_x - face_radius // 3
                right_eye_x = center_x + face_radius // 3
                eye_y = center_y - face_radius // 5
                
                # ë…¹ìƒ‰ ê³ ì–‘ì´ ëˆˆ
                cv2.ellipse(result, (left_eye_x, eye_y), (eye_width, eye_height), 0, 0, 360, (100, 255, 100), -1)
                cv2.ellipse(result, (right_eye_x, eye_y), (eye_width, eye_height), 0, 0, 360, (100, 255, 100), -1)
                
                # ì„¸ë¡œ ë™ê³µ
                pupil_width = eye_width // 3
                pupil_height = int(eye_height * 0.8)
                cv2.ellipse(result, (left_eye_x, eye_y), (pupil_width, pupil_height), 0, 0, 360, (20, 20, 20), -1)
                cv2.ellipse(result, (right_eye_x, eye_y), (pupil_width, pupil_height), 0, 0, 360, (20, 20, 20), -1)
                
                # í•˜ì´ë¼ì´íŠ¸
                highlight_radius = eye_width // 4
                cv2.circle(result, (left_eye_x - pupil_width//2, eye_y - pupil_height//3), highlight_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x - pupil_width//2, eye_y - pupil_height//3), highlight_radius, (255, 255, 255), -1)
                
                # ì½” (ì‘ì€ ì‚¼ê°í˜• - ë¶„í™)
                nose_y = center_y + face_radius // 8
                nose_size = face_radius // 7
                nose_pts = np.array([
                    [center_x, nose_y + nose_size//2],
                    [center_x - nose_size//2, nose_y - nose_size//2],
                    [center_x + nose_size//2, nose_y - nose_size//2]
                ], np.int32)
                cv2.fillPoly(result, [nose_pts], (150, 120, 255))
                
                # ì… (W ëª¨ì–‘)
                mouth_y = nose_y + nose_size
                # ì™¼ìª½ ê³¡ì„ 
                cv2.ellipse(result, (center_x - face_radius//6, mouth_y), 
                           (face_radius//6, face_radius//8), 0, 0, 180, (80, 60, 60), 2)
                # ì˜¤ë¥¸ìª½ ê³¡ì„ 
                cv2.ellipse(result, (center_x + face_radius//6, mouth_y), 
                           (face_radius//6, face_radius//8), 0, 0, 180, (80, 60, 60), 2)
                
                # ìˆ˜ì—¼ (3ê°œì”© ì–‘ìª½)
                whisker_length = face_radius // 2
                whisker_y_offset = face_radius // 8
                
                # ì™¼ìª½ ìˆ˜ì—¼
                for i in range(3):
                    y_offset = whisker_y_offset * (i - 1)
                    cv2.line(result, (center_x - face_radius//2, center_y + y_offset), 
                            (center_x - face_radius - whisker_length//2, center_y + y_offset - i*5), 
                            (80, 60, 60), 2)
                
                # ì˜¤ë¥¸ìª½ ìˆ˜ì—¼
                for i in range(3):
                    y_offset = whisker_y_offset * (i - 1)
                    cv2.line(result, (center_x + face_radius//2, center_y + y_offset), 
                            (center_x + face_radius + whisker_length//2, center_y + y_offset - i*5), 
                            (80, 60, 60), 2)
        
        return result
    except Exception as e:
        print(f"ê³ ì–‘ì´ ì–¼êµ´ ë§ˆìŠ¤í¬ ì˜¤ë¥˜: {e}")
        return image
    """ì–¼êµ´ì„ ê·€ì—¬ìš´ ê³° ì–¼êµ´ë¡œ ëŒ€ì²´"""
    try:
        if image.size == 0 or image.shape[0] < 10 or image.shape[1] < 10:
            return image
        
        result = image.copy()
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            for detection in results.detections:
                bboxC = detection.location_data.relative_bounding_box
                h, w, _ = image.shape
                x = int(bboxC.xmin * w)
                y = int(bboxC.ymin * h)
                width = int(bboxC.width * w)
                height = int(bboxC.height * h)
                
                # ê²½ê³„ í™•ì¸
                x = max(0, x)
                y = max(0, y)
                width = min(width, w - x)
                height = min(height, h - y)
                
                if width <= 0 or height <= 0:
                    continue
                
                # ì–¼êµ´ ì˜ì—­ì„ ì•½ê°„ í™•ì¥ (ê·€ í¬í•¨)
                margin = int(width * 0.3)
                bear_x = max(0, x - margin)
                bear_y = max(0, y - margin)
                bear_w = min(w - bear_x, width + margin * 2)
                bear_h = min(h - bear_y, height + margin * 2)
                
                # ê³° ì–¼êµ´ ê·¸ë¦¬ê¸°
                center_x = bear_x + bear_w // 2
                center_y = bear_y + bear_h // 2
                
                # ì–¼êµ´ (ì›) - ì—°í•œ ê°ˆìƒ‰
                face_radius = min(bear_w, bear_h) // 2
                cv2.circle(result, (center_x, center_y), face_radius, (150, 120, 80), -1)  # ì—°í•œ ê°ˆìƒ‰
                cv2.circle(result, (center_x, center_y), face_radius, (100, 70, 40), 3)  # ì§„í•œ ê°ˆìƒ‰ í…Œë‘ë¦¬
                
                # ê·€ (2ê°œ) - ê°ˆìƒ‰
                ear_radius = face_radius // 3
                left_ear_x = center_x - int(face_radius * 0.7)
                right_ear_x = center_x + int(face_radius * 0.7)
                ear_y = center_y - int(face_radius * 0.7)
                
                # ê·€ ë³¸ì²´
                cv2.circle(result, (left_ear_x, ear_y), ear_radius, (150, 120, 80), -1)
                cv2.circle(result, (left_ear_x, ear_y), ear_radius, (100, 70, 40), 2)
                cv2.circle(result, (right_ear_x, ear_y), ear_radius, (150, 120, 80), -1)
                cv2.circle(result, (right_ear_x, ear_y), ear_radius, (100, 70, 40), 2)
                
                # ê·€ ì•ˆìª½ - ë°ì€ ë…¸ë€ìƒ‰
                inner_ear_radius = ear_radius // 2
                cv2.circle(result, (left_ear_x, ear_y), inner_ear_radius, (100, 200, 255), -1)  # ë…¸ë€ìƒ‰
                cv2.circle(result, (right_ear_x, ear_y), inner_ear_radius, (100, 200, 255), -1)
                
                # ğŸ€ ë¦¬ë³¸ ì¶”ê°€ (ì˜¤ë¥¸ìª½ ê·€ ì˜†)
                ribbon_center_x = right_ear_x + int(ear_radius * 1.2)
                ribbon_center_y = ear_y - int(ear_radius * 0.3)
                ribbon_size = ear_radius // 2
                
                # ë¦¬ë³¸ ì™¼ìª½ ë‚˜ë¹„
                ribbon_left = (ribbon_center_x - ribbon_size, ribbon_center_y)
                cv2.circle(result, ribbon_left, ribbon_size, (100, 100, 255), -1)  # ë¶„í™ìƒ‰
                
                # ë¦¬ë³¸ ì˜¤ë¥¸ìª½ ë‚˜ë¹„
                ribbon_right = (ribbon_center_x + ribbon_size, ribbon_center_y)
                cv2.circle(result, ribbon_right, ribbon_size, (100, 100, 255), -1)
                
                # ë¦¬ë³¸ ì¤‘ì•™ ë§¤ë“­
                cv2.circle(result, (ribbon_center_x, ribbon_center_y), ribbon_size // 2, (80, 80, 200), -1)
                
                # ì–¼êµ´ ì¤‘ì•™ ë¶€ë¶„ - ë°ì€ ë…¸ë€ìƒ‰
                snout_radius = face_radius // 2
                snout_y = center_y + face_radius // 4
                cv2.circle(result, (center_x, snout_y), snout_radius, (120, 220, 255), -1)  # ë°ì€ ë…¸ë€ìƒ‰
                cv2.circle(result, (center_x, snout_y), snout_radius, (100, 180, 230), 2)  # í…Œë‘ë¦¬
                
                # ëˆˆ (2ê°œ) - í¬ê³  ë°˜ì§ì´ëŠ” ëˆˆ
                eye_radius = face_radius // 5
                left_eye_x = center_x - face_radius // 3
                right_eye_x = center_x + face_radius // 3
                eye_y = center_y - face_radius // 5
                
                # ëˆˆ í°ì
                cv2.circle(result, (left_eye_x, eye_y), eye_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x, eye_y), eye_radius, (255, 255, 255), -1)
                
                # ëˆˆë™ì
                pupil_radius = eye_radius * 2 // 3
                cv2.circle(result, (left_eye_x, eye_y), pupil_radius, (50, 30, 20), -1)
                cv2.circle(result, (right_eye_x, eye_y), pupil_radius, (50, 30, 20), -1)
                
                # ëˆˆ í•˜ì´ë¼ì´íŠ¸ (ë°˜ì§ì„)
                highlight_radius = eye_radius // 3
                cv2.circle(result, (left_eye_x - 3, eye_y - 3), highlight_radius, (255, 255, 255), -1)
                cv2.circle(result, (right_eye_x - 3, eye_y - 3), highlight_radius, (255, 255, 255), -1)
                
                # ì½” (í•˜íŠ¸ ëª¨ì–‘ ì‹œë„ - íƒ€ì›)
                nose_w = snout_radius // 2
                nose_h = snout_radius // 3
                nose_y = snout_y - snout_radius // 4
                cv2.ellipse(result, (center_x, nose_y), (nose_w, nose_h), 0, 0, 360, (50, 30, 20), -1)
                
                # ì… (ê·€ì—¬ìš´ ë¯¸ì†Œ)
                mouth_y = snout_y + snout_radius // 3
                # ì•„ë˜ ê³¡ì„ 
                cv2.ellipse(result, (center_x, mouth_y), (snout_radius // 2, snout_radius // 4), 
                           0, 0, 180, (50, 30, 20), 2)
                # ì½”ì—ì„œ ì…ìœ¼ë¡œ ì„ 
                cv2.line(result, (center_x, nose_y + nose_h), (center_x, mouth_y - snout_radius // 4), 
                        (50, 30, 20), 2)
                
                # ë³¼ (ë¶„í™ìƒ‰ ë¸”ëŸ¬ì‹œ)
                blush_radius = face_radius // 6
                left_blush_x = center_x - int(face_radius * 0.5)
                right_blush_x = center_x + int(face_radius * 0.5)
                blush_y = center_y + face_radius // 6
                
                # ë°˜íˆ¬ëª… ë¸”ëŸ¬ì‹œ íš¨ê³¼
                overlay = result.copy()
                cv2.circle(overlay, (left_blush_x, blush_y), blush_radius, (128, 128, 255), -1)
                cv2.circle(overlay, (right_blush_x, blush_y), blush_radius, (128, 128, 255), -1)
                cv2.addWeighted(overlay, 0.3, result, 0.7, 0, result)
        
        return result
    except Exception as e:
        print(f"ê³° ì–¼êµ´ ë§ˆìŠ¤í¬ ì˜¤ë¥˜: {e}")
        return image

# ìŒì„± ì¸ì‹ í•¨ìˆ˜
def record_audio_continuous(audio_queue, stop_event, audio_frames_queue=None, start_time=None):
    """ì—°ì†ì ìœ¼ë¡œ ìŒì„±ì„ ì¸ì‹í•˜ê³  ì˜¤ë””ì˜¤ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    import pyaudio
    
    recognizer = sr.Recognizer()
    recognizer.energy_threshold = 4000
    recognizer.dynamic_energy_threshold = True
    
    # PyAudio ì„¤ì •
    CHUNK = 2048  # ë²„í¼ í¬ê¸°
    FORMAT = pyaudio.paInt16  # 16-bit
    CHANNELS = 1  # ëª¨ë…¸
    RATE = 16000  # ìƒ˜í”Œë ˆì´íŠ¸
    
    p = pyaudio.PyAudio()
    
    # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì—´ê¸° - ë²„í¼ í¬ê¸° ì¦ê°€
    stream = p.open(
        format=FORMAT,
        channels=CHANNELS,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK,
        stream_callback=None  # ì½œë°± ì‚¬ìš© ì•ˆ í•¨ (ë¸”ë¡œí‚¹ ëª¨ë“œ)
    )
    
    print("ğŸ¤ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì¤€ë¹„ ì™„ë£Œ!")
    
    # ì²« ì²­í¬ ì½ê¸° ì‹œì‘ ì‹œê°„ ê¸°ë¡
    first_chunk_time = None
    audio_start_recorded = False
    chunk_count = 0
    
    # ìŒì„± ì¸ì‹ì„ ìœ„í•œ ë²„í¼ (ë³„ë„ ì²˜ë¦¬)
    speech_recognition_queue = queue.Queue()
    
    # ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì‹œì‘ (ë…¹ìŒê³¼ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰)
    def speech_recognition_worker():
        """ìŒì„± ì¸ì‹ì„ ë³„ë„ë¡œ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤"""
        while not stop_event.is_set():
            try:
                # ì¸ì‹í•  ì˜¤ë””ì˜¤ ë°ì´í„° ëŒ€ê¸°
                audio_data = speech_recognition_queue.get(timeout=1)
                if audio_data is None:
                    break
                
                try:
                    audio_data_obj = sr.AudioData(audio_data, RATE, 2)
                    text = recognizer.recognize_google(audio_data_obj, language='ko-KR')
                    if text:
                        audio_queue.put(text)
                        print(f"âœ… ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
                except sr.UnknownValueError:
                    pass  # ì¸ì‹ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                except sr.RequestError as e:
                    print(f"âš ï¸ ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
            except queue.Empty:
                continue
            except Exception as e:
                print(f"âš ï¸ ìŒì„± ì¸ì‹ ì˜¤ë¥˜: {e}")
    
    # ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì‹œì‘
    recognition_thread = threading.Thread(target=speech_recognition_worker, daemon=True)
    recognition_thread.start()
    
    # ìŒì„± ì¸ì‹ìš© ë²„í¼
    speech_buffer = []
    silence_duration = 0
    SILENCE_THRESHOLD = 500
    MAX_SILENCE_CHUNKS = 15
    
    try:
        while not stop_event.is_set():
            try:
                # ì²­í¬ë¥¼ ì½ê¸° ì§ì „ ì‹œê°„ ê¸°ë¡
                chunk_read_time = time.time()
                
                # ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì²­í¬ ì½ê¸° (ë¸”ë¡œí‚¹)
                # ì´ ë¶€ë¶„ì´ ìµœëŒ€í•œ ë¹ ë¥´ê²Œ ì‹¤í–‰ë˜ì–´ì•¼ í•¨!
                audio_chunk = stream.read(CHUNK, exception_on_overflow=False)
                chunk_count += 1
                
                # ì²« ì²­í¬ì˜ ì •í™•í•œ ì‹œê°„ ê¸°ë¡
                if first_chunk_time is None:
                    first_chunk_time = chunk_read_time
                    buffer_delay = CHUNK / RATE
                    first_chunk_time -= buffer_delay
                    print(f"ğŸ¤ ì˜¤ë””ì˜¤ ë…¹ìŒ ì‹œì‘ ì‹œê°„: {first_chunk_time} (ë²„í¼ ì§€ì—° {buffer_delay:.3f}ì´ˆ ë³´ì •)")
                
                # ì˜¤ë””ì˜¤ í”„ë ˆì„ ì €ì¥ (ìµœìš°ì„  ì‘ì—…!)
                if audio_frames_queue is not None:
                    relative_time = chunk_read_time - first_chunk_time
                    audio_frames_queue.put((first_chunk_time, relative_time, audio_chunk))
                    
                    if not audio_start_recorded:
                        audio_start_recorded = True
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥
                if chunk_count % 100 == 0:
                    print(f"ğŸ¤ ì˜¤ë””ì˜¤ ë…¹ìŒ ì¤‘: {chunk_count}ê°œ ì²­í¬ ìˆ˜ì§‘ë¨")
                
                # ìŒì„± ì¸ì‹ìš© ë²„í¼ì— ì¶”ê°€ (ë¹„ë¸”ë¡œí‚¹)
                speech_buffer.append(audio_chunk)
                
                # ìŒëŸ‰ ì²´í¬
                try:
                    audio_data = np.frombuffer(audio_chunk, dtype=np.int16)
                    volume = np.abs(audio_data).mean()
                    
                    if volume < SILENCE_THRESHOLD:
                        silence_duration += 1
                    else:
                        silence_duration = 0
                    
                    # ì¹¨ë¬µ ê°ì§€ ì‹œ ìŒì„± ì¸ì‹ íì— ì¶”ê°€ (ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŒ)
                    if len(speech_buffer) > 10 and silence_duration >= MAX_SILENCE_CHUNKS:
                        combined_audio = b''.join(speech_buffer)
                        # íê°€ ê°€ë“ ì°¨ì§€ ì•Šì•˜ìœ¼ë©´ ì¶”ê°€
                        if speech_recognition_queue.qsize() < 5:
                            speech_recognition_queue.put(combined_audio)
                        speech_buffer = []
                        silence_duration = 0
                    
                    # ë²„í¼ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ ì´ˆê¸°í™”
                    if len(speech_buffer) > 300:
                        speech_buffer = speech_buffer[-150:]
                except:
                    pass  # ìŒëŸ‰ ì²´í¬ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                    
            except IOError as e:
                print(f"âš ï¸ ì˜¤ë””ì˜¤ IO ì—ëŸ¬ (ë¬´ì‹œ): {e}")
                continue
            except Exception as e:
                print(f"âš ï¸ ì˜¤ë””ì˜¤ ì½ê¸° ì˜¤ë¥˜: {e}")
                time.sleep(0.001)
                
    finally:
        # ìŒì„± ì¸ì‹ ìŠ¤ë ˆë“œ ì¢…ë£Œ
        speech_recognition_queue.put(None)
        recognition_thread.join(timeout=1)
        
        print(f"ğŸ¤ ì´ {chunk_count}ê°œ ì˜¤ë””ì˜¤ ì²­í¬ ìˆ˜ì§‘ë¨")
        stream.stop_stream()
        stream.close()
        p.terminate()
        print("ğŸ¤ ì˜¤ë””ì˜¤ ë…¹ìŒ ì¢…ë£Œ")

# ê°ì • ë¶„ì„ í•¨ìˆ˜
def analyze_emotion_quick(image: np.ndarray, model, face_detector) -> tuple[str, float, tuple]:
    """ë¹ ë¥¸ ê°ì • ë¶„ì„ (ì‹¤ì‹œê°„ìš©) - ì–¼êµ´ ìœ„ì¹˜ë§Œ ë°˜í™˜"""
    emotion = 'ë‹´ë‹´í•¨'
    confidence = 0.0
    face_bbox = None
    
    if model is None or face_detector is None:
        return emotion, confidence, face_bbox
    
    try:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detector.process(image_rgb)
        
        if results.detections:
            detection = results.detections[0]
            bboxC = detection.location_data.relative_bounding_box
            h, w, _ = image.shape
            x = int(bboxC.xmin * w)
            y = int(bboxC.ymin * h)
            width = int(bboxC.width * w)
            height = int(bboxC.height * h)
            
            x = max(0, x)
            y = max(0, y)
            width = min(width, w - x)
            height = min(height, h - y)
            
            face_bbox = (x, y, width, height)
            
            face = image[y:y+height, x:x+width]
            
            if face.size > 0:
                face_pil = Image.fromarray(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))
                emotion_results = model(face_pil)
                
                if emotion_results:
                    # ì˜ì–´ ê°ì •ì„ í•œê¸€ë¡œ ë³€í™˜
                    emotion_eng = emotion_results[0]['label'].lower()
                    emotion = EMOTION_TRANSLATION.get(emotion_eng, 'ë‹´ë‹´í•¨')
                    confidence = emotion_results[0]['score']
    
    except Exception as e:
        print(f"ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
    
    return emotion, confidence, face_bbox

# ë¹„ë””ì˜¤ ì €ì¥ í•¨ìˆ˜
def save_video(frames: list, filename: str, fps: int = 20):
    """í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¹„ë””ì˜¤ íŒŒì¼ë¡œ ì €ì¥"""
    if not frames or len(frames) == 0:
        return None
    
    height, width, _ = frames[0].shape
    
    # H.264 ì½”ë± ì‚¬ìš© (ë¸Œë¼ìš°ì € í˜¸í™˜ì„± ìš°ìˆ˜)
    # ì—¬ëŸ¬ ì½”ë± ì‹œë„
    codecs_to_try = [
        ('avc1', cv2.VideoWriter_fourcc(*'avc1')),  # H.264
        ('H264', cv2.VideoWriter_fourcc(*'H264')),  # H.264 alternative
        ('X264', cv2.VideoWriter_fourcc(*'X264')),  # H.264 alternative
        ('mp4v', cv2.VideoWriter_fourcc(*'mp4v')),  # MPEG-4 (fallback)
    ]
    
    out = None
    for codec_name, fourcc in codecs_to_try:
        try:
            out = cv2.VideoWriter(filename, fourcc, fps, (width, height))
            if out.isOpened():
                print(f"âœ… ë¹„ë””ì˜¤ ì½”ë± '{codec_name}' ì‚¬ìš©")
                break
            else:
                out.release()
                out = None
        except Exception as e:
            print(f"âš ï¸ ì½”ë± '{codec_name}' ì‹¤íŒ¨: {e}")
            if out:
                out.release()
            out = None
    
    if out is None or not out.isOpened():
        print("âŒ ëª¨ë“  ì½”ë± ì‹¤íŒ¨, ê¸°ë³¸ ì½”ë± ì‚¬ìš©")
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(filename, fourcc, fps, (width, height))
    
    for frame in frames:
        out.write(frame)
    
    out.release()
    print(f"âœ… ë¹„ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {filename}")
    return filename

def save_audio_frames(audio_frames: list, filename: str, trim_start_seconds: float = 0.0):
    """ì˜¤ë””ì˜¤ í”„ë ˆì„ì„ WAV íŒŒì¼ë¡œ ì €ì¥ (ì‹¤ì‹œê°„ ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ ê²°í•©, ì‹œì‘ ë¶€ë¶„ ì œê±° ì˜µì…˜)"""
    if not audio_frames or len(audio_frames) == 0:
        print("âš ï¸ ì €ì¥í•  ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    try:
        import wave
        
        # ëª¨ë“  ì˜¤ë””ì˜¤ ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©
        combined_audio = b''.join(audio_frames)
        
        print(f"ğŸ“Š ì˜¤ë””ì˜¤ ì •ë³´: {len(audio_frames)}ê°œ ì²­í¬, ì´ {len(combined_audio)} bytes")
        
        # ì‹œì‘ ë¶€ë¶„ ì œê±°ê°€ í•„ìš”í•œ ê²½ìš°
        if trim_start_seconds > 0:
            SAMPLE_RATE = 16000
            SAMPLE_WIDTH = 2  # 16-bit = 2 bytes
            
            # ì œê±°í•  ë°”ì´íŠ¸ ìˆ˜ ê³„ì‚°
            bytes_to_trim = int(trim_start_seconds * SAMPLE_RATE * SAMPLE_WIDTH)
            
            # 2ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (16-bit ìƒ˜í”Œì´ë¯€ë¡œ)
            bytes_to_trim = (bytes_to_trim // 2) * 2
            
            if bytes_to_trim < len(combined_audio):
                combined_audio = combined_audio[bytes_to_trim:]
                print(f"âœ‚ï¸ ì‹œì‘ ë¶€ë¶„ {trim_start_seconds:.3f}ì´ˆ ({bytes_to_trim} bytes) ì œê±°")
            else:
                print(f"âš ï¸ ì œê±°í•  ì‹œê°„ì´ ì „ì²´ ì˜¤ë””ì˜¤ë³´ë‹¤ ê¹€")
        
        # WAV íŒŒì¼ë¡œ ì €ì¥
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(1)  # ëª¨ë…¸
            wf.setsampwidth(2)  # 16-bit (pyaudio.paInt16)
            wf.setframerate(16000)  # 16kHz
            wf.writeframes(combined_audio)  # ì „ì²´ ì˜¤ë””ì˜¤ë¥¼ í•œ ë²ˆì— ì €ì¥
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚°
        duration = len(combined_audio) / (2 * 16000)  # 2 bytes per sample, 16000 samples/sec
        print(f"âœ… ì˜¤ë””ì˜¤ ì €ì¥ ì™„ë£Œ: {filename} (ê¸¸ì´: {duration:.2f}ì´ˆ)")
        return filename
    except Exception as e:
        print(f"âŒ ì˜¤ë””ì˜¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def merge_video_audio(video_path: str, audio_path: str, output_path: str, video_fps: float = None):
    """ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ë¥¼ ë³‘í•© (imageio-ffmpeg ì‚¬ìš©, ì •í™•í•œ ì‹±í¬)"""
    try:
        import imageio_ffmpeg as ffmpeg
        import subprocess
        import wave
        import cv2
        
        print("ğŸ¬ ë¹„ë””ì˜¤-ì˜¤ë””ì˜¤ ë³‘í•© ì‹œì‘...")
        
        # ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸
        cap = cv2.VideoCapture(video_path)
        video_frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_fps_original = cap.get(cv2.CAP_PROP_FPS)
        cap.release()
        
        # ì‹¤ì œ FPSê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì›ë³¸ FPS ì‚¬ìš©
        if video_fps is None:
            video_fps = video_fps_original
        
        video_duration = video_frame_count / video_fps if video_fps > 0 else 1
        
        print(f"ğŸ“Š ë¹„ë””ì˜¤ ì •ë³´: {video_frame_count}í”„ë ˆì„, FPS={video_fps:.2f}, ê¸¸ì´={video_duration:.2f}ì´ˆ")
        
        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        with wave.open(audio_path, 'rb') as wf:
            audio_frames = wf.getnframes()
            audio_rate = wf.getframerate()
            audio_duration = audio_frames / float(audio_rate)
        
        print(f"ğŸ“Š ì˜¤ë””ì˜¤ ì •ë³´: ê¸¸ì´={audio_duration:.2f}ì´ˆ, ìƒ˜í”Œë ˆì´íŠ¸={audio_rate}Hz")
        
        # ê¸¸ì´ ì°¨ì´ í™•ì¸
        duration_diff = abs(video_duration - audio_duration)
        print(f"ğŸ“Š ê¸¸ì´ ì°¨ì´: {duration_diff:.2f}ì´ˆ")
        
        # imageio-ffmpegë¥¼ ì‚¬ìš©í•œ ë³‘í•©
        ffmpeg_exe = ffmpeg.get_ffmpeg_exe()
        
        # ffmpeg ëª…ë ¹ì–´ë¡œ ë³‘í•©
        # -r: ì…ë ¥ ë¹„ë””ì˜¤ì˜ í”„ë ˆì„ë ˆì´íŠ¸ ëª…ì‹œì  ì„¤ì •
        # -itsoffset: ì˜¤ë””ì˜¤ ì‹œì‘ ì‹œê°„ ì¡°ì • (í•„ìš”ì‹œ)
        cmd = [
            ffmpeg_exe,
            '-y',  # ë®ì–´ì“°ê¸°
            '-r', str(video_fps),  # ì…ë ¥ ë¹„ë””ì˜¤ FPS ëª…ì‹œ
            '-i', video_path,
            '-i', audio_path,
            '-c:v', 'libx264',  # ë¹„ë””ì˜¤ H.264 ì¸ì½”ë”©
            '-preset', 'ultrafast',  # ë¹ ë¥¸ ì¸ì½”ë”©
            '-r', str(video_fps),  # ì¶œë ¥ ë¹„ë””ì˜¤ FPS ëª…ì‹œ
            '-c:a', 'aac',  # ì˜¤ë””ì˜¤ AAC ì¸ì½”ë”©
            '-b:a', '128k',  # ì˜¤ë””ì˜¤ ë¹„íŠ¸ë ˆì´íŠ¸
            '-strict', 'experimental',
            '-map', '0:v:0',  # ì²« ë²ˆì§¸ ì…ë ¥ì˜ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
            '-map', '1:a:0',  # ë‘ ë²ˆì§¸ ì…ë ¥ì˜ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼
            '-shortest',  # ì§§ì€ ìª½ì— ë§ì¶¤
            '-async', '1',  # ì˜¤ë””ì˜¤ ë™ê¸°í™”
            '-vsync', 'cfr',  # ì¼ì •í•œ í”„ë ˆì„ë ˆì´íŠ¸ ìœ ì§€
            '-max_muxing_queue_size', '1024',  # í í¬ê¸° ì¦ê°€
            output_path
        ]
        
        print(f"ğŸ”§ ffmpeg ëª…ë ¹ ì‹¤í–‰ ì¤‘...")
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True
        )
        
        if result.returncode == 0:
            print(f"âœ… ë¹„ë””ì˜¤-ì˜¤ë””ì˜¤ ë³‘í•© ì™„ë£Œ: {output_path}")
            
            # ê²°ê³¼ íŒŒì¼ ê²€ì¦
            cap_result = cv2.VideoCapture(output_path)
            result_fps = cap_result.get(cv2.CAP_PROP_FPS)
            result_frame_count = int(cap_result.get(cv2.CAP_PROP_FRAME_COUNT))
            cap_result.release()
            
            result_duration = result_frame_count / result_fps if result_fps > 0 else 0
            print(f"âœ… ìµœì¢… ê²°ê³¼: FPS={result_fps:.2f}, ê¸¸ì´={result_duration:.2f}ì´ˆ")
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            try:
                os.remove(video_path)
                os.remove(audio_path)
            except:
                pass
            
            return output_path
        else:
            print(f"âŒ ë³‘í•© ì˜¤ë¥˜:")
            print(result.stderr)
            # ì˜¤ë¥˜ ì‹œ ë¹„ë””ì˜¤ë§Œ ì‚¬ìš©
            try:
                os.rename(video_path, output_path)
            except:
                pass
            return output_path
        
    except ImportError as ie:
        print(f"âš ï¸ imageio-ffmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: {ie}")
        print("ì„¤ì¹˜ ë°©ë²•: pip install imageio-ffmpeg")
        # ë¹„ë””ì˜¤ë¥¼ ìµœì¢… ê²½ë¡œë¡œ ì´ë™
        try:
            os.rename(video_path, output_path)
        except:
            pass
        return output_path
    except Exception as e:
        print(f"âŒ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ì‹œ ë¹„ë””ì˜¤ë¥¼ ìµœì¢… ê²½ë¡œë¡œ ì´ë™
        try:
            os.rename(video_path, output_path)
        except:
            pass
        return output_path

# ë©”ì¸ UI
st.title("ğŸ“” ê°ì • ì˜ìƒ ì¼ê¸° - Emotion Video Diary")
st.markdown("*ì›¹ìº ìœ¼ë¡œ ì˜ìƒ ì¼ê¸°ë¥¼ ê¸°ë¡í•˜ê³ , ì œë¯¸ë‚˜ì´ AIì—ê²Œ ë”°ëœ»í•œ í”¼ë“œë°±ì„ ë°›ì•„ë³´ì„¸ìš”.*")

st.markdown("---")

st.subheader("ğŸ“¹ ì˜ìƒ ì¼ê¸° ê¸°ë¡í•˜ê¸°")

# 1. ë…¹í™” ìƒíƒœ í‘œì‹œ
status_placeholder = st.empty()

# ë…¹í™” ì „ ìƒíƒœ í‘œì‹œ
if not st.session_state.webcam_active and not st.session_state.pending_save:
    status_placeholder.info("ì•„ë˜ ë…¹í™” ì‹œì‘ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”")

# ëª¨ë¸ ë¡œë“œ
with st.spinner("AI ëª¨ë¸ ë¡œë”© ì¤‘..."):
    emotion_model = load_emotion_model()
    face_detector = load_face_detector()

if emotion_model is None or face_detector is None:
    st.error("âš ï¸ AI ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•´ì£¼ì„¸ìš”.")
    st.stop()

# ë ˆì´ì•„ì›ƒ êµ¬ì„±
if st.session_state.pending_save:
    webcam_placeholder = st.empty()
else:
    col_webcam, col_text = st.columns([2, 1])

    with col_webcam:
        webcam_placeholder = st.empty()
        
        if not st.session_state.webcam_active and not st.session_state.pending_save:
            waiting_image = np.zeros((480, 640, 3), dtype=np.uint8)
            waiting_image[:] = (50, 50, 50)
            webcam_placeholder.image(waiting_image, channels="BGR", width=640)

    with col_text:
        voice_text_placeholder = st.empty()
        st.session_state.voice_text_placeholder = voice_text_placeholder  # session_stateì— ì €ì¥
        
        if st.session_state.recording and st.session_state.voice_recording:
            # ë…¹í™” ì¤‘ì¼ ë•Œë§Œ ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ í‘œì‹œ
            current_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì¸ì‹ ì¤‘... ë§ì”€í•´ì£¼ì„¸ìš”)"
            voice_text_placeholder.text_area(
                f"ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥)",
                value=current_text,
                height=480,
                disabled=True,
                key=f"voice_display_{time.time()}"
            )
        else:
            # ë…¹í™” ì¤‘ì´ ì•„ë‹ˆë©´ ê¸°ë³¸ ë©”ì‹œì§€ë§Œ í‘œì‹œ
            voice_text_placeholder.text_area(
                "ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥)",
                value="(ìŒì„± ì…ë ¥ ëŒ€ê¸° ì¤‘...)",
                height=480,
                disabled=True,
                key="voice_display_empty"
            )

# 3. ë…¹í™” ë²„íŠ¼
if not st.session_state.pending_save:
    if not st.session_state.recording:
        start_recording = st.button("ğŸ”´ ë…¹í™” ì‹œì‘", type="primary", use_container_width=True)
    else:
        start_recording = False
        stop_recording = st.button("â¹ï¸ ë…¹í™” ì¤‘ì§€ & ì €ì¥", type="secondary", use_container_width=True)


# ë…¹í™” ì‹œì‘ ì²˜ë¦¬
if 'start_recording' in locals() and start_recording:
    if not st.session_state.recording:
        print(f"ğŸ¬ ë…¹í™” ì‹œì‘ - ì´ì „ transcribed_text: '{st.session_state.get('transcribed_text', '')}'")
        
        # ë¨¼ì € ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ìƒíƒœë¥¼ ë™ì‹œì— ì´ˆê¸°í™”
        st.session_state.recording = True
        st.session_state.webcam_active = True
        st.session_state.video_frames = []
        st.session_state.video_frame_times = []
        st.session_state.emotion_timeline = []
        st.session_state.recording_start_datetime = datetime.now()
        st.session_state.transcribed_text = ""  # ëª…ì‹œì  ì´ˆê¸°í™”
        st.session_state.gemini_advice = None
        
        print(f"âœ… transcribed_text ì´ˆê¸°í™” ì™„ë£Œ: '{st.session_state.transcribed_text}'")
        
        # ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
        st.session_state.voice_recording = True
        st.session_state.audio_queue = queue.Queue()
        st.session_state.audio_frames_queue = queue.Queue()
        st.session_state.audio_frames = []
        st.session_state.stop_event = threading.Event()
        
        # ë™ê¸°í™” ê¸°ì¤€ ì‹œê°„ì€ ì˜¤ë””ì˜¤ ìŠ¤ë ˆë“œ ë‚´ë¶€ì—ì„œ ì„¤ì •
        st.session_state.recording_start_time = None  # ë‚˜ì¤‘ì— ì„¤ì •ë¨
        
        # ì˜¤ë””ì˜¤ ìŠ¤ë ˆë“œ ì‹œì‘ (ë‚´ë¶€ì—ì„œ ì²« í”„ë ˆì„ê³¼ ë™ì‹œì— ì‹œì‘)
        st.session_state.audio_thread = threading.Thread(
            target=record_audio_continuous,
            args=(st.session_state.audio_queue, st.session_state.stop_event, st.session_state.audio_frames_queue, None)
        )
        st.session_state.audio_thread.daemon = True
        st.session_state.audio_thread.start()
        
        print(f"ğŸ¬ ë…¹í™” ì‹œì‘: ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ë™ì‹œ ì‹œì‘")
        
        st.rerun()

# ë…¹í™” ì¤‘ì§€ ì²˜ë¦¬
if st.session_state.recording and 'stop_recording' in locals() and stop_recording:
    # ë…¹í™” ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
    recording_end_time = time.time()
    
    st.session_state.recording = False
    st.session_state.webcam_active = False
    
    if st.session_state.voice_recording:
        st.session_state.stop_event.set()
        st.session_state.voice_recording = False
        time.sleep(0.3)  # ì˜¤ë””ì˜¤ ìŠ¤ë ˆë“œê°€ ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ì²˜ë¦¬í•  ì‹œê°„
        
        # ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì§‘ (íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜)
        audio_frames_with_time = []
        while not st.session_state.audio_frames_queue.empty():
            try:
                audio_data = st.session_state.audio_frames_queue.get_nowait()
                audio_frames_with_time.append(audio_data)
            except queue.Empty:
                break
        
        print(f"ğŸ“Š ìˆ˜ì§‘ëœ ì˜¤ë””ì˜¤ ì²­í¬: {len(audio_frames_with_time)}ê°œ")
        
        # ë¹„ë””ì˜¤-ì˜¤ë””ì˜¤ ë™ê¸°í™”ë¥¼ ìœ„í•œ trim ì‹œê°„ ê³„ì‚°
        trim_start_seconds = 0.0
        
        # ë¹„ë””ì˜¤ ì‹œì‘ ì‹œê°„ê³¼ ì˜¤ë””ì˜¤ ì‹œì‘ ì‹œê°„ì„ ë¹„êµí•˜ì—¬ ë™ê¸°í™”
        if st.session_state.recording_start_time and audio_frames_with_time:
            video_start_time = st.session_state.recording_start_time
            
            # ìƒˆë¡œìš´ í˜•ì‹: (audio_start_time, relative_time, chunk)
            if len(audio_frames_with_time) > 0 and isinstance(audio_frames_with_time[0], tuple) and len(audio_frames_with_time[0]) == 3:
                audio_start_time = audio_frames_with_time[0][0]  # ì˜¤ë””ì˜¤ì˜ ì‹¤ì œ ì‹œì‘ ì‹œê°„
                
                print(f"â° ë¹„ë””ì˜¤ ì‹œì‘: {video_start_time:.6f}")
                print(f"â° ì˜¤ë””ì˜¤ ì‹œì‘: {audio_start_time:.6f}")
                
                time_diff = video_start_time - audio_start_time
                print(f"â° ì‹œê°„ ì°¨ì´: {time_diff:.6f}ì´ˆ (ì–‘ìˆ˜=ì˜¤ë””ì˜¤ê°€ ë¨¼ì €, ìŒìˆ˜=ë¹„ë””ì˜¤ê°€ ë¨¼ì €)")
                
                # ì˜¤ë””ì˜¤ê°€ ë¹„ë””ì˜¤ë³´ë‹¤ ë¨¼ì € ì‹œì‘í•œ ê²½ìš° (ì¼ë°˜ì ì¸ ê²½ìš°)
                if time_diff > 0:
                    print(f"âœ‚ï¸ ì˜¤ë””ì˜¤ ì‹œì‘ ë¶€ë¶„ ì œê±° í•„ìš”: {time_diff:.3f}ì´ˆ")
                    
                    # ëª¨ë“  ì²­í¬ ì‚¬ìš©í•˜ë˜, save_audio_framesì—ì„œ ì •ë°€í•˜ê²Œ ì œê±°
                    st.session_state.audio_frames = [chunk for _, _, chunk in audio_frames_with_time]
                    trim_start_seconds = time_diff
                    
                    print(f"âœ… ì „ì²´ {len(st.session_state.audio_frames)}ê°œ ì²­í¬ ì‚¬ìš©, ì €ì¥ ì‹œ {trim_start_seconds:.3f}ì´ˆ ì œê±° ì˜ˆì •")
                
                # ë¹„ë””ì˜¤ê°€ ì˜¤ë””ì˜¤ë³´ë‹¤ ë¨¼ì € ì‹œì‘í•œ ê²½ìš° (ì´ìƒí•œ ê²½ìš°)
                else:
                    print(f"âš ï¸ ë¹„ì •ìƒ: ë¹„ë””ì˜¤ê°€ ì˜¤ë””ì˜¤ë³´ë‹¤ ë¨¼ì € ì‹œì‘ë¨ - ì „ì²´ ì˜¤ë””ì˜¤ ì‚¬ìš©")
                    st.session_state.audio_frames = [chunk for _, _, chunk in audio_frames_with_time]
                
        # trim ì‹œê°„ ì €ì¥ (ë‚˜ì¤‘ì— save_audio_framesì—ì„œ ì‚¬ìš©)
        st.session_state.audio_trim_start = trim_start_seconds
        
        # ì´ì „ í˜•ì‹ ì²˜ë¦¬
        if not hasattr(st.session_state, 'audio_frames') or not st.session_state.audio_frames:
            # íƒ€ì… ë³€í™˜ (tupleì—ì„œ chunkë§Œ ì¶”ì¶œ)
            st.session_state.audio_frames = []
            for item in audio_frames_with_time:
                if isinstance(item, tuple):
                    if len(item) == 3:
                        st.session_state.audio_frames.append(item[2])  # chunk
                    elif len(item) == 2:
                        st.session_state.audio_frames.append(item[1])  # chunk
                else:
                    st.session_state.audio_frames.append(item)
            st.session_state.audio_trim_start = 0.0
    
    final_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì…ë ¥ ì—†ìŒ)"
    print(f"ğŸ“ ì €ì¥í•  í…ìŠ¤íŠ¸: '{final_text}'")
    print(f"ğŸ“ transcribed_text ìƒíƒœ: '{st.session_state.transcribed_text}'")
    
    if st.session_state.video_frames and len(st.session_state.video_frames) > 0:
        status_placeholder.info("ğŸ’¾ ì˜ìƒ ì¼ê¸° ì €ì¥ ì¤‘...")
        
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            video_filename = f"emotion_diary_{timestamp}.mp4"
            text_filename = f"emotion_diary_{timestamp}.txt"
            
            # ì ˆëŒ€ ê²½ë¡œë¡œ ì €ì¥ (ì¤‘ìš”!)
            video_temp_path = str(VIDEOS_DIR.absolute() / f"temp_video_{timestamp}.mp4")
            audio_temp_path = str(VIDEOS_DIR.absolute() / f"temp_audio_{timestamp}.wav")
            video_path = str(VIDEOS_DIR.absolute() / video_filename)
            text_path = str(VIDEOS_DIR.absolute() / text_filename)
            
            # ì‹¤ì œ ë…¹í™” ì‹œê°„ ê³„ì‚° (ì´ˆ ë‹¨ìœ„)
            recording_end_time = time.time()
            actual_video_duration = recording_end_time - st.session_state.recording_start_time
            
            print(f"ğŸ“Š ë…¹í™” ì •ë³´ (ì‹œê°„ ê¸°ë°˜): í”„ë ˆì„={len(st.session_state.video_frames)}, ë¹„ë””ì˜¤ ì‹œê°„={actual_video_duration:.2f}ì´ˆ")
            
            # â­ ë¨¼ì € ì˜¤ë””ì˜¤ë¥¼ ì €ì¥í•˜ì—¬ ì •í™•í•œ ê¸¸ì´ íŒŒì•…
            actual_fps = len(st.session_state.video_frames) / actual_video_duration if actual_video_duration > 0 else 20
            
            if st.session_state.audio_frames and len(st.session_state.audio_frames) > 0:
                # trim ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
                trim_start = getattr(st.session_state, 'audio_trim_start', 0.0)
                audio_saved = save_audio_frames(st.session_state.audio_frames, audio_temp_path, trim_start_seconds=trim_start)
                
                if audio_saved:
                    # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
                    import wave
                    with wave.open(audio_temp_path, 'rb') as wf:
                        audio_frames_count = wf.getnframes()
                        audio_rate = wf.getframerate()
                        audio_duration = audio_frames_count / float(audio_rate)
                    
                    print(f"ğŸ¤ ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")
                    
                    # â­ í•µì‹¬: ë¹„ë””ì˜¤ FPSë¥¼ ì˜¤ë””ì˜¤ ê¸¸ì´ì— ì •í™•íˆ ë§ì¶¤
                    actual_fps = len(st.session_state.video_frames) / audio_duration if audio_duration > 0 else actual_fps
                    
                    print(f"ğŸ¬ ì˜¤ë””ì˜¤ ê¸°ì¤€ ì •í™•í•œ FPS: {actual_fps:.2f}")
                    print(f"ğŸ“Š ë¹„ë””ì˜¤ ê¸¸ì´ (ì˜¤ë””ì˜¤ ë§ì¶¤): {len(st.session_state.video_frames) / actual_fps:.2f}ì´ˆ")
                    print(f"âœ… ì˜ˆìƒ ê¸¸ì´ ì°¨ì´: 0.00ì´ˆ (ì™„ë²½í•œ ë™ê¸°í™”!)")
            
            # ì •í™•í•œ FPSë¡œ ë¹„ë””ì˜¤ ì €ì¥
            print(f"ğŸ¬ ë¹„ë””ì˜¤ ì €ì¥ ì¤‘ (FPS={actual_fps:.2f})...")
            save_video(st.session_state.video_frames, video_temp_path, fps=actual_fps)
            
            # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ë³‘í•©
            if st.session_state.audio_frames and len(st.session_state.audio_frames) > 0:
                if audio_saved:
                    
                    # ë¹„ë””ì˜¤ì™€ ì˜¤ë””ì˜¤ ë³‘í•© (ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ì¤€ FPS ì‚¬ìš©)
                    video_path = merge_video_audio(video_temp_path, audio_temp_path, video_path, video_fps=actual_fps)
                else:
                    # ì˜¤ë””ì˜¤ ì €ì¥ ì‹¤íŒ¨ ì‹œ ë¹„ë””ì˜¤ë§Œ ì‚¬ìš©
                    os.rename(video_temp_path, video_path)
            else:
                # ì˜¤ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ë¹„ë””ì˜¤ë§Œ ì €ì¥
                os.rename(video_temp_path, video_path)
                print("âš ï¸ ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ë§Œ ì €ì¥ë©ë‹ˆë‹¤.")
            
            if st.session_state.emotion_timeline:
                emotions_list = [e['emotion'] for e in st.session_state.emotion_timeline]
                emotion_counts = pd.Series(emotions_list).value_counts()
                dominant_emotion = emotion_counts.index[0] if len(emotion_counts) > 0 else "ë‹´ë‹´í•¨"
                avg_confidence = np.mean([e['confidence'] for e in st.session_state.emotion_timeline])
            else:
                dominant_emotion = "ë‹´ë‹´í•¨"
                avg_confidence = 0.0
            
            # ë§ì¶¤í˜• AI ì˜ˆì¸¡
            personalized_emotion, personalized_confidence, is_personalized = \
                st.session_state.personalized_model.predict(
                    st.session_state.emotion_timeline,
                    final_text,
                    dominant_emotion
                )
            
            if st.session_state.recording_start_datetime:
                elapsed = datetime.now() - st.session_state.recording_start_datetime
                elapsed_seconds = int(elapsed.total_seconds())
                recording_duration = f"{elapsed_seconds // 60:02d}:{elapsed_seconds % 60:02d}"
            else:
                recording_duration = "00:00"
            
            st.session_state.save_data = {
                'timestamp': timestamp,
                'video_filename': video_filename,
                'video_path': video_path,
                'text_filename': text_filename,
                'text_path': text_path,
                'final_text': final_text,
                'dominant_emotion': dominant_emotion,
                'avg_confidence': avg_confidence,
                'frame_count': len(st.session_state.video_frames),
                'recording_duration': recording_duration,
                'emotion_timeline': st.session_state.emotion_timeline.copy(),
                'anonymize_method': anonymize_option,
                'personalized_emotion': personalized_emotion,
                'personalized_confidence': personalized_confidence,
                'is_personalized': is_personalized,
                'actual_fps': actual_fps  # ì‹¤ì œ FPS ì €ì¥
            }
            
            st.session_state.pending_save = True
            st.session_state.emotion_confirmed = False
            st.session_state.confirmed_emotion = None
            st.session_state.advice_loading = False
            st.session_state.gemini_advice = None
            st.session_state.processing_emotion = None
            st.session_state.video_frames = []
            st.session_state.audio_frames = []
            st.session_state.recording_start_time = None
            st.session_state.recording_start_datetime = None
            # transcribed_textëŠ” ìœ ì§€ (ê°ì • ì„ íƒ í™”ë©´ì—ì„œ í‘œì‹œìš©)
            
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            import traceback
            st.error(traceback.format_exc())
    else:
        st.warning("âš ï¸ ë…¹í™”ëœ í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤!")
        st.session_state.video_frames = []
        st.session_state.recording_start_time = None
        st.session_state.recording_start_datetime = None

# ê¸°ë¶„ ì„ íƒ UI
if st.session_state.pending_save and st.session_state.save_data:
    save_data = st.session_state.save_data
    
    if not st.session_state.emotion_confirmed:
        # ë¡œë”© ì¤‘ì¼ ë•Œ ë©”ì‹œì§€ í‘œì‹œ
        if st.session_state.get('advice_loading', False):
            status_placeholder.info("ì œë¯¸ë‚˜ì´ AIê°€ ì¼ê¸°ë¥¼ ë¶„ì„í•˜ê³  ì¡°ì–¸ì„ ì‘ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        # AI ì¶”ì²œ í‘œì‹œ (ë¡œë”© ì¤‘ì´ ì•„ë‹ ë•Œ)
        elif save_data['is_personalized']:
            status_placeholder.success(
                f"âœ¨ ë§ì¶¤í˜• AI ì¶”ì²œ: **{save_data['personalized_emotion']}** "
                f"(í™•ì‹ ë„: {save_data['personalized_confidence']*100:.1f}%)"
            )
            st.info("ì‚¬ìš©ìë‹˜ì˜ ê³¼ê±° ê°ì • íŒ¨í„´ì„ ë¶„ì„í•œ ë§ì¶¤ ì¶”ì²œì…ë‹ˆë‹¤!")
        else:
            status_placeholder.info(
                f"âœ¨ AI ì¶”ì²œ: **{save_data['dominant_emotion']}** "
                f"(ê¸°ë³¸ ë¶„ì„ ê²°ê³¼)"
            )
        
        emotion_options = [
            "ğŸ˜Š í–‰ë³µí•¨",
            "ğŸ˜¢ ìŠ¬í””",
            "ğŸ˜  í™”ë‚¨",
            "ğŸ˜² ë†€ëŒ",
            "ğŸ˜ ë‹´ë‹´í•¨",
            "ğŸ˜¨ ë‘ë ¤ì›€",
            "ğŸ¤¢ í˜ì˜¤"
        ]
        
        # ì¶”ì²œ ê°ì •ì„ ê¸°ë³¸ ì„ íƒìœ¼ë¡œ
        recommended_emotion = save_data['personalized_emotion'] if save_data['is_personalized'] else save_data['dominant_emotion']
        emotion_map = {
            'í–‰ë³µí•¨': 0, 'ìŠ¬í””': 1, 'í™”ë‚¨': 2, 'ë†€ëŒ': 3,
            'ë‹´ë‹´í•¨': 4, 'ë‘ë ¤ì›€': 5, 'í˜ì˜¤': 6
        }
        
        default_index = emotion_map.get(recommended_emotion, 4)
        
        selected_emotion = st.radio(
            "ğŸ­ ì˜¤ëŠ˜ì˜ ê°ì •ì„ ì„ íƒí•´ì£¼ì„¸ìš”:",
            emotion_options,
            index=default_index,
            key="emotion_radio",
            disabled=st.session_state.get('advice_loading', False)
        )
        
        confirm_emotion = st.button(
            "âœ… ê°ì • í™•ì •í•˜ê¸°", 
            type="primary", 
            use_container_width=True, 
            key="confirm_bottom_btn",
            disabled=st.session_state.get('advice_loading', False)
        )
        
        if confirm_emotion:
            # ì„ íƒëœ ê°ì •ì—ì„œ í•œê¸€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            final_mood = selected_emotion.split()[1]
            
            # í•™ìŠµ ë°ì´í„°ì— ì¶”ê°€
            st.session_state.personalized_model.add_training_sample(
                save_data['emotion_timeline'],
                save_data['final_text'],
                save_data['dominant_emotion'],
                final_mood
            )
            
            # ëª¨ë¸ í•™ìŠµ (3ê°œ ì´ìƒì¼ ë•Œ)
            if len(st.session_state.personalized_model.training_data) >= 3:
                if st.session_state.personalized_model.train():
                    pass  # ì¡°ìš©íˆ í•™ìŠµë§Œ ì§„í–‰
            
            # ì œë¯¸ë‚˜ì´ AI ì¡°ì–¸ ìƒì„± í”Œë˜ê·¸ ì„¤ì •
            st.session_state.advice_loading = True
            st.session_state.processing_emotion = final_mood
            st.rerun()  # ì¦‰ì‹œ reruní•˜ì—¬ ë²„íŠ¼ ë¹„í™œì„±í™”
    
    # advice_loadingì´ Trueì¸ ê²½ìš° ì¡°ì–¸ ìƒì„± (ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬)
    if st.session_state.get('advice_loading', False) and not st.session_state.emotion_confirmed:
        # ì´ë¯¸ ìœ„ì—ì„œ status_placeholder.info()ë¥¼ í˜¸ì¶œí–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìƒëµ
        
        final_mood = st.session_state.processing_emotion
        
        gemini_advice = get_gemini_advice(
            final_mood,
            save_data['final_text'],
            save_data['emotion_timeline']
        )
        st.session_state.gemini_advice = gemini_advice
        st.session_state.advice_loading = False
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ (ì œë¯¸ë‚˜ì´ ì¡°ì–¸ í¬í•¨)
        with open(save_data['text_path'], 'w', encoding='utf-8') as f:
            f.write(f"=== ê°ì • ì˜ìƒ ì¼ê¸° ===\n")
            f.write(f"ë‚ ì§œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}\n")
            f.write(f"ì˜¤ëŠ˜ì˜ ê°ì •: {final_mood}\n")
            f.write(f"ìµëª…í™” ë°©ì‹: {save_data['anonymize_method']}\n")
            f.write(f"\n=== ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥) ===\n\n")
            f.write(save_data['final_text'])
            f.write(f"\n\n=== ê°ì • ë¶„ì„ ê²°ê³¼ ===\n")
            if save_data['emotion_timeline']:
                emotions_list = [e['emotion'] for e in save_data['emotion_timeline']]
                emotion_counts = pd.Series(emotions_list).value_counts()
                f.write(f"ì£¼ìš” ê°ì •: {save_data['dominant_emotion']}\n")
                f.write(f"í‰ê·  í™•ì‹ ë„: {save_data['avg_confidence']*100:.1f}%\n")
                f.write(f"\nê°ì • ë¶„í¬:\n")
                for emotion, count in emotion_counts.items():
                    percentage = (count / len(emotions_list)) * 100
                    f.write(f"  - {emotion}: {count}íšŒ ({percentage:.1f}%)\n")
            f.write(f"\n=== AI ì¶”ì²œ ===\n")
            if save_data['is_personalized']:
                f.write(f"ë§ì¶¤í˜• AI ì¶”ì²œ: {save_data['personalized_emotion']} (í™•ì‹ ë„: {save_data['personalized_confidence']*100:.1f}%)\n")
            else:
                f.write(f"ê¸°ë³¸ AI ë¶„ì„: {save_data['dominant_emotion']}\n")
            f.write(f"ìµœì¢… ì„ íƒ: {final_mood}\n")
            
            # ì œë¯¸ë‚˜ì´ ì¡°ì–¸ ì¶”ê°€
            f.write(f"\n=== ì œë¯¸ë‚˜ì´ AIì˜ ì¡°ì–¸ ===\n\n")
            f.write(gemini_advice)
        
        # ì¼ê¸° í•­ëª© ì €ì¥
        entry = {
            'timestamp': save_data['timestamp'],
            'emotion': final_mood,
            'diary_text': save_data['final_text'],
            'video_filename': save_data['video_filename'],
            'video_path': save_data['video_path'],
            'text_filename': save_data['text_filename'],
            'text_path': save_data['text_path'],
            'dominant_emotion': save_data['dominant_emotion'],
            'avg_confidence': save_data['avg_confidence'],
            'frame_count': save_data['frame_count'],
            'recording_duration': save_data['recording_duration'],
            'emotion_timeline': save_data['emotion_timeline'],
            'anonymize_method': save_data['anonymize_method'],
            'voice_input_used': True,
            'ai_recommended': save_data['personalized_emotion'] if save_data['is_personalized'] else save_data['dominant_emotion'],
            'is_personalized': save_data['is_personalized'],
            'gemini_advice': gemini_advice
        }
        
        st.session_state.diary_entries.append(entry)
        
        # ë¡œì»¬ì— ì €ì¥
        save_local_data(st.session_state.diary_entries)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        stats = st.session_state.user_stats
        stats['total_entries'] += 1
        stats['emotion_distribution'][final_mood] = stats['emotion_distribution'].get(final_mood, 0) + 1
        
        # AI-ì‚¬ìš©ì ì¼ì¹˜ìœ¨ ê³„ì‚°
        recommended = save_data['personalized_emotion'] if save_data['is_personalized'] else save_data['dominant_emotion']
        matches = sum(1 for e in st.session_state.diary_entries 
                     if e['emotion'] == e['ai_recommended'])
        stats['ai_vs_user_agreement'] = (matches / stats['total_entries']) * 100
        stats['last_updated'] = datetime.now().isoformat()
        
        save_user_stats(stats)
        st.session_state.user_stats = stats
        
        st.session_state.confirmed_emotion = final_mood
        st.session_state.emotion_confirmed = True
        
        st.rerun()
    
    # ê°ì •ì´ í™•ì •ë˜ê³  ì¡°ì–¸ë„ ìƒì„±ëœ í›„ ìµœì¢… ê²°ê³¼ í‘œì‹œ
    elif st.session_state.emotion_confirmed and st.session_state.confirmed_emotion:
        status_placeholder.success(f"âœ… ì˜ìƒ ì¼ê¸°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤! (ê°ì •: {st.session_state.confirmed_emotion})")
        
        # ì œë¯¸ë‚˜ì´ ì¡°ì–¸ í‘œì‹œ
        if st.session_state.gemini_advice:
            st.info(st.session_state.gemini_advice)
        
        if save_data['emotion_timeline'] and len(save_data['emotion_timeline']) > 0:
            timeline_df = pd.DataFrame(save_data['emotion_timeline'])
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                emotion_counts = timeline_df['emotion'].value_counts()
                fig_pie = px.pie(
                    values=emotion_counts.values,
                    names=emotion_counts.index,
                    title="ì˜¤ëŠ˜ì˜ ê°ì • ë¶„í¬",
                    color_discrete_sequence=px.colors.qualitative.Set3
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col_chart2:
                timeline_df['time_seconds'] = timeline_df['frame'] / 20
                timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
                
                max_time = timeline_df['time_seconds'].max()
                
                fig_line = px.line(
                    timeline_df,
                    x='time_seconds',
                    y='confidence_percent',
                    color='emotion',
                    title="ì˜¤ëŠ˜ì˜ ê°ì • ë³€í™”",
                    markers=True,
                    labels={
                        'time_seconds': 'ì˜ìƒ ì‹œê°„ (ì´ˆ)',
                        'confidence_percent': 'í™•ì‹ ë„ (%)',
                        'emotion': 'ê°ì •'
                    }
                )
                
                fig_line.update_yaxes(range=[0, 100], dtick=10, title="í™•ì‹ ë„ (%)")
                
                import math
                x_max = math.ceil(max_time / 10) * 10
                fig_line.update_xaxes(range=[0, x_max], dtick=10, title="ì˜ìƒ ì‹œê°„ (ì´ˆ)")
                
                st.plotly_chart(fig_line, use_container_width=True)
            
            st.markdown("**ğŸ“‹ ê°ì • íƒ€ì„ë¼ì¸**")
            dominant_emoji = emotion_emoji_map.get(save_data.get('dominant_emotion', 'ë‹´ë‹´í•¨'), 'ğŸ“')
            st.markdown(
                f"<small>ì£¼ìš” ê°ì •: {dominant_emoji} {save_data.get('dominant_emotion', 'ë‹´ë‹´í•¨')}</small>",
                unsafe_allow_html=True
            )
            st.markdown(
                f"<small>í‰ê·  í™•ì‹ ë„: {save_data.get('avg_confidence', 0)*100:.1f}%</small>",
                unsafe_allow_html=True
            )
            display_timeline = timeline_df[['frame', 'timestamp', 'emotion', 'confidence']].copy()
            display_timeline['confidence'] = display_timeline['confidence'].apply(lambda x: f"{x*100:.1f}%")
            display_timeline.columns = ['í”„ë ˆì„', 'ì‹œê°„', 'ê°ì •', 'í™•ì‹ ë„']
            st.dataframe(display_timeline, use_container_width=True, height=200)
        
        col_dl1, col_dl2 = st.columns(2)
        
        with col_dl1:
            if os.path.exists(save_data['video_path']):
                with open(save_data['video_path'], 'rb') as f:
                    video_bytes = f.read()
                    st.download_button(
                        label="ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                        data=video_bytes,
                        file_name=save_data['video_filename'],
                        mime="video/mp4",
                        type="secondary",
                        use_container_width=True,
                        key="download_video_saved"
                    )
            else:
                st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
        
        with col_dl2:
            if os.path.exists(save_data['text_path']):
                with open(save_data['text_path'], 'r', encoding='utf-8') as f:
                    text_content = f.read()
                    st.download_button(
                        label="ğŸ“„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                        data=text_content,
                        file_name=save_data['text_filename'],
                        mime="text/plain",
                        type="secondary",
                        use_container_width=True,
                        key="download_text_saved"
                    )
            else:
                st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
        
        complete_action = st.button("âœ… í™•ì¸ ì™„ë£Œ", type="primary", use_container_width=True, key="complete_bottom_btn")
        
        if complete_action:
            st.session_state.pending_save = False
            st.session_state.save_data = None
            st.session_state.emotion_confirmed = False
            st.session_state.confirmed_emotion = None
            st.session_state.gemini_advice = None
            st.session_state.transcribed_text = ""  # ìŒì„± í…ìŠ¤íŠ¸ ì´ˆê¸°í™” ì¶”ê°€
            st.session_state.audio_frames = []  # ì˜¤ë””ì˜¤ í”„ë ˆì„ë„ ì´ˆê¸°í™”
            st.session_state.video_frames = []  # ë¹„ë””ì˜¤ í”„ë ˆì„ë„ ì´ˆê¸°í™”
            
            st.rerun()

# ìµëª…í™” ë§µí•‘
anonymize_map = {
    "ì›ë³¸": None,
    "ë¸”ëŸ¬": "blur",
    "ê³° ì–¼êµ´ ğŸ»": "bear",
    "í† ë¼ ì–¼êµ´ ğŸ°": "rabbit",
    "ê³ ì–‘ì´ ì–¼êµ´ ğŸ±": "cat"
}

# ì›¹ìº  ì‹¤í–‰
if st.session_state.webcam_active:
    loading_image = np.zeros((480, 640, 3), dtype=np.uint8)
    loading_image[:] = (50, 50, 50)
    
    webcam_placeholder.image(loading_image, channels="BGR", width=640)
    status_placeholder.info("ğŸ“¹ ì›¹ìº ì„ ì‹œì‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
    
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        status_placeholder.error("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›¹ìº ì´ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        st.session_state.webcam_active = False
    else:
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        init_image = np.zeros((480, 640, 3), dtype=np.uint8)
        init_image[:] = (50, 50, 50)
        
        webcam_placeholder.image(init_image, channels="BGR", width=640)
        status_placeholder.info("ğŸ“¹ ì›¹ìº  ì´ˆê¸°í™” ì¤‘...")
        
        for _ in range(5):
            ret, _ = cap.read()
            if not ret:
                break
            time.sleep(0.1)
        
        ready_image = np.zeros((480, 640, 3), dtype=np.uint8)
        ready_image[:] = (50, 50, 50)
        
        webcam_placeholder.image(ready_image, channels="BGR", width=640)
        status_placeholder.success("âœ… ì›¹ìº  ì¤€ë¹„ ì™„ë£Œ! ğŸ¤ ìŒì„± ì…ë ¥ í™œì„±í™”ë¨!")
        time.sleep(0.5)
        
        frame_count = 0
        
        while st.session_state.webcam_active:
            ret, frame = cap.read()
            
            if not ret:
                st.error("âŒ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            text_updated = False
            if st.session_state.voice_recording:
                try:
                    while not st.session_state.audio_queue.empty():
                        new_text = st.session_state.audio_queue.get_nowait()
                        if st.session_state.transcribed_text:
                            st.session_state.transcribed_text += " " + new_text
                        else:
                            st.session_state.transcribed_text = new_text
                        text_updated = True
                        print(f"ğŸ“ ìŒì„± í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸: '{st.session_state.transcribed_text}'")
                except queue.Empty:
                    pass
            
            # í…ìŠ¤íŠ¸ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìœ¼ë©´ í™”ë©´ì— ë°˜ì˜
            if text_updated and hasattr(st.session_state, 'voice_text_placeholder'):
                current_text = st.session_state.transcribed_text if st.session_state.transcribed_text else "(ìŒì„± ì¸ì‹ ì¤‘... ë§ì”€í•´ì£¼ì„¸ìš”)"
                st.session_state.voice_text_placeholder.text_area(
                    f"ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥)",
                    value=current_text,
                    height=480,
                    disabled=True,
                    key=f"voice_update_{time.time()}"
                )
            
            anonymized_frame = frame.copy()
            if anonymize_map[anonymize_option] == "blur":
                anonymized_frame = blur_frame(anonymized_frame)
            elif anonymize_map[anonymize_option] == "bear":
                anonymized_frame = bear_face_mask(anonymized_frame, face_detector)
            elif anonymize_map[anonymize_option] == "rabbit":
                anonymized_frame = rabbit_face_mask(anonymized_frame, face_detector)
            elif anonymize_map[anonymize_option] == "cat":
                anonymized_frame = cat_face_mask(anonymized_frame, face_detector)
            
            face_bbox = None
            if frame_count % 3 == 0:
                emotion, confidence, face_bbox = analyze_emotion_quick(
                    frame.copy(), emotion_model, face_detector
                )
                st.session_state.current_emotion = emotion
                
                if st.session_state.recording:
                    st.session_state.emotion_timeline.append({
                        'frame': len(st.session_state.video_frames) + 1,
                        'emotion': emotion,
                        'confidence': confidence,
                        'timestamp': datetime.now().strftime('%H:%M:%S')
                    })
            
            display_frame = anonymized_frame.copy()
            
            if face_bbox:
                x, y, w, h = face_bbox
                emotion = st.session_state.current_emotion
                
                if frame_count % 3 == 0 and 'confidence' in locals():
                    text = f"{emotion} ({confidence*100:.1f}%)"
                else:
                    text = f"{emotion}"
                
                cv2.rectangle(display_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                
                display_frame = put_korean_text(
                    display_frame,
                    text,
                    (x, y-35),
                    font_size=20,
                    color=(0, 255, 0)
                )
            
            if st.session_state.recording:
                # ì²« í”„ë ˆì„ ì €ì¥ ì‹œ ì •í™•í•œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                if len(st.session_state.video_frames) == 0:
                    st.session_state.recording_start_time = time.time()
                    print(f"ğŸ¬ ì²« í”„ë ˆì„ ìº¡ì²˜: ì‹œì‘ ì‹œê°„ = {st.session_state.recording_start_time}")
                
                st.session_state.video_frames.append(display_frame)
                # ê° í”„ë ˆì„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë¡ (ë™ê¸°í™”ë¥¼ ìœ„í•´)
                if hasattr(st.session_state, 'video_frame_times'):
                    st.session_state.video_frame_times.append(time.time() - st.session_state.recording_start_time)
            
            frame_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)
            webcam_placeholder.image(frame_rgb, channels="RGB", width=640)
            
            # ì „ì—­ emotion_emoji_map ì‚¬ìš©
            emoji = emotion_emoji_map.get(st.session_state.current_emotion, 'ğŸ˜')
            
            if st.session_state.recording:
                if st.session_state.recording_start_datetime:
                    elapsed = datetime.now() - st.session_state.recording_start_datetime
                    elapsed_seconds = int(elapsed.total_seconds())
                    minutes = elapsed_seconds // 60
                    seconds = elapsed_seconds % 60
                    
                    voice_status = ""
                    if st.session_state.voice_recording:
                        word_count = len(st.session_state.transcribed_text.split()) if st.session_state.transcribed_text else 0
                        voice_status = f" | ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘ (ë‹¨ì–´: {word_count}ê°œ)"
                    
                    emotion_status = f" | {emoji} {st.session_state.current_emotion}"
                    
                    status_placeholder.success(
                        f"ğŸ”´ ë…¹í™” ì¤‘: {minutes:02d}:{seconds:02d} | í”„ë ˆì„: {len(st.session_state.video_frames)}{emotion_status}{voice_status}"
                    )
            else:
                if len(st.session_state.video_frames) > 0:
                    status_placeholder.warning(f"â¹ï¸ ë…¹í™” ì¤‘ì§€ë¨ ({len(st.session_state.video_frames)} í”„ë ˆì„)")
                else:
                    status_placeholder.info("âšª ëŒ€ê¸° ì¤‘")
            
            frame_count += 1
            
            # í”„ë ˆì„ ê°„ ëŒ€ê¸° ì‹œê°„ ì¶”ê°€ (ì¼ì •í•œ FPS ìœ ì§€)
            # ëª©í‘œ: ì•½ 20 FPS (0.05ì´ˆ ê°„ê²©)
            time.sleep(0.001)  # ìµœì†Œ ëŒ€ê¸°ë¡œ CPU ë¶€í•˜ ê°ì†Œ, ì‹¤ì œ FPSëŠ” ìë™ ê³„ì‚°ë¨
        
        cap.release()

# ì €ì¥ëœ ì¼ê¸° ëª©ë¡
st.markdown("---")
st.subheader("ğŸ“š ì €ì¥ëœ ì˜ìƒ ì¼ê¸°")

if st.session_state.diary_entries:

    
    # ê°œë³„ ì¼ê¸°
    for i, entry in enumerate(reversed(st.session_state.diary_entries)):
        emotion = entry.get('emotion', 'ë¯¸ê¸°ë¡')
        emotion_emoji = emotion_emoji_map.get(emotion, 'ğŸ“')
        emotion_display = f" - {emotion_emoji} {emotion}" if 'emotion' in entry else ""
        ai_rec = entry.get('ai_recommended', 'ì—†ìŒ')
        is_personalized = "AI ë§ì¶¤ ì¶”ì²œ" if entry.get('is_personalized', False) else "AI ê¸°ë³¸ ì¶”ì²œ"
        
        with st.expander(f"ğŸ“” ì¼ê¸° #{len(st.session_state.diary_entries)-i} - {entry['timestamp']}{emotion_display}"):
            # ì˜ìƒ ì¬ìƒ (ìƒë‹¨, ì™¼ìª½)
            col_video, col_text = st.columns([2, 1])
            
            with col_video:
                st.markdown("**ğŸ¬ ì˜ìƒ ì¬ìƒ**")
                
                video_path = entry.get('video_path', '')
                video_filename = entry.get('video_filename', '')
                
                if video_path and os.path.exists(video_path):
                    try:
                        # ë‘¥ê·¼ ëª¨ì„œë¦¬ ìŠ¤íƒ€ì¼ ì ìš©
                        st.markdown("""
                        <style>
                        video {
                            border-radius: 10px;
                            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
                            max-width: 640px;
                            width: 100%;
                            height: auto;
                        }
                        /* í…ìŠ¤íŠ¸ ì˜ì—­ê³¼ ë¶€ëª¨ div ë†’ì´ë¥¼ ì˜ìƒ(480px)ê³¼ ë™ì¼í•˜ê²Œ */
                        [data-baseweb="base-input"] {
                            height: 270px !important;
                        }
                        textarea {
                            height: 270px !important;
                            max-height: 480px !important;
                            overflow-y: auto !important;
                        }
                        </style>
                        """, unsafe_allow_html=True)
                        

                        # ìƒëŒ€ ê²½ë¡œë¡œ ì˜ìƒ ì¬ìƒ
                        st.video(f"emotion_diary_data/videos/{video_filename}")
                        
                    except Exception as e:
                        st.error(f"âŒ ì˜ìƒ ì¬ìƒ ì˜¤ë¥˜: {e}")
                else:
                    st.warning(f"âš ï¸ ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            with col_text:
                st.write("**ğŸ¤ ìŒì„± ì…ë ¥:** ì‚¬ìš©ë¨ âœ…")
                
                # diary_textê°€ ì—†ìœ¼ë©´ í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ê¸°
                diary_text = entry.get('diary_text', '')
                
                print(f"ğŸ“ ì¼ê¸° #{len(st.session_state.diary_entries)-i} ({entry.get('timestamp', 'unknown')}) í…ìŠ¤íŠ¸ ë¡œë“œ ì‹œë„")
                print(f"  - diary_text from entry: '{diary_text[:50] if diary_text else '(empty)'}...' (ê¸¸ì´: {len(diary_text)})")
                print(f"  - text_path: {entry.get('text_path', 'None')}")
                
                if not diary_text and entry.get('text_path'):
                    text_path = entry.get('text_path')
                    print(f"  - í…ìŠ¤íŠ¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(text_path)}")
                    
                    if os.path.exists(text_path):
                        try:
                            with open(text_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                print(f"  - íŒŒì¼ ë‚´ìš© ê¸¸ì´: {len(content)}")
                                
                                # "=== ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥) ===" ë¶€ë¶„ ì¶”ì¶œ
                                if "=== ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥) ===" in content:
                                    parts = content.split("=== ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥) ===")
                                    if len(parts) > 1:
                                        # ë‹¤ìŒ === ê¹Œì§€ ì¶”ì¶œ
                                        text_section = parts[1]
                                        if "===" in text_section:
                                            diary_text = text_section.split("===")[0].strip()
                                        else:
                                            diary_text = text_section.strip()
                                        
                                        print(f"  - ì¶”ì¶œëœ í…ìŠ¤íŠ¸: '{diary_text[:50]}...'")
                                        
                                        if not diary_text:
                                            diary_text = "(ìŒì„± ì…ë ¥ ì—†ìŒ)"
                                else:
                                    print(f"  - êµ¬ë¶„ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                                    diary_text = "(í…ìŠ¤íŠ¸ íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜)"
                        except Exception as e:
                            print(f"  - í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
                            import traceback
                            traceback.print_exc()
                            diary_text = f"(í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e})"
                
                if not diary_text:
                    diary_text = "(ìŒì„± ì…ë ¥ ì—†ìŒ)"
                
                print(f"  - ìµœì¢… í‘œì‹œ í…ìŠ¤íŠ¸: '{diary_text[:50] if len(diary_text) > 50 else diary_text}...' (ê¸¸ì´: {len(diary_text)})")
                
                # timestampë¥¼ ì‚¬ìš©í•œ ê³ ìœ  í‚¤ë¡œ Streamlit ìºì‹± ë¬¸ì œ ë°©ì§€
                unique_key = f"voice_display_{entry.get('timestamp', i)}"
                
                st.text_area(
                    f"ì¼ê¸° ë‚´ìš© (ìŒì„± ì…ë ¥)",
                    value=diary_text,
                    disabled=True,
                    key=unique_key
                )
            
            # ê°ì • ì •ë³´
            st.markdown("---")
            col_info1, col_info2 = st.columns(2)
            with col_info1:
                if 'emotion' in entry:
                    user_emotion_emoji = emotion_emoji_map.get(entry['emotion'], 'ğŸ“')
                    st.write(f"**âœ¨ ê·¸ë‚ ì˜ ê°ì •:** {user_emotion_emoji} {entry['emotion']}")
                
                ai_rec_emoji = emotion_emoji_map.get(ai_rec, 'ğŸ“')
                if entry['emotion'] == ai_rec:
                    st.success("âœ… AI ì¶”ì²œê³¼ ì¼ì¹˜")
                else:
                    st.info(f"â„¹ï¸ ì‚¬ìš©ìê°€ ë‹¤ë¥¸ ê°ì • ì„ íƒ (AIì œì•ˆ: {ai_rec_emoji} {ai_rec})")

        
                
            with col_info2:
                st.write(f"**ğŸ¬ í”„ë ˆì„ ìˆ˜:** {entry['frame_count']}")
                st.write(f"**ğŸ”’ ìµëª…í™”:** {entry['anonymize_method']}")
                st.write(f"**â±ï¸ ë…¹í™” ì‹œê°„:** {entry.get('recording_duration', '00:00')}")
                st.write(f"**ğŸ“ ì˜ìƒ ê¸¸ì´:** ì•½ {entry['frame_count'] / 20:.1f}ì´ˆ")
            
            # ì œë¯¸ë‚˜ì´ ì¡°ì–¸ í‘œì‹œ
            if entry.get('gemini_advice'):
                st.markdown("---")
                st.info(entry['gemini_advice'])

            if entry.get('emotion_timeline') and len(entry['emotion_timeline']) > 0:
                timeline_df = pd.DataFrame(entry['emotion_timeline'])
                
                col_chart1, col_chart2 = st.columns(2)
                
                with col_chart1:
                    emotion_counts = timeline_df['emotion'].value_counts()
                    fig_pie = px.pie(
                        values=emotion_counts.values,
                        names=emotion_counts.index,
                        title="ê°ì • ë¶„í¬",
                        color_discrete_sequence=px.colors.qualitative.Set3,
                        height=300
                    )
                    st.plotly_chart(fig_pie, use_container_width=True)
                
                with col_chart2:
                    timeline_df['time_seconds'] = timeline_df['frame'] / 20
                    timeline_df['confidence_percent'] = timeline_df['confidence'] * 100
                    
                    max_time = timeline_df['time_seconds'].max()
                    
                    fig_line = px.line(
                        timeline_df,
                        x='time_seconds',
                        y='confidence_percent',
                        color='emotion',
                        title="ê°ì • ë³€í™”",
                        markers=True,
                        height=300,
                        labels={
                            'time_seconds': 'ì˜ìƒ ì‹œê°„ (ì´ˆ)',
                            'confidence_percent': 'í™•ì‹ ë„ (%)',
                            'emotion': 'ê°ì •'
                        }
                    )
                    
                    fig_line.update_yaxes(range=[0, 100], dtick=10, title="í™•ì‹ ë„ (%)")
                    
                    import math
                    x_max = math.ceil(max_time / 10) * 10
                    fig_line.update_xaxes(range=[0, x_max], dtick=10, title="ì˜ìƒ ì‹œê°„ (ì´ˆ)")
                    
                    st.plotly_chart(fig_line, use_container_width=True)

            # ê°ì • íƒ€ì„ë¼ì¸ í‘œì‹œ
            if entry.get('emotion_timeline') and len(entry['emotion_timeline']) > 0:
                st.markdown("**ğŸ“‹ ê°ì • íƒ€ì„ë¼ì¸ (Emotion Timeline)**")
                dominant_emoji = emotion_emoji_map.get(entry.get('dominant_emotion', 'ë‹´ë‹´í•¨'), 'ğŸ“')
                st.markdown(
                    f"<small>ì£¼ìš” ê°ì •: {dominant_emoji} {entry.get('dominant_emotion', 'ë‹´ë‹´í•¨')}</small>",
                    unsafe_allow_html=True
                )
                st.markdown(
                    f"<small>í‰ê·  í™•ì‹ ë„: {entry.get('avg_confidence', 0)*100:.1f}%</small>",
                    unsafe_allow_html=True
                )

                timeline_df = pd.DataFrame(entry['emotion_timeline'])
                
                # íƒ€ì„ë¼ì¸ í…Œì´ë¸” í‘œì‹œ
                display_timeline = timeline_df[['frame', 'timestamp', 'emotion', 'confidence']].copy()
                display_timeline['confidence'] = display_timeline['confidence'].apply(lambda x: f"{x*100:.1f}%")
                display_timeline.columns = ['í”„ë ˆì„', 'ì‹œê°„', 'ê°ì •', 'í™•ì‹ ë„']
                st.dataframe(display_timeline, use_container_width=True, height=200)
    
    
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.markdown("---")
            col_dl1, col_dl2 = st.columns(2)
            
            with col_dl1:
                if os.path.exists(entry['video_path']):
                    with open(entry['video_path'], 'rb') as f:
                        video_bytes = f.read()
                        st.download_button(
                            label="ğŸ“¥ ì˜ìƒ ë‹¤ìš´ë¡œë“œ",
                            data=video_bytes,
                            file_name=entry['video_filename'],
                            mime="video/mp4",
                            key=f"download_video_{i}",
                            use_container_width=True
                        )
                else:
                    st.warning("âš ï¸ ì˜ìƒ íŒŒì¼ ì—†ìŒ")
            
            with col_dl2:
                if os.path.exists(entry['text_path']):
                    with open(entry['text_path'], 'r', encoding='utf-8') as f:
                        text_content = f.read()
                        st.download_button(
                            label="ğŸ“„ í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            data=text_content,
                            file_name=entry['text_filename'],
                            mime="text/plain",
                            key=f"download_text_{i}",
                            use_container_width=True
                        )
                else:
                    st.warning("âš ï¸ í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")
    # ì „ì²´ í†µê³„
    st.markdown("---")
    st.markdown("### ğŸ“Š ì „ì²´ ê°ì • ë¶„ì„")
    
    col_left, col_middle, col_right = st.columns([1, 1, 2])
    
    with col_left:
        # ì›í˜• ê·¸ë˜í”„
        all_emotions = [e['emotion'] for e in st.session_state.diary_entries]
        emotion_df = pd.DataFrame({'ê°ì •': all_emotions})
        emotion_counts = emotion_df['ê°ì •'].value_counts()
        
        fig_overall = px.pie(
            values=emotion_counts.values,
            names=emotion_counts.index,
            title="ì „ì²´ ê°ì • ë¶„í¬",
            color_discrete_sequence=px.colors.qualitative.Pastel
        )
        st.plotly_chart(fig_overall, use_container_width=True)
    
    with col_middle:
        # í†µê³„ ì •ë³´ - ë°˜ì‘í˜• ì—¬ë°±
        st.markdown("""
        <style>
        @media (min-width: 768px) {
            .stats-spacing {
                margin-top: 80px;
            }
        }
        </style>
        <div class="stats-spacing"></div>
        """, unsafe_allow_html=True)
        
        # AI ì¶”ì²œ vs ì‚¬ìš©ì ì„ íƒ ë¹„êµ
        ai_matches = sum(1 for e in st.session_state.diary_entries 
                        if e['emotion'] == e.get('ai_recommended', ''))
        match_rate = (ai_matches / len(st.session_state.diary_entries)) * 100
        
        personalized_count = sum(1 for e in st.session_state.diary_entries 
                                if e.get('is_personalized', False))
        
        # ì‘ì€ ê¸€ì”¨ë¡œ í†µê³„ í‘œì‹œ
        st.markdown(f"""
        <div style="font-size: 14px;">
        <p><strong>AI-ì‚¬ìš©ì ì¼ì¹˜ìœ¨:</strong> {match_rate:.1f}%</p>
        <p><strong>ë§ì¶¤í˜• ì¶”ì²œ ì‚¬ìš©:</strong> {personalized_count}íšŒ</p>
        <p><strong>í•™ìŠµ ë°ì´í„°:</strong> {len(st.session_state.personalized_model.training_data)}ê°œ</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col_right:
        # ì‹œê³„ì—´ ê°ì • ë³€í™” ê·¸ë˜í”„
        st.markdown("**ğŸ“… ê°ì • íƒ€ì„ë¼ì¸ (Emotion Timeline)**")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        timeline_data = []
        for entry in st.session_state.diary_entries:
            try:
                # timestamp í˜•ì‹: YYYYMMDD_HHMMSS
                timestamp_str = entry['timestamp']
                dt = datetime.strptime(timestamp_str, '%Y%m%d_%H%M%S')
                
                timeline_data.append({
                    'ë‚ ì§œ': dt.date(),  # ë‚ ì§œë§Œ ì¶”ì¶œ
                    'ë‚ ì§œì‹œê°„': dt,
                    'ê°ì •': entry['emotion'],
                    'ì´ëª¨ì§€': emotion_emoji_map.get(entry['emotion'], 'ğŸ˜')
                })
            except Exception as e:
                print(f"ë‚ ì§œ ë³€í™˜ ì˜¤ë¥˜: {e}")
                continue
        
        if timeline_data:
            timeline_df = pd.DataFrame(timeline_data)
            timeline_df = timeline_df.sort_values('ë‚ ì§œì‹œê°„')
            
            # ê°ì •ë³„ ìƒ‰ìƒ ë§¤í•‘
            emotion_colors = {
                'í–‰ë³µí•¨': '#FFD700',  # ê¸ˆìƒ‰
                'ìŠ¬í””': '#4169E1',    # íŒŒë€ìƒ‰
                'í™”ë‚¨': '#FF4500',    # ë¹¨ê°„ìƒ‰
                'ë†€ëŒ': '#FF69B4',    # ë¶„í™ìƒ‰
                'ë‹´ë‹´í•¨': '#A9A9A9',  # íšŒìƒ‰
                'ë‘ë ¤ì›€': '#800080',  # ë³´ë¼ìƒ‰
                'í˜ì˜¤': '#228B22'     # ì´ˆë¡ìƒ‰
            }
            
            # ì‚°ì ë„ ìŠ¤íƒ€ì¼ ê·¸ë˜í”„ (ê° ê°ì •ì„ ìƒ‰ìƒìœ¼ë¡œ êµ¬ë¶„)
            fig_scatter = px.scatter(
                timeline_df,
                x='ë‚ ì§œ',
                y='ê°ì •',
                color='ê°ì •',
                text='ì´ëª¨ì§€',  # ì´ëª¨ì§€ë¥¼ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
                title='',
                labels={'ë‚ ì§œ': 'ë‚ ì§œ', 'ê°ì •': 'ê°ì •'},
                color_discrete_map=emotion_colors
            )
            
            # ë§ˆì»¤ë¥¼ íˆ¬ëª…í•˜ê²Œ í•˜ê³  ì´ëª¨ì§€ë§Œ í‘œì‹œ
            fig_scatter.update_traces(
                marker=dict(size=1, opacity=0),  # ë§ˆì»¤ë¥¼ ê±°ì˜ ë³´ì´ì§€ ì•Šê²Œ
                textfont=dict(size=30),  # ì´ëª¨ì§€ í¬ê¸°
                textposition='middle center'
            )
            
            # ëª¨ë“  ê°ì • ë ˆì´ë¸”ì„ Yì¶•ì— í‘œì‹œ
            all_emotions = ['í–‰ë³µí•¨', 'ìŠ¬í””', 'í™”ë‚¨', 'ë†€ëŒ', 'ë‹´ë‹´í•¨', 'ë‘ë ¤ì›€', 'í˜ì˜¤']
            
            fig_scatter.update_layout(
                height=500,
                hovermode='closest',
                xaxis_title='ë‚ ì§œ',
                yaxis_title='ê°ì •',
                showlegend=True,
                legend=dict(
                    title="ê°ì •",
                    orientation="v",
                    yanchor="top",
                    y=1,
                    xanchor="left",
                    x=1.02
                ),
                yaxis=dict(
                    categoryorder='array',
                    categoryarray=all_emotions  # ëª¨ë“  ê°ì •ì„ ìˆœì„œëŒ€ë¡œ í‘œì‹œ
                )
            )
            
            # Xì¶•ì„ í•˜ë£¨ ê°„ê²©ìœ¼ë¡œ ì„¤ì •
            fig_scatter.update_xaxes(
                dtick=86400000.0,  # 1ì¼ = 86400000 ë°€ë¦¬ì´ˆ
                tickformat='%Y-%m-%d'
            )
            
            st.plotly_chart(fig_scatter, use_container_width=True)
        else:
            st.info("ì‹œê³„ì—´ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
else:
    st.info("ğŸ“­ ì•„ì§ ì €ì¥ëœ ì˜ìƒ ì¼ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ë…¹í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")